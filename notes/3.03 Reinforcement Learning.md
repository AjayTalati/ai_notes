
# Reinforcement learning

MDPs as described above have been fully-observed in the sense that we knew all of their parameters (probabilities and so on). Because everything was known in advance, we could conduct __offline planning__, that is, formulate a plan without needing to interact with the world.

MDP parameters aren't always known from the onset - we may not know the reward function $R$ or even the transition model $T$, and then we must engage in __online planning__, in which we must interact with the world to learn more about it to better formulate a plan.

Online planning involves __reinforcement learning__, where agents can learn in what states rewards or goals are located without needing to know from the start.

Reinforcement learning in summary:

- the agent interacts with its environment and receives feedback in the form of rewards
- the agent's utility is defined by the reward function
- the agent must learn to act so as to maximize expected rewards
- learning is based on observed samples of outcomes

With reinforcement learning, we still assume a MDP, it's just not fully specified - that is, we don't know $R$ and we might not know $T$.

If we do know $T$, a __utility-based agent__ can learn $R$ and thus $V$, which we can then use for MDP.

If $T$ and $R$ are both unknown, a __Q-learning agent__ can learn $Q(s,a)$ without needing either. Where $V(s)$ is the value over states, $Q(s,a)$ is the value over state-action pairs and can also be used with MDP.

A __reflex agent__ can also directly learn the policy $\pi(s)$ without needing to know $T$ or $R$.

Reinforcement learning agents can be _passive_, which means the agent has a fixed policy and learns $R$ and $T$ (if necessary) while executing that policy.

Alternatively, an _active_ reinforcement learning agent changes its policy as it goes and learns.

Passive learning has the drawbacks that it can take awhile to converge on good estimates for the unknown quantities, and it may limit how much of the space is actually explored, and as such, there may be little or no information about some states and better paths may remain unknown.

## Model-based learning

Model-based learning is a simple approach to reinforcement learning.

The basic idea:

- learn an approximate model based on experiences
- solve for values (i.e. using value iteration or policy iteration) as if the learned model were correct

In more detail:

1. learn an empirical MDP model
  - count outcomes $s'$ for each $s, a$
  - normalize to get an estimate of $\hat T(s,a,s')$
  - discover each $\hat R(s,a,s')$ when we experience $(s,a,s')$
2. solve the learned MPD (e.g. value iteration or policy iteration)

### Temporal Difference Learning (TDL)

In __temporal difference learning__, the agent moves from one state $s$ to the next $s'$, looks at the reward difference between the states, then backs up (propagates) the values (as in value iteration) from one state to the next.

We run this many times, reaching a terminal state, then restarting, and so on, to get better estimates of the rewards (utilities) for each state.

We keep track of rewards for visited states as $U(s)$ and also the number of times we have visited each state as $N(s)$.

The main part of the algorithm is:

- if $s'$ is new then $U[s'] = r'$
- if $s$ is not null then
  - increment $N_s[s]$
  - $U[s] = U[s] + \alpha(N_s[s])(r + \gamma U[s] - U[s])$

Where $\alpha$ is the learning rate and $\gamma$ is the discount factor.

#### Greedy TDL

One method of active reinforcement learning is the greedy approach.

Given new estimates for the rewards, we recompute a new optimal policy and then use that policy to guide exploration of the space. This gives us new estimates for the rewards, and so on, until convergence.

Thus it is greedy in the sense that it always tries to go for policies that seem immediately better, although in the end that doesn't necessarily guarantee the overall optimal policy (this is the exploration vs exploitation problem).

One alternate approach is to randomly try a non-optimal action, thus exploring more of the space. This works, but can be slow to converge.

### Exploration agent

A approach better than TDL is to use an __exploration agent__, which favors exploring more when it is uncertain. More specifically, we can use the same TDL algorithm, but while $N_s < \epsilon$, where $\epsilon$ is some exploration threshold, we set $U[s] = R$, where $R$ is the largest reward we expect to get. When $N_s > \epsilon$, we start using the learned reward as with regular TDL.

## Model-free learning

With model-free learning, instead of trying to estimate $T$ or $R$, we take actions and the actual outcome to what we expected the outcome would be.

With __passive reinforcement learning__, the agent is given an existing policy and just learns from the results of that policy's execution (that is, learns the state values; i.e. this is essentially just policy evaluation, except this is _not_ offline, this involves interacting with the environment).

To compute the values for each state under $\pi$, we can use __direct evaluation__:

- act according to $\pi$
- every time we visit a state, record what the sum of discounted rewards turned out to be
- average those samples

Direct evaluation is simple, doesn't require any knowledge of $T$ or $R$, and _eventually_ gets the correct average values. However, it throws out information about state connections, since each state is learned separately - for instance, if we have a state $s_i$ with a positive reward, and another state $s_j$ that leads into it, it's possible that direct evaluation assigns state $s_j$ a negative reward, which doesn't make sense - since it leads to a state with a positive reward, it should also have some positive reward. Given enough time/samples, this will eventually resolve, but that can require a long time.

Policy evaluation, on the other hand, _does_ take in account the relationship between states, since the value of each state is a function of its child states, i.e.

$$
V_{k+1}^{\pi_i}(s) = \sum_{s'} T(s, \pi_k(s), s') [R(s, \pi_k(s), s') + \gamma V_k^{\pi_i}(s')]
$$

However, we don't know $T$ and $R$. Well, we could just try actions and take samples of outcomes $s'$ and average:

$$
V_{k+1}^{\pi}(s) = \frac{1}{n}\sum_i \text{sample}_i
$$

Where each $\text{sample}_i = R(s, \pi(s), s_i') + \gamma V_k^{\pi}(s_i')$. $R(s, \pi(s), s_i')$ is just the observed reward from taking the action.

This is called __sample-based policy evaluation__.

One challenge here: when you try an action, you end up in a new state - how do you get back to the original state to try another action? We don't know anything about the MDP so we don't necessarily know what action will do this.

So really, we only get one sample, and then we're off to another state.

With _temporal difference learning_, we learn from each experience ("episode"); that is, we update $V(s)$ each time we experience a transition $(s,a,s',r)$. The likely outcomes $s'$ will contribute updates more often. The policy is still fixed (given), and we're still doing policy evaluation.

Basically, we have an estimate $V(s)$, and then we take an action and get a new sample. We update $V(s)$ like so:

$$
V^{\pi}(s) = (1 - \alpha) V^{\pi}(s) + (\alpha) \text{sample}
$$

So we specify a learning rate $\alpha$ (usually small, e.g. $\alpha=0.1$) which controls how much of the old estimate we keep. This learning rate can be decreased over time.

This is an exponential moving average.

This update can be re-written as:

$$
V^{\pi}(s) = V^{\pi}(s) + \alpha (\text{sample} - V^{\pi}(s))
$$

The term $(\text{sample} - V^{\pi}(s))$ can be interpreted as an error, i.e. how off our current estimate $V^{\pi}(s)$ was from the observed sample.

So we still never learn $T$ or $R$, we just keep running sample averages instead; hence temporal difference learning is a model-free method for doing policy evaluation.

However, it doesn't help with coming up with a new policy, since we need Q-values to do so.

### Q-Learning

With _active reinforcement learning_, the agent is actively trying new things rather than following a fixed policy.

The fundamental trade-off in active reinforcement learning is __exploitation vs exploration__. When you land on a decent strategy, do you just stick with it? What if there's a better strategy out there? How do you balance using your current best strategy and searching for an even better one?

Remember that value iteration requires us to look at $\max_a$ over the set of possible actions from a state:

$$
V_{k+1}(s) = \max_a \sum_{s'} T(s,a,s') [R(s,a,s') + \gamma V_k(s')]
$$

However, we can't compute maximums from samples since the maximum is always unknown (there's always the possibility of a new sample being larger; we can only compute averages from samples).

We can instead iteratively compute Q-values (Q-value iteration):

$$
Q_{k+1}(s,a) = \sum_{s'} T(s,a,s') [R(s,a,s') + \gamma \max_{a'} Q(s', a')]
$$

Remember that while a value $V(s)$ is the value of a state, a Q-value $Q(s,a)$ is the value of an action (from a particular state).

Here the $\max$ term pushed inside, and we are ultimately just computing an average, so we can compute this from samples.

This is the basis of the __Q-Learning algorithm__, which is just sample-based Q-value iteration.

We learn $Q(s,a)$ values as we go:

- take an action $a$ from a state $s$ and see the outcome as a sample $(s,a,s',r)$.
- consider the old estimate $Q(s,a)$
- consider the new sample estimate: $\text{sample} = R(s,a,s') + \gamma \max_{a'} Q(s', a')$, where $R(s,a,s') = r$, i.e. the reward we just received
- incorporate this new estimate into a running average:

$$
Q(s,a) = (1 - \alpha)Q(s,a) + (\alpha)\text{sample}
$$

This can also be written:

$$
Q(s,a) =_{\alpha} R(s,a,s') + \gamma \max_{a'} Q(s', a')
$$

These updates emulate Bellman updates as we do in known MDPs.

Q-learning converges to an optimal policy, even if you're acting suboptimally. When an optimal policy is still learned from suboptimal actions, it is called __off-policy learning__.

We still, however, need to explore and decrease the learning rate (but not too quickly or you'll stop learning things).

In Q-Learning, we don't need $P$ or the reward/utility function. We directly learn the rewards/utilities of state-action pairs, $Q(s,a)$.

With this we can just choose our optimal policy as:

$$
\pi(s) = \argmax_a \sum_{s'} Q(s,a)
$$

The $Q$ update formula is simply:

$$
Q(s,a) = Q(s,a) + \alpha(R(s) + \gamma Q(s', a') - Q(s,a))
$$

Where $\alpha$ is the learning rate and $\gamma$ is the discount factor.

Again, we can back up (as with value iteration) to propagate these values through the network.

### Exploration vs exploitation

Up until now we have not considered how we select actions. So how do we? That is, how do we explore?

One simple method is to sometimes take random actions ($\epsilon$-greedy). With a small probability $\epsilon$, act randomly, with probability $1-\epsilon$, act on the current best policy.

After the space is thoroughly explored, you don't want to keep moving randomly - so you can decrease $\epsilon$ over time.

Alternatively, we can use _exploration functions_. Generally, we want to explore areas we have high uncertainty for. More specifically, an exploration function takes a value estimate $u$ and a visit count $n$ and returns an optimistic utility. For example: $f(u,n) = u + \frac{k}{n}$.

We can modify our Q-update to incorporate an exploration function:

$$
Q(s,a) =_{\alpha} R(s,a,s') + \gamma \max_{a'} f(Q(s', a'), N(s',a'))
$$

This encourages the agent not only to try unknown states, but to also try states that lead to unknown states.

In addition to exploration and exploitation, we also introduce a concept of __regret__. Naturally, mistakes are made as the space is explored - regret is a measure of the total mistake cost. That is, it is the difference between your expected rewards and optimal expected rewards.

We can try to minimize regret - to do so, we must not only learn to be optimal, but we must _optimally_ learn how to be optimal.

For example: both random exploration and exploration functions are optimal, but random exploration has higher regret.

### Approximate Q-Learning

Sometimes spaces are far too large to satisfactorily explore. This can be a limit of memory (since Q-learning keeps a table of Q-values) or simply that there are too many states to visit in a reasonable time. In fact, this is the rule rather than the exception. So in practice we cannot learn about every state.

The general idea of __approximate Q-learning__ is to transfer learnings from one state to other similar states. For example, if we learn from exploring one state that a fire pit is bad, then we can generalize that all fire pit states are probably bad.

This is an approach like machine learning - we want to learn general knowledge from a few training states; the states are represented by features (for example, we could have a binary feature $\text{has fire pit}$). Then we describe q-states in terms of features, e.g. as linear functions (called a __Q-function__):

$$
Q(s,a) = w_1 f_1(s,a) + w_2 f_2(s,a) + \dots + w_n f_n(s,a)
$$

Note that we can do the same for value functions as well, i.e.

$$
V(s) = w_1 f_1(s) + w_2 f_2(s) + \dots + w_n f_n(s)
$$

So we observe a transition $(s,a,r,s')$ and then we compute the difference of this observed transition from what we expected, i.e:

$$
\text{difference} = [r + \gamma \max_{a'} Q(s', a')] - Q(s,a)
$$

With exact Q-learning, we would update $Q(s,a)$ like so:

$$
Q(s,a) = Q(s,a) + \alpha [\text{difference}]
$$

With approximate Q-learning, we instead update the weights, and we do so in proportion to their feature values:

$$
w_i = w_i + \alpha [\text{difference}] f_i (s,a)
$$

This is the same as least-squares regression.

That is, given a point $x$, with features $f(x)$ and target value $y$, the error is:

$$
\text{error}(w) = \frac{1}{2} (y - \sum_k w_k f_k(x))^2
$$

The derivative of the error with respect to a weight $w_m$ is:

$$
\frac{\partial \text{error}(w)}{\partial w_m} = - ( y- \sum_k w_k f_k(x)) f_m(x)
$$

Then we update the weight:

$$
w_m = w_m + \alpha (y - \sum_k w_k f_k(x)) f_m(x)
$$

In terms of approximate Q-learning, the target $y$ is $r + \gamma \max_{a'} Q(s', a')$ and our prediction is $Q(s,a)$:

$$
w_m = w_m + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s,a)] f_m (s,a)
$$


### Policy Search

Q-learning tries to model the states by learning q-values. However, a feature-based Q-learning model that models the states well does not necessarily translate to a good feature-based policy (and vice-versa).

Instead of trying to model the unknown states, we can directly try to learn the policies that maximize rewards.

So we can use Q-learning and learn a decent solution, then fine-tune by hill climbing on feature weights. That is, we learn an initial linear Q-function, the nudge each feature weight and up and down to see if the resulting policy is better.

We test whether or not a policy is better by running many sample episodes.

If we have many features, we have to test many new policies, and this hill climbing approach becomes impractical. There are better methods (not discussed here).

### Summary

A helpful table:

For a known MDP, we can compute an offline solution:

| Goal                          | Technique                 |
|-------------------------------|---------------------------|
| Compute $V^*, Q^*, \pi^*$     | Value or policy iteration |
| Evaluate a fixed policy $\pi$ | Policy evaluation         |

For an unknown MDP, we can use model-based approaches:

| Goal                          | Technique                                         |
|-------------------------------|---------------------------------------------------|
| Compute $V^*, Q^*, \pi^*$     | Value or policy iteration on the approximated MDP |
| Evaluate a fixed policy $\pi$ | Policy evaluation on the approximated MDP         |

Or we can use model-free approaches:

| Goal                          | Technique      |
|-------------------------------|----------------|
| Compute $V^*, Q^*, \pi^*$     | Q-learning     |
| Evaluate a fixed policy $\pi$ | Value learning |

## References

- [Reinforcement Learning - Part 1](http://outlace.com/Reinforcement-Learning-Part-1/). Brandon B.
- [CS188: Artificial Intelligence](https://www.edx.org/course/artificial-intelligence-uc-berkeleyx-cs188-1x). Dan Klein, Pieter Abbeel. University of California, Berkeley (edX).
- [Intro to Artificial Intelligence](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271). CS271. Peter Norvig, Sebastian Thrun. Udacity.

