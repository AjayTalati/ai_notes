
$$
\providecommand{\argmax}{\operatorname*{argmax}}
\providecommand{\argmin}{\operatorname*{argmin}}
$$


# Probabilistic Machine Learning

Fundamentally, machine learning is all about data:

- Stochastic, chaotic, and/or complex generative processes
- Noisily observed
- Partially observed

So there is a lot of uncertainty - we can use probability theory to express this uncertainty in the form of probabilistic models.


### Probabilistic modeling

We have some data $x_1, x_2, \dots, x_n$ and some latent variables $y_1, y_2, \dots, y_n$ we want to uncover, which correspond to each of our data points.

We have a parameter $\theta$.

A probabilistic model is just a parameterized joint distribution over all the variables:

$$
P(x_1, \dots, x_n, y_1, \dots, y_n | \theta)
$$

We usually interpret such models as a _generative_ model - how was our observed data _generated_ by the world?

So the problem of inference is about learning about our latent variables given the observed data, which we can get via the posterior distribution:

$$
P(y_1, \dots, y_n | x_1, \dots, x_n, \theta) = \frac{P(x_1, \dots, x_n, y_1, \dots, y_n | \theta)}{P(x_1, \dots, x_n|\theta)}
$$

Learning is typically posed as a _maximum likelihood_ problem; that is, we try to find $\theta$ which maximizes the probability of our observed data:

$$
\theta^{ML} = \argmax_{\theta} P(x_1, \dots, x_n | \theta)
$$

Then, to make a prediction we want to compute the conditional distribution of some future data:

$$
P(x_{n+1}, y_{n+1}|x_1, \dots, x_n, \theta)
$$

Or, for classification, if we some classes, each parameterizing a joint distribution, we want to pick the class which maximizes the probability of the observed data:

$$
\argmax_{c}P(x_{n+1}|\theta^c)
$$

In probabilistic learning, you typically have two separate problems:

1. Learning the parameters
2. Inference (computing the posterior distribution of the latent variables given the observed data and parameters)


### Bayesian modeling

Bayesian modeling treats those two problems as one.

We first have a prior distribution over our parameters (i.e. what are the likely parameters?) $P(\theta)$.

From this we compute a posterior distribution which combines both inference and learning:

$$
P(y_1, \dots, y_n, \theta | x_1, \dots, x_n) = \frac{P(x_1, \dots, x_n, y_1, \dots, y_n | \theta)P(\theta)}{P(x_1, \dots, x_n)}
$$

Then prediction is to compute the conditional distribution of the new data point given our observed data, which is the marginal of the latent variables and the parameters:

$$
P(x_{n+1}|x_1, \dots, x_n) = \int P(x_{n+1}|\theta)P(\theta|x_1, \dots, x_n) d\theta
$$

Classification then is to predict the distributions of the new datapoint given data from other classes, then finding the class which maximizes it:

$$
P(x_{n+1}|x_1^c, \dots, x_n^c) = \int P(x_{n+1}|\theta^c)P(\theta^c|x_1^c, \dots, x_n^c) d\theta^c
$$

#### Bayesian probability modeling examples

These examples are all parametric models.

##### Model-based clustering

- model data from heterogeneous unknown sources
- $K$ unknown sources (clusters)
- each cluster/source is modeled using a parametric model (e.g. a Gaussian distribution)

For a given data point $i$, we have:

$$
z_i | \pi \sim \text{Discrete}(\pi)
$$

Where $z_i$ is the cluster label for which data point $i$ belongs to. This is the latent variable we want to discover.

$\pi$ is the _mixing proportions_ which is the vector of probabilities for each class $k$, that is:

$$
\pi = (\pi_i, \dots, \pi_K) | \alpha \sim \text{Dirichlet}(\frac{\alpha}{K}, \dots, \frac{\alpha}{K})
$$

That is, $\pi_k = P(z_i = k)$.

We also model each data point $x_i$ as being drawn from a source (cluster) like so, where $F$ is however we are modeling the cluster (e.g. a Gaussian), parameterized by $\theta^*_{z_i}$, that is some parameters for the $z_i$-labeled cluster:

$$
x_i | z_i, \theta^*_k \sim F(\theta^*_{z_i})
$$

(Note that the star, as in $\theta^*$, is used to denote the optimal solution for $\theta$.)

For this approach we have two priors over parameters of the model:

- For the mixing proportions, we typically use a Dirichlet prior (above) because it has the nice property of being a conjugate prior with multinomial distributions.
- For each cluster $k$ we use some prior $H$, that is $\theta^*_k | H \sim H$.

Graphically, this is:

![Model-based clustering plate model](assets/model_based_clustering.svg)

##### Hidden Markov Models

HMMs can be thought of as clustering over time; that is, each state is a "cluster".

The data points and latent variables are sequences, and $\pi_k$ becomes the transition probability given the state (cluster) $k$. $\theta^*_k$ becomes the emission distribution for $x$ given state $k$.


## Nonparametric models

First: a parametric model is one in which the capacity is fixed and does not increase with the amount of training data. For example, a linear classifier, a neural network with fixed number of hidden units, etc. The amount of parameters is finite, and the particular amount is determined before any data is observed (e.g. with linear regression, _we_ decide the number of parameters that will be used, rather than learning it from the data).

### What is a nonparametric model?

- counterintuitively, it does not mean a model without parameters. Rather, it means a model with a very large number of parameters (e.g. infinite). Here, "nonparametric" refers more to "not a parametric model", _not_ "without parameters".
- could also be defined as a parametric model where the number of parameters increases with the data, instead of fixing the number of parameters (that is, the number of things we can learn) as is the case with parametric models. I.e. the capacity of the model increases with the amount of training data.
- can also be defined as a family of distributions that is dense in some large space relevant to the problem at hand.
    - For example, with a regression problem, the space of possible solutions may be all continuous functions, which is infinite-dimensional (if you have infinite cardinality). A nonparametric model can span this infinite space.

To expand and visualize the last point, consider the regression problem example.

![Space of continuous functions](assets/nonparametric_01.svg)

This is the space of continuous functions, where $f^*$ is the function we are looking for.

![Space of continuous functions w/ parametric model](assets/nonparametric_02.svg)

With a parametric model, we have a finite number of parameters, so we can only cover a fraction of this space (the orange square).

![Space of continuous functions w/ nonparametric model](assets/nonparametric_03.svg)

However, with a nonparametric model, we can have infinite parameters and cover the entire space. We apply some assumptions, e.g. favoring simpler functions over complex ones, so we can apply a prior to the space which assigns more mass to simpler functions (the darker parts in the accompanying figure). But every part of the space still has some mass.

It is possible to create a nonparametric model by nesting a parametric learning algorithm inside another parametric learning algorithm. The outer learning algorithm learns the number of parameters, whereas the inner learning algorithm performs as it normally would (learning the parameters themselves).


### Why use a Bayesian nonparametric approach?

1. Model selection
    - e.g. clustering - you have to specify the number of clusters. Too many and you overfit, too few and you underfit.
    - with a Bayesian approach you are not doing any optimizing (such as finding a maximum likelihood), you are just computing a posterior distribution. So there is no "fitting" happening, so you cannot overfit.
    - If you have a large model or one which grows with the amount of data, you can avoid underfitting too.
- (of course, you can still specify an incorrect model and get poor performance)

2. Useful properties of Bayesian nonparametric models
    - _Exchangeability_ - you can permute your data without affecting learning (i.e. order of your data doesn't matter)
    - Can model Zipf, Heap, and other power laws
    - Flexible ways of building complex models from simpler parts

Nonparametric models still make modeling assumptions, they are just less constrained than most parametric models.

There are also _semiparametric_ models in which they are nonparametric in some ways and parametric in others.


## The Dirichlet Process

The Dirichlet process is "the cornerstone of Bayesian nonparametrics".

It is a stochastic process - a model over an infinite collection of random variables.

There are a few ways to think about Dirichlet processes:

- the infinite limit of a Gibbs sampler for finite mixture models
- the Chinese Restaurant Process
- The stick-breaking construction


### Finite Mixture Models

This is a continuation of the model-based Clustering approach mentioned earlier.

We want to learn, via inference, values for $\pi$, $z_i$, and $\theta^*_k$.

We can use a form of MCMC sampling - Gibbs sampling.

(to do: this is incomplete)


### Chinese Restaurant Process

#### Partitions

Given a set $S$, a partition $\varrho$ is a disjoint family of non-empty subsets (clusters) of $S$ whose union is $S$. So a partition is some configuration of clusters which encompasses the members of $S$.

E.g.

$$
\begin{aligned}
S &= \{A,B,C,D,E,F\} \\
\varrho &= \{\{A,D\}, \{B,C,E\}, \{F\}\}
\end{aligned}
$$

The set of all partitions of $S$ is denoted $\mathcal P_S$.

_Random partitions_ are random variables taking value in $\mathcal P_S$.

#### The Chinese Restaurant Process (CRP)

The CRP is an example of random partitions and involves a sequence of customers coming into a restaurant. Each customer decides whether or not to sit at a new (empty) table or join a table with other customers. The customers are sociable so prefer to join tables with more customers, but there is still some probability that they will sit at a new table:

$$
\begin{aligned}
P(\text{sit at new table}) &= \frac{\alpha}{\alpha + \sum_{c \in \varrho} n_c} \\
P(\text{sit at table $c$}) &= \frac{n_c}{\alpha + \sum_{c \in \varrho} n_c}
\end{aligned}
$$

Where $n_c$ is the number of customers at a table $c$ and $\alpha$ is a parameter.

Here the customers correspond to members of the set $S$, and tables are the clusters in a partition $\varrho$ of $S$.

This process has a _rich-get-richer_ property, in that large clusters are more likely to attract more customers, thus growing larger, and so on.

If you multiply all the conditional probabilities together, the overall probability of the partition $\varrho$, called the _exchangeable partition probability function_ (EPPF), is:

$$
P(\varrho|\alpha) = \frac{\alpha^{|\varrho|}\Gamma(\alpha)}{\Gamma(n + \alpha)} \prod_{c \in \varrho} \Gamma(|c|)
$$

This probability ends up not depending on the sequence in which customers arrive - so this is an _exchangeable random partition_.

The $\alpha$ parameter affects the number of clusters in the partition - the larger the $\alpha$, the more clusters we expect to see.

#### Model-based Clustering with the Chinese Restaurant Process

Given a dataset $S$, we want to partition it into clusters of similar items.

Each cluster $c \in \varrho$ is described by a model $F(\theta^*_c)$, for example a Gaussian, parameterized by $\theta^*_c$.

We model each item in each cluster as drawn from that cluster's model.

We are going to use a Bayesian approach, so we introduce a prior over $\varrho$ and $\theta^*_c$ and the compute posteriors over both. We use a CRP mixture model; that is we use a Chinese Restaurant Process for the prior over $\varrho$ and an independent and identically distributed (iid) prior $H$ over the cluster parameters $\theta^*_c$.

So the CRP mixture model in more detail:

- $\varrho \sim CRP(\alpha)$
- $\theta^*_c | \varrho \sim H \text{ for } c \in \varrho$
- $x_i|\theta^*_c,\varrho \sim F(\theta^*_c) \text{ for } c \in \varrho \text{ with } i \in c$


## References

- Yee Whye Teh, MLSS 2013 (Max Planck Institute for Intelligent Systems, Tübingen) <https://www.youtube.com/watch?v=dNeW5zoNJ7g>
- IFT 725 Review of fundamentals



---

## Infinite Mixture Models and the Dirichlet Process

(this is basically a paraphrasing of [this post by Edwin Chen](http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/).)

Many clustering methods require the specification of a fixed number of clusters. However, in real-world problems there may be an infinite number of possible clusters - in the case of food there may be Italian or Chinese or fast-food or vegetarian food and so on. Nonparametric Bayesian methods allow parameters to change with the data; e.g. as we get more data we can let the number of clusters grow.

Say we have some data, where each data point is some vector.

We can view our data from a generative perspective: we can assume that the true clusters in the data are each defined by some model with some parameters, such as Gaussians with $\mu_i$ and $\sigma_i$ parameters. We further assume that these parameters themselves come from a distribution $G_0$ Then we assume the data is generated by selecting a cluster, then taking a sample from that cluster.

Ok, how then do we assign the data points to groups?

### Chinese Restaurant Process

(see explanation above)

(As a side note, the Indian Buffet Process is an extension of the CRP in which customers can sample food from multiple tables, that is, they can belong to multiple clusters.)

More formally:

- Generate table assignments $g_1, \dots, g_2 \sim CRP(\alpha)$, that is, according to a Chinese Restaurant Process. $g_i$ is the table assigned to datapoint $i$.
- We generate table parameters $\phi_1, \dots, \phi_m \sim G_0$ according to the base distribution $G_0$, where $\phi_k$ is the parameter for the $k$th distinct group.
- Given the table assignments and table parameters, generate each datapoint $p_i \sim F(\phi_{g_i})$ from a distribution $F$ with the specified table parameters. For example, $F$ could be a Gaussian and $phi_i$ might be a vector specifying the mean and standard deviation.


### Polya Urn Model

Basically the same as the Chinese Restaurant Process, except that while the CRP specifies a distribution over partitions (see above), the Polya Urn model does that and also assigns parameters to each group.

Say we have an urn containing $\alpha G_0(x)$ balls of some color $x$ for each possible value of $x$. $G_0$ is our base distribution and $G_0(x)$ is the probability of sampling $x$ from $G_0$.

Then we iteratively pick a ball at random from the urn, place it back and also place an additional new ball of the same color of the one we drew.

As $\alpha$ increases (that is, we draw more new ball colors from the base distribution, which is the same as placing more weight on our prior), the colors in the urn tend towards the base distribution.

More formally:

- Generate colors $\phi_1, \dots, \phi_n \sim Polya(G_0, \alpha)$, that is, according to a Polya Urn Model. $\phi_i$ is the color of the $i$th ball.
- Given the ball colors, generate each datapoint $p_i \sim F(\phi_i)$ (where we are using $F$ is a way like in the Chinese Restaurant Process above).


### Stick-Breaking Process

The stick-breaking process is also very similar to the CRP and the Polya Urn model.

We start with a "stick" of length one, then generate a random variable $\beta_1 \sim Beta(1, \alpha)$. Since we're drawing from the Beta distribution, $\beta_1$ will be a real number between 0 and 1 with the expected value $\frac{1}{1 + \alpha}$.

Then break off the stick at $\beta_1$. We define $w_1$ to be the length of the left stick.

Then we take the right piece (the one we broke off) and generate $\beta_1 \sim Beta(1, \alpha)$.

Then break off the stick at $\beta_2$, set $w_2$ to be the length of the stick to the right, and so on.

Here $\alpha$ again functions as a dispersion parameter; when it is low there are few, denser clusters, when it is high, there are more clusters.

More formally:

- Generate group probabilities (stick lengths) $w_1, \dots, w_{\infty} \sim Stick(\alpha)$, that is, according to a Stick-Breaking process.
- Generate group parameters $\phi_1, \dots, \phi_{\infty} \sim G_0$, where $\phi_k$ is the parameter for the $k$th distinct group.
- Generate group assignments $g_1, \dots, g_n \sim Multinomial(w_1, \dots, w_{\infty})$ for each datapoint.
- Given group assignments and group parameters, generate each datapoint $p_i \sim F(\phi_{g_i})$ (where we are using $F$ is a way like in the Chinese Restaurant Process above).

### Dirichlet Process

The CRP, Polya Urn Model, and Stick-Breaking Process are all connected to the Dirichlet Process.

Suppose we have a Dirichlet process $DP(G_0, \alpha)$ where $G_0$ is the base distribution and $\alpha$ is the dispersion parameter. Say we want to sample $x_i \sim G$, where $G$ is a distribution sampled from our Dirichlet Process, $G \sim DP(G_0, \alpha)$.

We could generate these $x_i$ values by taking a Polya Urn Model with color distribution $G_0$ and dispersion $\alpha$ - then $x_i$ could be the color of the $i$th ball in the urn.

Or we could generate these $x_i$ by assigning customers to tables via a CRP with dispersion $\alpha$. Then all the customers for a table is given the same value (e.g. color) sampled from $G_0$. $x_i$ is the value/color given to the $i$th customer; here $x_i$ can be thought of as the parameters for table $i$.

Or we could generate weights $w_k$ via a Stick-Breaking Process with dispersion $\alpha$. Then we give each weight $w_k$ a value/color $v_k$ sampled from $G_0$. We assign $x_i$ to $v_k$ with probability $w_k$.

More formally:

- Generate a distribution $G \sim DP(G_0, \alpha)$ from a Dirichlet process with base distribution $G_0$ and a dispersion parameter $\alpha$.
- Generate group-level parameters $x_i \sim G$ where $x_i$ is the group parameter for the $i$th datapoint. Note that $x_i$ is not the same as $\phi_i$; $x_i$ is the parameter associated to the group that the $i$th data point belongs to whereas $\phi_k$ is the parameter of the $k$th distinct group.
- Given group-level parameters $x_i$, generate each datapoint $p_i \sim F(x_i)$ (where we are using $F$ is a way like in the Chinese Restaurant Process above).

### Gibbs Sampling

Now that we have the generative model, we can use it to calculate the probability of some set of group assignments for our data points. But how do we learn what a good set of group assignments is?

We can use Gibbs Sampling, that is:

- Take the set of data points, and randomly initialize group assignments.
- Pick a point. Fix the group assignments of all the other points, and assign the chosen point a new group (which can be either an existing cluster or a new cluster) with a CRP-ish probability (as described in the models above) that depends on the group assignments and values of all the other points.
- We will eventually converge on a good set of group assignments, so repeat the previous step until happy.

### References

- Edwin Chen, <http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/>

## Parametric models vs nonparametric models

Parametric models are relatively rigid; once you choose a model, there are some limitations to what forms that model can take (i.e. how it can fit to the data), and the only real flexibility is in the parameters which can be adjusted. For instance, with linear regression, the model must take the form of $y = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n$; you can only adjust the $\beta_i$ values. If the "true" model does not take this form, we probably won't be able to estimate it well because the model we chose fundamentally does not conform to it.

Parametric models, on the other hand, offer greater freedom of fit.

As an example, a histogram can be considered a nonparametric representation of a probability density - it "let's the data speak for itself", so to speak (you may hear nonparametric models described in this way). The density that forms in the histogram is determined directly by the data. You don't make any assumptions about what the distribution is beforehand - e.g. you don't have to say, "I think this might be a normal distribution", and then try to force the normal probability density function onto the data.

Nonparametric models don't actually mean there are no parameters, but it is perhaps better described as not having a fixed set of parameters.

### References

- Kernel Density Estimation and Kernel Regression. Justin Esarey. <https://www.youtube.com/watch?v=QSNN0no4dSI>
- Deep Learning. Yoshua Bengio, Ian Goodfellow, Aaron Courville. <http://www-labs.iro.umontreal.ca/~bengioy/dlbook/>