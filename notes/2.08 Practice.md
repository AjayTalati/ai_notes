
# In Practice

## Machine Learning System Design

Before you start building your machine learning system, you should:

- Be explicit about the problem.
    - Start with a very specific and well-defined question: what do you want to predict, and what do you have to predict it with?
- Brainstorm some possible strategies.
    - What features might be useful?
    - Do you need to collect more data?
- Try and find good input data
    - Randomly split data into:
        - training sample
        - testing sample
        - if enough data, a validation sample too
- Use features of or features built from the data that may help with prediction

Then to start:

- Start with a simple algorithm which can be implemented quickly.
    - Apply a machine learning algorithm
    - Estimate the parameters for the algorithm on your training data
- Test the simple algorithm on your validation data, evaluate the results
- Plot _learning curves_ to decide where things need work:
    - Do you need more data?
    - Do you need more features?
    - And so on.
- Error analysis: manually examine the examples in the validation set that your algorithm made errors on. Try to identify patterns in these errors. Are there categories of examples that the model is failing on in particular? Are there any other features that might help?

If you have an idea for a feature which may help, it's best to just test it out. This process is much easier if you have a single metric for your model's performance. You can use validation error or others, mentioned below.

When it comes to _skewed classes_ (or _high bias data_), metric selection is more nuanced.

For instance, say you have a dataset where only 0.5% of the data is in category 1 and the rest is in category 0. You run your model and find that it categorized 99.5% of the data correctly! But because of the skew in that data, your model could just be: classify each example in category 0, and it would achieve that accuracy.

Note that the convention is to set the rare class to 1 and the other class to 0. That is, we try to predict the rare class.

Instead, you may want to use _precision/recall_ as your evaluation metric.

|    |      1T        |       0T       |
|----|----------------|----------------|
| 1P | True positive  | False positive |
| 0P | False negative | True negative  |

Where 1T/0T indicates the actual class and 1P/0P indicates the predicted class.

_Precision_ is the number of true positives over the total number predicted as positive. That is, what fraction of the examples labeled as positive actually are positive?

$$
\frac{\text{true positives}}{\text{true positives} + \text{false positives}}
$$

_Recall_ is the number of true positives over the number of actual positives. That is, what fraction of the positive examples in the data were identified?

$$
\frac{\text{true positives}}{\text{true positives} + \text{false negatives}}
$$

So in the previous example, our simple classifier would have a recall of 0.

There is a trade-off between precision and recall.

Say you are using a logistic regression model for this classification task. Normally, the category threshold in logistic regression is 0.5, that is, predict class 1 if $h_{\theta}(x) \geq 0.5$ and predict class 0 if $h_{\theta}(x) < 0.5$.

But you may want to only classify an example as 1 if you're very confidence. So you may change the threshold to 0.9 to be stricter about your classifications. In this case, you would increase precision, but lower recall since the model may not be confident enough about some of the more ambiguous positive examples.

Conversely, you may want to lower the threshold to avoid false negatives, in which case recall increases, but precision decreases.

So how do you compare precision/recall values across algorithms to determine which is best? You can condense precision and recall into a single metric: the $F_1$ score (also just called the _F score_, which is the harmonic mean of the precision and recall):

$$
F_1 \text{score} = 2 \frac{PR}{P+R}
$$

Although more data doesn't always help, it generally does. Many algorithms perform significantly better as they get more and more data. Even relatively simple algorithms can outperform more sophisticated ones, solely on the basis of having more training data.

If your algorithm doesn't perform well, here are some things to try:

- Get more training examples (can help with high variance problems)
- Try smaller sets of features (can help with high variance problems)
- Try additional features (can help with high bias problems)
- Try adding polynomial features ($x_1^2, x_2^2, x_1 x_2$, etc) (can help with high bias problems)
- Try decreasing the regularization parameter $\lambda$ (can help with high bias problems)
- Try increasing the regularization parameter $\lambda$ (can help with high variance problems)

## Machine learning diagnostics

In machine learning, a diagnostic is:

> A test that you can run to gain insight [about] what is/isn't working with a learning algorithm, and gain guidance as to how best to improve its performance.

They take time to implement but can save you a lot of time by preventing you from going down fruitless paths.

### Learning curves

To generate a learning curve, you deliberately shrink the size of your training set and see how the training and validation errors change as you increase the training set size. This way you can see how your model improves (or doesn't, if something unexpected is happening) with more training data.

With smaller training sets, we expect the training error will be low because it will be easier to fit to less data. So as training set size grows, the average training set error is expected to grow.
Conversely, we expect the average validation error to decrease as the training set size increases.

If it seems like the training and validation error curves are flattening out at a high error as training set size increases, then you have a high bias problem. The curves flattening out indicates that getting more training data will not (by itself) help much.

On the other hand, high variance problems are indicated by a large gap between the training and validation error curves as training set size increases. You would also see a low training error. In this case, the curves are converging and more training data would help.

## Large Scale Machine Learning

### Map Reduce

You can distribute the workload across computers to reduce training time.

For example, say you're running batch gradient descent with $b=400$.

$$
\theta_j := \theta_j - \alpha \frac{1}{400} \sum^400_{i=1} (h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}_j
$$

You can divide up (map) your batch so that different machines calculate the error of a subset (e.g. with 4 machines, each machine takes 100 examples) and then those results are combined (reduced/summed) back on a single machine. So the summation term becomes distributed.

Map Reduce can be applied wherever your learning algorithm can be expressed as a summation over your training set.

Map Reduce also works across multiple cores on a single computer.

## Online (live/streaming) machine learning

### Distribution Drift

Say you train a model on some historical data and then deploy your model in a production setting where it is working with live data.

It is possible that the distribution of the live data starts to _drift_ from the distribution your model learned. This change may be due to factors in the real world that influence the data.

Ideally you will be able to detect this drift as it happens, so you know whether or not your model needs adjusting. A simple way to do it is to continually evaluate the model by computing some validation metric on the live data. If the distribution is stable, then this validation metric should remain stable; if the distribution drifts, the model starts to become a poor fit for the new incoming data, and the validation metric will worsen.

## References

- IFT 725 Review of fundamentals
- [Exploratory Data Analysis Course Notes](https://sux13.github.io/DataScienceSpCourseNotes/4_EXDATA/Exploratory_Data_Analysis_Course_Notes.pdf), Xing Su
- Johns Hopkins' Data Science Specialization (Coursera 2015)
- _Mining Massive Datasets_ (Coursera & Stanford, 2014). Jure Leskovec, Anand Rajaraman, Jeff Ullman.
- Andrew Ng's Coursera _Machine Learning_ course (2014)
- MIT 6.034 (Fall 2010): Artificial Intelligence. Patrick H. Winston.
- Computational Statistics II. Chris Fonnesbeck. SciPy 2015: <https://www.youtube.com/watch?v=heFaYLKVZY4> and <https://github.com/fonnesbeck/scipy2015_tutorial>
- Introduction to Artificial Intelligence (Udacity CS271): <https://www.udacity.com/wiki/cs271>, Peter Norvig and Sebastian Thrun.
- Evaluating Machine Learning Models. Alice Zheng. 2015.
- CS188: Artificial Intelligence. Dan Klein, Pieter Abbeel. University of California, Berkeley (edX).
- [_Deep Learning_](http://www.iro.umontreal.ca/~bengioy/dlbook). Yoshua Bengio, Ian J. Goodfellow, Aaron Courville. Book in preparation for MIT Press. 2015.
- CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Neural Networks Part 2: Setting up the Data and the Loss. Andrej Karpathy. <https://cs231n.github.io/neural-networks-1/>
- <https://cs231n.github.io/linear-classify/>
- Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle, Ryan P. Adams.
- Elements of Statistical Learning, 10th edition.
- Johns Hopkins' Practical Machine Learning course.
- <http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/>
- POLS 509: Hierarchical Linear Models. Justin Esarey. <https://www.youtube.com/watch?v=g_4z6o7XZbQ>
- <https://chronicles.mfglabs.com/learning-to-learn-or-the-advent-of-augmented-data-scientists-20873282e181>


