ways to evaluate your model's performance on (some of) your known data:

- hold out (just set aside some portion of the data for validation; this is less reliable if the amount of data is small such that the held out portion is very small)
- k-fold cross-validation (better than hold out for small datasets)
    - the training set is divided into $k$ folds
    - iteratively take $k-1$ folds for training and validate on the remaining fold
    - average the results
    - there is also "leave-one-out" cross-validation which is k-fold cross-validation where $k=n$ ($n$ is the number of datapoints)
- bootstrapping
    - new datasets are generated by sampling with replacement (uniformly at random) from the original dataset
    - then train on the bootstrapped dataset and validate on the unselected data
- jackknife resampling
    - essentially to leave-one-out cross-validation, since leave-one-out is basically sampling without replacement


## classification

in addition to AUC and accuracy and so on, you can use:

__Log-loss__: used when the classifier output is not a class but a probability, as is the case with logistic regression. It penalizes the classifier based on how far off it is, e.g. if it predicts 1 with probability of 0.51 but the correct class is 0, it is less "wrong" than if it had predicted class 1 with probability 0.95.

For a binary classifier, log-loss is computed:

$$
-\frac{1}{n} \sum_i^N y_i \log(p_i) + (1-y_i) \log(1 - p_i)
$$

Log-loss is the cross entropy b/w the distribution of the true labels and the predictions. It is related to relative entropy (that is, Kullback-Leilber divergence).

## validation vs testing

_Validation_ refers to the phase where you are tuning your model and its hyperparameters. Once you do that, you want to _test_ this model on a new set of data it has not seen yet (i.e. data which has not been used in cross-validation or bootstrapping or whatever method you used). This is to simulate the model's performance on completely new data and see how it does.

## hyperparameter search

### grid search

just searching through combinations of different hyperparameters and seeing which combination performs the best. Generally hyperparameters are searched over specific intervals or scales, depending on the particular hyperparameter. It may be 10, 20, 30, etc or 1e-5, 1e-4, 1e-3, etc. It is easy to parallelize but quite brute-force.

### random search

surprisingly, randomly sampling from the full grid often works just as well as a complete grid search, but in much less time.

Intuitively: if we want the hyperparameter combination leading to the top 5% of performance, then any random hyperparameter combination from the grid has a 5% chance of leading to that result. If we want to successfully find such a combination 95% of the time, how many random combinations do we need to run through?

If we take $n$ hyperparameter combinations, the probability that all $n$ are outside of this 5% of top combinations is $(1 - 0.05)^n$, so the probability that at least one is in the 5% is just $1 - (1-0.05)^n$. If we want to find one of these combinations 95% of the time, that is, we want the probability that at least one of them to be what we're looking for to be 95%, then we just set $1 - (1-0.05)^n = 0.95$, and thus $n \geq 60$, so we need to try only 60 random hyperparamter combinations at minimum to have a 95% chance of finding at least one hyperparameter combination that yields top 5% performance for the model.

## References

- Evaluating Machine Learning Models. Alice Zheng. 2015.
