Statistical models are useful for:

- describing the data
- estimated parameters of the process that may have generated the data
- make predictions

We assume that the data we have was generated by some underlying process (e.g. some distribution) which is parameterized in some way. Note that sometimes we may get mixtures of distributions (e.g. if you have a bimodal distribution); that is, the data is generated by multiple processes that we need to pull apart.

We don't know these parameters so we must _estimate_ them.

In _parametric_ inference we choose a distribution we think makes sense, then estimate parameters for that distribution such that it best fits the data. For example, if we choose a normal distribution, our parameters are the mean $\mu$ and the standard deviation $\sigma$.

Two commonly methods of estimation are:

1. _Method of moments_: choose parameters such that the sample moments (typically the sample mean and variance) match the theoretical moments of our chosen distribution.
2. _Maximum likelihood_: choose the parameters to maximize the likelihood, which measures how likely it is to observe our given sample.


### Maximum likelihood

We assume that our data $y = y_1, \dots, y_n$ is distributed to according to some distribution $P(Y_i = y_i|\theta)$; that is, it is parameterized by $\theta$.

We say that the total probability of each datapoint, $\prod_{i=1}^n P(y_i|\theta)$ is the _likelihood_ of our data - that is, how likely it is to have the data we have, given our estimate for $\theta$. We want to choose $\theta$ such that this probability - this likelihood - is maximized.

You often want to deal with log likelihoods instead, in which the _log likelihood_ is instead $\sum_{i=1}^n \log P(y_i|\theta)$

So then, to find the maximum likelihood, we take the derivative of the likelihood, set it to zero, and solve (like you would any optimization problem).

The result however may not have a closed-form solution (it cannot be solved analytically) and thus we must use numerical optimization.

There are a variety of numerical optimization methods, but the general idea is that an initial guess is made for the solution, then this guess is iteratively improved upon until it approximates the solution.

One such method is the Newton-Raphson algorithm.

#### Example

Say we have a coin and we're not sure it's fair or not.

We flip it 8 times ($N=8$) and get `TTTHHHHH`. What is the probability of tails ($P(T)$)?

We set $\pi = P(T)$ ($\pi$ is typical notation here).

A Bernoulli distribution is appropriate here:

$$
P(y_i) = \pi^{y_i}(1-\pi)^{1-y_i}
$$

Where $y_i \in \\{0,1\\}$ (i.e. is either H or T). So then $P(y_i=1) = \pi$ (tails) and $P(y_i=0) = 1-\pi$ (heads).

The likelihood of our data then is:

$$
P(\text{data}) = \prod_{i=1}^N P(y_i)
$$

If the number of tails is $n$ (and thus the number of heads is $N-n$), we can also write this as:

$$
P(\text{data}) = \pi^n (1-\pi)^{N-n}
$$

For this example, this works out to be:

$$
P(\text{data}) = \pi^3 (1-\pi)^5
$$

Then we want to find a value for $\pi$ which maximizes this equation.

As mentioned above, it's often easier to work with log likelihood instead, so we'd be working with:

$$
\log P(\text{data}) = 3\log\pi + 5\log(1-\pi)
$$

So then to get the maximum you'd solve for where the derivative is equal to 0.

You should get $\pi = 3/8$.


## Model Checking

How do you quantitatively evaluate how good your model is against the data?

You can compare the observed quantiles of the data to the quantiles from your estimated model.


## Kernel density estimates

Sometimes we don't want the parameters of our data's distribution, but just a smoothed representation of it. Kernel density estimation allows us to get this representation. It is a _nonparametric_ method because it makes no assumptions about the form of the underlying distribution (i.e. no assumptions about its parameters).

KDE places symmetric densities, generated by some kernel function, on each observation and then sums these densities. The kernel function determines the shape of these densities and the bandwidth determines their spread. Typically, a Gaussian kernel function is used, so the bandwidth is equivalent to the variance.

## Generalized Linear Models

Linear regression is a useful linear model but requires that some assumptions hold, such as that the error is normally distributed. There may be times where regression can't fit a good line - in such cases, a different generalized linear model (GLM, of which one is the normal linear regression) may be more appropriate.

### Logistic Regression

Logistic regression is also a GLM - you're fitting a line which models the probability of being in the positive class. We can use the Bernoulli distribution since it models events with two possible outcomes and is parameterized by only the probability of the positive outcome, $p$. Thus our line would look something like:

$$
p_i = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n + \epsilon
$$

But to represent a probability, $y$ values must be bound to $[0, 1]$. Currently, our model can be linear or polynomial and thus can output any continuous value. So we have to apply a transformation to constrain $y$; we do so by applying a __logit transformation__:

$$
logit(p) = \log(\frac{p}{1-p}) = x
$$

The $\frac{p}{1-p}$ term constraints the output to be positive.
The $\log$ operation constrains the values to $[0, 1]$.

The inverse of the logit transformation is:

$$
p = \frac{1}{1+\exp(-x)}
$$

So the model is now:

$$
logit(p) = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n \epsilon
$$

So the likelihood here is:

$$
L(y|p) = \prod^n_{i=1} p_i^{y_i}(1-p_i)^{1-y_i}
$$

And the log likelihood then is:

$$
l(y|p) = \sum^n_{i=1} y_i \log(p_i) + (1-y_i) \log(1-p_i)
$$


## Model Selection

You typically must choose from multiple models for a given dataset - how do you decide?

There are a few rules of thumb, such as simpler models (e.g. with less parameters) are preferred.

You can capture this rule of thumb using _Akaike's Information Criterion_ (AIC) which balances the fit of the model (in terms of likelihood) with the number of parameters needed for that fit. It is computed:

$$
AIC = n \log (\hat{\sigma}^2) + 2p
$$

Where $p$ is the number of parameters in the model and $\hat{\sigma}^2 = \frac{RSS}{n-p-1}$ (reminder: $RSS$ is the residual sum of squares).

The intuition here is that as the number of parameters increases, the residual sum of squares goes down, but the second term (which is a penalty) also increases.

AIC is a metric of information distance between a given model and some "true" model. Of course, we don't know what the true model is, so AIC values aren't interpretable in the absolute sense. But these values are useful relative to one another, because we get a relative measure of model quality.

So we want the model with the lowest AIC.


## References

- Computational Statistics II. Chris Fonnesbeck. SciPy 2015: <https://www.youtube.com/watch?v=heFaYLKVZY4> and <https://github.com/fonnesbeck/scipy2015_tutorial>
- Introduction to Artificial Intelligence (Udacity CS271): <https://www.udacity.com/wiki/cs271>, Peter Norvig and Sebastian Thrun.
