
$$
\DeclareMathOperator{\Img}{im}
$$


# Functions

Fundamentally, a function is a relationship (mapping) between the values of some set $X$ and some set $Y$ :

$$ f:X \to Y $$

![A function is a mapping between domains.](assets/function.svg textwidth=0.4)

A function can map a set to itself. For example, $f(x) = x^2$, also notated $f:x \mapsto x^2$, is the mapping of all real numbers to all real numbers, or $f:\mathbb R \to \mathbb R$.

The set you are mapping _from_ is called the __domain__.
The set that is being mapped _to_ is called the __codomain__.
The __range__ is the subset of the codomain which the function actually maps to (a function doesn't necessarily map to _every_ value in the codomain. But where it does, the range = the codomain).

Functions which map to $\mathbb R$ are known as __scalar-valued__ or __real-valued__ functions.

Functions which map to $\mathbb R^n$ where $n > 1$ are known as __vector-valued__ functions.


### Identity functions

An identity function maps something to itself:

$$
\begin{aligned}
I_X &: X \to X \\
I_X(a) &= a, \forall \, a \in X
\end{aligned}
$$


### The inverse of a function

Say we have a function $f: X \to Y$, where $f(a) = b$ for any $a \in X$.

We say $f$ is __invertible__ if and only if there exists a function $f^{-1}: Y \to X$ such that $f^{-1} \circ f = I_X$ and $f \circ f^{-1} = I_Y$.

The inverse of a function is _unique_, that is, it is _surjective_ and _injective_, that is, there is a unique $x$ for each $y$.

### Surjective functions

A __surjective__ function, also called "onto", is a function $f: X \to Y$ where, for every $y \in Y$ there exists _at least_ one $x \in X$ such that $f(x) = y$. That is, every $y$ has at least one corresponding $x$ value.

This can also be expressed as:

$$ \Img(f) = Y $$

since the image of the transformation encompasses the entire codomain $Y$. Because of that, this can also be represented as:

$$ \text{range}(f) = Y $$

### Injective functions

An __injective__ function, also called "one-to-one", is a function $f: X \to Y$ where, for every $y \in Y$, there exists _at most_ one $x \in X$ such that $f(x) = y$.

That is, not all $y$ necessarily has a corresponding $x$, but those that do only have _one_ corresponding $x$.

### Surjective & injective functions

A function can be both surjective and injective, which just means that for every $y \in Y$ there exists exactly one $x \in X$ such that $f(x) = y$, that is, every $y$ has exactly one corresponding $x$.

The inverse of a function is surjective and injective!

### Convex and non-convex functions

> A convex function is a continuous function whose value at the midpoint of every interval in its domain does not exceed the arithmetic mean of its values at the ends of the interval. [source](http://mathworld.wolfram.com/ConvexFunction.html).

A convex region is one in which any two points in the region can be joined by a straight line that does not leave the region.

Which is to say that a convex function has a minimum, and only one (and this is also the only position where the derivative is 0).

More formally, a function is convex if the second derivative is positive everywhere. A function can be convex on a range $[a,b]$ if its second derivative is positive everywhere in that range.

In higher dimensions, these derivatives aren't scalar values, so we instead define convexity if the Hessian $H$ (the matrix of second derivatives) is _positive semi-definite_ (notated $H \succeq 0$). It is _strictly_ convex if $H$ is _positive definite_ ($H \succ 0$).

![Convex and non-convex functions](assets/convex_nonconvex.svg)

### Transcendental functions

__Transcendental__ functions are those that are not polynomial, e.g. $\sin, \exp, \log, \text{etc}$.

### Logarithms

Logarithms have many useful properties, such as turning multiplication into addition:

$$
\log(xy) = \log(x) + \log(y)
$$

Note that $\log(x)$, without any base, typically implies the natural log, i.e. $\log_e(x)$, sometimes notated $\ln(x)$, which has the inverse $\exp(x)$, more commonly seen as $e^x$.


# Other useful concepts

## Solving analytically vs numerically

Often you may see a distinction made between solving a problem __analytically__ (sometimes __algebraeically__ is used) and solving a problem __numerically__.

Solving a problem analytically means you can exploit properties of the objects and equations, e.g. through methods from calculus and so on, and you can avoid substituting numerical values for the variables you are manipulating. If a problem may be solved analytically, the resulting solution is called a __closed form__ solution (or the __analytic__ solution) and is an exact solution.

Not all problems can be solved analytically; generally more complex mathematical models have no closed form solution. Such problems need to be _approximated_ numerically, which involves evaluating the equations many times by substituting different numerical values for variables. The result is an approximate (__numerical__) solution.

## Linear vs nonlinear models

You'll often see a caveat with algorithms that they only work for linear models or it touted that it applies to nonlinear models as well.

A __linear model__ is a model which takes the general form:

$$
y = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
$$

Note that this function does not need to produce a literal line (that is, the "linear" constraint does not apply to the predictor variables $x_1, \dots, x_n$). For instance, the function $y = x^2$ is linear. "Linear" rather refers to the parameters; i.e. the function must be "linear in the parameters", meaning that the parameters $\beta_0, \dots, \beta_n$ themselves must form a line (or its equivalent in whatever dimensional space you're working in).

A __nonlinear model__ includes parameters such as $\beta^2$ or $\beta_0 \beta_1$ (that is, multiple parameters in the same term) which is _not_ linear.

## Metrics

Say you have a bunch of random variables $X_i$ which take on values in a label space $V$. If $X_i$ and $X_j$ are connected by an edge, we want them to take on "similar" values.

How do we define "similar"?

We'll use a distance function $\mu: V \times V \to R^+$, which needs to satisfy:

- _reflexivity_: $\mu(v,v)=0$ for all $v$
- _symmetry_: $\mu(v_1,v_2)=\mu(v_2, v_1)$ for all $v_1, v_2$
- _triangle inequality_: $\mu(v_1, v_2) \leq \mu(v_1, v_3) + \mu(v_3, v_2)$ for all $v_1, v_2, v_3$

If all these are satisfied, we say that $\mu$ is a __metric__.

If only reflexivity and symmetry are satisfied, we have a __semi-metric__ instead.

So we can create a feature $f_{ij}(X_i, X_j) = \mu(X_i, X_j)$ and then this works out such that:

$$
\exp(- w_{ij} f_{ij} (X_i, X_j)), w_{ij} > 0
$$

that the lower the distance (metric), the higher the probability.

Metrics are very common features in machine learning.


## References

- [Convex Function](http://mathworld.wolfram.com/ConvexFunction.html). Weisstein, Eric W. Wolfram MathWorld.
