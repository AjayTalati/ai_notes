
# Neural Nets

When it comes down to it, a neural net is just a very sophisticated way of fitting a curve, so you shouldn't be dazzled just by the mention of it - it's not a magic solution. A lot of a neural net's effectiveness depends on, for instance, how you represent your input parameters.

## Biological basis

Artificial neural networks (ANNs) are based off of biological neural networks such as the human brain.
Neural networks are composed of _neurons_ which send signals to each other in response to certain inputs.

A single neuron takes in one or more inputs (via dendrites), processes it, and fires one output (via its axon).

![Source: <http://ulcar.uml.edu/~iag/CS/Intro-to-ANN.html>](assets/neuron.png)

Note that the term "unit" is often used instead of "neuron" when discussing artificial neural networks to dissociate these from the biological version - while there is some basis in biological neural networks, there are vast differences, so it is a deceit to present them as analogous.


## Perceptron: a simple artificial neuron

A _perceptron_, first described by Frank Rosenblatt in 1957, is an artificial neuron (a computational model of a biological neuron, first introduced in 1943 by Warren McCulloch and Walter Pitts).
It too has multiple inputs, processes them, and returns one output.

Each input has a weight associated with it.

In the simplest artificial neuron, a "binary" or "classic spin" neuron, the neuron "fires" an output of "1" if the weighted sum of these inputs is above some _threshold_, or "-1" if otherwise.

A single-layer perceptron can't learn XOR:

![XOR](assets/xor.svg)

A line can't be drawn to separate the $A$s from the $B$s; that is, this is not a linearly separable problem. Single-layer perceptrons cannot represent linearly inseparable functions.

### Activation functions

The function that determines the output is known as the _activation function_. In the binary/classic spin case, it might look like:

    weights = [...]
    inputs  = [...]
    sum_w = sum([weights[i] * inputs[i] for i in range(len(inputs))])

    def activate(sum_w, threshold):
        return 1 if sum_w > threshold else -1

Or:

$$
\begin{aligned}
  \mbox{output} & = & \left\{ \begin{array}{ll}
      -1 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
      1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
      \end{array} \right.
\end{aligned}
$$

Note that $w \cdot x = \sum_j w_j x_j$, so it can be notated as a dot product where the weights and inputs are vectors.

In some interpretations, the "binary" neuron returns "0" or "1" instead of "-1" or "1".

An activation function can generally be described as some function:

$$\text{output} = f(w \cdot x + b)$$

where $b$ is the bias (see below).

#### Common activation functions

![Sigmoid activation function](assets/sigmoid.svg)

A common activation function is the _sigmoid_ function, which takes input and squashes it to be in $[0,1]$, it has the form:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

However, the sigmoid activation function has some problems. If the activation yields values at the tails of 0 or 1, the gradient ends up being almost 0. In backpropagation, this local gradient is multiplied with the gradient of the node's output against the total error - if this local gradient is near 0, it "kills" the gradient preventing any signal from going further backwards in the network. For this reason, when using the sigmoid activation function you must be careful of how you initialize the weights - if they are too large, you will "saturate" the network and kill the gradient in this way.

Furthermore, sigmoid outputs are not zero-centered:

> This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. x>0 elementwise in f=wTx+b)), then the gradient on the weights w will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression f). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above. [source](https://cs231n.github.io/neural-networks-1/) (TODO revisit this/clarify)

![tanh activation function](assets/tanh.svg)

The _tanh_ activation function is another option; it squishes values to be in $[-1, 1]$. However, while its output is zero-centered, it suffers from the same activation saturation issue that the sigmoid does.

![ReLU activation function](assets/relu.svg)

The Rectified Linear Unit (ReLU) is $f(x) = \max(0,x)$, that is, it just thresholds at 0. Compared to the sigmoid/tanh functions, it converges with stochastic gradient descent quickly. Though there is not the same saturation issue as with the sigmoid/tanh functions, ReLUs can still "die" in a different sense - their weights can be updated such that the neuron never activates again, which causes the gradient through that neuron to be zero from then on, thus resulting in the same "killing" of the gradient as with sigmoid/tanh. In practice, lowering the learning rate can avoid this.

Leaky ReLUs are an attempt to fix this problem. Rather than outputting 0 when $x < 0$, there will instead be a small negative slope ($\sim 0.01$) when $x < 0$. However, it does not always work well.

There are also some units which defy the conventional activation form of $\text{output} = f(w \cdot x + b)$. One is the _Maxout_ neuron. It's function is $\max(w_1^Tx+b_1, w_2^Tx + b_2)$, which is a generalization of the ReLU and the leaky ReLU (both are special forms of Maxout). It has the benefits of ReLU but does not suffer the dying ReLU problem, but it's main drawback is that it doubles the number of parameters for each neuron (since there are two weight vectors and two bias units).

Karpathy suggests:

> Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of "dead" units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout. [source](https://cs231n.github.io/neural-networks-1/)


##### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Neural Networks Part 1: Setting up the Architecture. Andrej Karpathy. <https://cs231n.github.io/neural-networks-1/>

### Bias

The activation threshold is often expressed instead as a perceptron's _bias_ $b$, where `bias == -threshold`:

$$
\begin{aligned}
  \mbox{output} & = & \left\{ \begin{array}{ll}
      -1 & \mbox{if } w \cdot x + b \leq 0 \\
      1 & \mbox{if } w \cdot x + b > 0
      \end{array} \right.
\end{aligned}
$$

A lower bias means that a stronger signal is necessary for the perceptron to fire.

## Multilayered Perceptron (MLP, Feed-Forward ANN)

A _multilayered perceptron_ (MLP) is a simple neural network with an input layer, and output layer, and one or more intermediate layers of neurons.

When we describe the network in terms of layers as a "$N$-layer" neural network, we leave out the input layer (i.e. a 1-layer neural network has an input and an output layer, a 2-layer one has an input, a hidden, and an output layer.). ANNs may also be described by its number of nodes (units), or, more commonly, by the number of parameters in the entire network. (CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Neural Networks Part 1: Setting up the Architecture. Andrej Karpathy. <https://cs231n.github.io/neural-networks-1/>)



Two layer networks are limited to convex functions (TO DO how are layers counted? are input and outputs counted in Neural Computing: Theory and Practice?), whereas 3+ layer networks have no such limitation.

This model is often called _feed-forward_ because values go into the input layer and are fed into subsequent layers.

Different _learning algorithms_ (such as backpropagation, detailed below) can train such a network so that its weights are adjusted appropriately for a given task. It's worth emphasizing that the _structure_ of the network is distinct from the _learning algorithm_ which tunes its weights and biases.

### Training a perceptron via "backpropagation"

The most common algorithm for adjusting a perceptron's weights and biases is the _back-propagation of error_:

- The initial NN state does not matter; the weights and biases may be random
- Training data is input into the NN to the output neurons, in feed-forward style
- The error of the output is then propagated backwards (from the output layer back to the input layer).
- As the error is propagated, weights and biases are adjusted to minimize the remaining error between the actual and desired outputs
- The amount weights and biases are adjusted is determined by a _delta rule_ or _delta function_. This often involves some constant which manages the "momentum" of learning. This learning constant can help "jiggle" the network out of local optima, but you want to take care that it isn't set so high that the network will also jiggle out of the global optima. As a simple example:

        # LEARNING_CONSTANT is defined elsewhere
        def adjust_weight(weight, error, input):
            return weight + error * input * LEARNING_CONSTANT

    - In some cases, a _simulated annealing_ approach is used, where the learning constant may be tempered (made smaller, less drastic) as the network evolves, to avoid jittering the network out of the global optima.

#### Notes on backpropagation:
- a backpropagation NN (BPNN), like most machine learning models, is capable of overfitting (becoming tuned to a very specific training data; lacking any ability to generalize to new input data).
- BPNNs are very much a "black box" in that you don't really know what's going on in the intermediary layers
- training can be slow, but it usually isn't too bad on fast machines


## Backpropagation

Backpropagation is just the calculation of partial derivatives (the gradient) by moving backwards through the network (from output to input), accumulating them by applying the chain rule. "Backpropagation" is almost just a special term for the chain rule in the context of training neural networks. This is because a neural network can be thought of as a composition of functions, in which case to compute the derivative of the overall function, you just apply the chain rule for computing derivatives.

Side note: composition of functions as in, each layer represents a function taking in the inputs of the previous layer's output, e.g. if the previous layer is a function that outputs a vector, $g(x)$, then the next layer, if we call it a function $f$, is $f(g(x))$.

Backpropagation is key because it is how deep neural networks (multilayer perceptrons) learn. Backpropagation computes the gradient of the loss function with respect to the weights in the network (i.e. the derivatives of the loss function with respect to each weight in the network) in order to update the weights.

We compute the total error for the network on the training data and then want to know how a change in an individual weight within the network affects this total error (i.e. the result of our cost function), e.g. $\frac{\partial E_{\text{total}}}{\partial w_i}$.

Consider the following simple neural net:

![Simple neural network](assets/simplenn.svg)

Here's a single neuron expanded:

![Single sigmoid neuron](assets/single_neuron.svg)

Remember that a neuron processes its inputs by computing the dot product of its weights and inputs (i.e. the sum of its weight-input products) and then passes this resulting _net input_ into its activation function (in this case, it is the sigmoid function).

Say we have passed some training data through the network and computed the total error as $E_\text{total}$. To update the weight $w_{2,1}$, for example, we are looking for the partial derivative $\frac{\partial E_{\text{total}}}{\partial w_{2,1}}$, which by the chain rule is equal to:

$$
\frac{\partial E_{\text{total}}}{\partial w_{2,1}} = \frac{\partial E_\text{total}}{\partial o_{2,1}} \times \frac{\partial o_{2,1}}{\partial i_{2,1}} \times \frac{\partial i_{2,1}}{\partial w_{2,1}}
$$

Then we take this value and subtract it, multiplied by a learning rate $\eta$ (sometimes notated $\alpha$), from the current weight $w_{2,1}$ to get $w_{2,1}$'s updated weight, though updates are only actually applied after these update values have been computed for all of the network's weights.

If we wanted to calculate the update value for $w_{1,1}$, we do something similar:

$$
\frac{\partial E_{\text{total}}}{\partial w_{1,1}} = \frac{\partial E_\text{total}}{\partial o_{1,1}} \times \frac{\partial o_{1,1}}{\partial i_{1,1}} \times \frac{\partial i_{1,1}}{\partial w_{1,1}}
$$

Any activation function can be used with backprop, it just must be differentiable anywhere.

### References

- A Step by Step Backpropagation Example. Matt Mazur. March 17, 2015. <http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/>
- Gradient Descent with Backpropagation. July 31, 2015. <http://outlace.com/Beginner-Tutorial-Backpropagation/>

### Alternate explanation of the chain rule

(adapted from the CS231n notes cited below)

Refresher on derivatives: say you have a function $f(x,y,z)$. The derivative of $f$ with respect to $x$ is called a _partial derivative_, since it is only with respect to one of the variables, is notated $\frac{\partial f}{\partial x}$ and is just a function that tells you how much $f$ changes due to $x$ at any point. The gradient is just a vector of these partial derivatives, so that there is a partial derivative for each variable (i.e. here it would be a vector of the partial derivative of $f$ wrt $x$, and then wrt $y$, and then wrt $z$).

As a simple example, consider the function $f(x,y) = xy$. The derivatives here are just $\frac{\partial f}{\partial x} = y, \frac{\partial f}{\partial y} = x$ What does this mean? Well, take $\frac{\partial f}{\partial x} = y$. This means that, at any given point, increasing $x$ by a infinitesimal amount will change the output of the function by $y$ times the amount that $x$ changed. So if $y = -3$, then any small change in $x$ will decrease $f$ by that amount times $-3$.

Now consider the function $f(x,y,z) = (x+y)z$. We can derive this by declaring $q = x+y$ and then re-writing $f$ to be $f=qz$. We can compute the gradient of $f$ in this form (note that it is the same as $f(x,y) = xy$ from before): $\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q$. The gradient of $q$ is also simple: $\frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1$. We can combine these gradients to get the gradient of $f$ wrt to $x,y,z$ instead of wrt to $q,z$ as we have now. We can get the missing partial derivatives wrt to $x$ and $y$ by using the chain rule, which just requires that we multiply the appropriate paritals:

$$
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}, \frac{\partial f}{\partial y} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial y}
$$

In code (adapted from the CS231 notes cited below)

    # set some inputs
    x = -2; y = 5; z = -4

    # perform the forward pass
    q = x + y # q becomes 3
    f = q * z # f becomes -12

    # perform the backward pass (backpropagation) in reverse order:
    # first backprop through f = q * z
    dfdz = q # df/dz = q, so gradient on z becomes 3
    dfdq = z # df/dq = z, so gradient on q becomes -4
    dqdx = 1.
    dqdy = 1.
    # now backprop through q = x + y
    dfdx = dqdx * dfdq # dq/dx = 1. And the multiplication here is the chain rule!
    dfdy = dqdy * dfdq # dq/dy = 1

So essentially you can decompose any function into smaller, simpler functions, compute the gradients for those, then use the chain rule to aggregate them into the original function's gradient.

#### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Backpropagation, Intuitions. Andrej Karpathy. <https://cs231n.github.io/optimization-2/>


## Choosing the network configuration

The general structure of a neural network is input layer -> 0 or more hidden layers -> output layer.

Neural networks always have one input layer, and the size of that input layer is equal to the input dimensions (i.e. one node per feature), though sometimes you may have an additional bias node.

Neural networks always have one output layer, and the size of that output layer depends on what you're doing. For instance, if your neural network will be a regressor (i.e. for a regression problem), then you'd have a single output node (unless you're doing multivariate regression). Same for binary classification. However with softmax (more than just two classes) you have one output node per class label.

How do you know how many hidden layers to have? How do you know what size each hidden layer should be?

If your data is linearly separable, then you don't need any hidden layers (and you probably don't need a neural network either and a linear or generalized linear model may be plenty).

Neural networks with additional hidden layers become difficult to train; networks with multiple hidden layers are the subject of deep learning. For many problems, one hidden layer suffices, and you may not see any performance improvement from adding additional hidden layers.

A rule of thumb for deciding the size of the hidden layer is that the size should be between the size between the input size and output size (for example, the mean of their sizes).


## Recurrent neural networks (Feed-Back ANN)

A _recurrent neural network_ is a _feed-back_ neural network, that is, it is an ANN where the outputs of neurons are fed back into their inputs, continuing until stopped externally (or just continuing for a specific duration). They are less common than feedforward ANNs but have properties which may give them advantages over feedforward ANNs for certain problems.


Feedfoward networks are contrasted to recurrent networks (recurrent networks are just feedforward networks with some feedback loops).


## Sigmoid neurons (aka logistic neurons)

A sigmoid neuron is another artificial neuron, similar to a perceptron. However, while the perceptron has a binary output, the sigmoid neuron has a continuous output, $\sigma(w \cdot x+b)$, defined by a special activation function known as the _sigmoid function_ $\sigma$ (also known as the _logistic function_):

$$
\begin{aligned}
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
\end{aligned}
$$

which can also be written:

$$
\begin{aligned}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.
\end{aligned}
$$

Note that if $z = w \cdot x+b$ is a large positive number, then $e^{-z} \approx 0$ and thus $\sigma(z) \approx 1$. If $z$ is a large negative number, then $e^{-z} \rightarrow \infty$ and thus $\sigma(z) \approx 0$. So at these extremes, the sigmoid neuron behaves like a perceptron.

Here is the sigmoid function visualized:

![The sigmoid function](assets/sigmoid.svg)


Which is a smoothed out step function (which is how a perceptron operates):

![The step function](assets/step.svg)


Sigmoid neurons are useful because small changes in weights and biases will only produce small changes in output from a given neuron (rather than switching between binary output values, which may be too drastic).

## Cost/loss/objective/error functions

To determine the error, a cost (or "loss" or "objective" or "error") function is used. A common one is the _quadratic_ cost function, also known as _mean squared error_ (MSE):

$$
\begin{aligned}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\end{aligned}
$$

where

- $w$ = all weights in the network
- $b$ = all biases in the network
- $n$ = the number of training inputs
- $x$ = an input vector
- $a$ = the vector of outputs from the network when $x$ is input
- $y(x)$ = the desired output vector

When $C(w,b) \approx 0$, then $a \approx y(x)$.

You want to optimize (minimize) $C(w,b)$. See [Optimization](Optimization.md) for some approaches. A very common one is gradient descent.

Note that this also averages the costs by including $\frac{1}{n}$. Sometimes just the sum is used, e.g. in cases where the number of training inputs are not known or if more training data is being provided in real-time.

Note that this cost function takes into account _all_ training inputs. If using gradient descent, you may want to opt for the stochastic variant (stochastic gradient descent) in which the cost function considers only a random subset (a "mini-batch") of the training examples.

## RBF (neural) network

You can base your activation function off of [Radial Basis Functions (RBFs)](Radial Basis Functions (RBF).md):

$$
f(X) = \sum_{i=1}^N a_i p(||b_i X - c_i||)
$$

where

- $X$ = input vector of attributes
- $p$ = the RBF
- $c$ = vector center (peak) of the RBF
- $a$ = the vector coefficient/weight for each RBF
- $b$ = the vector coefficient/weight for each input attribute

### Radial Basis Functions (RBF)

A radial basis function (RBF) is a function which is:

- symmetric about its center, which is its peak (with a value of 1)
- can be in $n$ dimensions, but always returns a single scalar value $r$, the distance (usually Euclidean) b/w the input vector and the RBF's peak:

    $$r = ||x - x_i||$$

$\phi$ is used to denote a RBF.

#### Examples

##### 1D Gaussian RBF:

![1D Gaussian RBF](assets/gaussian_rbf.svg)

Defined as:

$$
\phi(r) = e^{-r^2}
$$

###### The Ricker Wavelet

![The Ricker Wavelet](assets/ricker_rbf.svg)

Defined as:

$$
\phi(r) = (1-r^2) \cdot e^{-\frac{r^2}{2}}
$$


## Deep neural networks

Many problems can be broken down into subproblems, each of which can be addressed by a separate neural network.

Say for example we want to know whether or not a face is in an image. We could break that down (_decompose_ it) into subproblems like:

- is there an eye?
- is there an ear?
- is there a nose?
- etc.

We could train a neural network on each of these subproblems. We could even break these subproblems further (e.g. "Is there an eyelash?", "Is there an iris?", etc) and train neural networks for those, and so on.

Then if we want to identify a face, we can aggregate these networks into a larger network.

This kind of multi-layered neural net is a _deep neural network_.

Multilayer nns must have nonlinear activation functions, otherwise they are equivalent to a single layer network aggregating its weights.

That is, a 2 layer network has weight vectors $W_1$ and $W_2$ and input X. The network computes $(XW_1)W_2$, which is equivalent to $X(W_1W_2)$, so the network is equivalent to a single layer network with weight vectors $W_1W_2$

Deep networks are harder to train - a simple stochastic gradient descent + backpropagation approach is not as effective or quick.

Newer techniques (2006 onward) based on SGD and backpropagation allow deeper (5-10 hidden layers) and larger networks to be effectively trained.

These deep networks can perform much better than shallow networks (networks with just one hidden layer) because they can embody a complex hierarchy of concepts.

## Sources

- _Crash Introduction to Artificial Neural Networks_, Ivan Galkin: <http://ulcar.uml.edu/~iag/CS/Intro-to-ANN.html>
- <http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html>
- _The Nature of Code_, Daniel Shiffman: <http://natureofcode.com/book/chapter-10-neural-networks></http:>
- _Neural Networks and Deep Learning_, Michael Nielsen: <http://neuralnetworksanddeeplearning.com>
- _Neural Networks_, Christos Stergiou & Dimitrios Siganos: <http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html>
- <http://www.ai-junkie.com/ann/evolved/nnt1.html>


## Convolutional Neural Networks (CNN)

A _convolutional neural network_ is one in which multiple copies of the same neuron (i.e. "copies" meaning they share the same weights) as used to form _convolutional layers_, which allows for many neurons which share parameters, thus keeping the number of parameters relatively small.

A convolutional layer computes features which then may be fed into further convolutional layers or to a fully-connected layer.

![A 2D convolutional neural net [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_01.png)

The figure shows a 2D convolutional neural net with inputs layer $x$ and two convolutional layers $A$ and $B$, consisting of copied neurons (different for each layer), which then feed into a fully-connected layer $F$.

A convolutional layer operates on a _window_ of inputs from its preceding layer; that is, it takes as input multiple neurons from the preceding layer, and these windows overlap. The figure below shows a 1D neural net with a window size of 3 for the $A$ convolutional layer.

![$A$ is a convolutional layer with a window size of 3 [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_02.png)

Often there may be _pooling layers_ between convolutional layers. The intuition here is that you may not need a fine-grained resolution of your inputs, so you can group them in some way into a single representative input. For instance, with image recognition - you may not care if something is one or two pixels off, so you might pool those features together.

A popular pooling layer is the _max-pooling layer_, which output the maximum of features in its window. This allows convolutional layers to look at larger sections of data (i.e. patches) and more invariant to small transformations of the data.

![A 1D CNN with a max-pooling layer [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_03.png)

In this figure there is a max-pooling layer between the convolutional layers $A$ and $B$. The maximum of each pair of $A$ fed into the max-pooling layer will be passed along to $B$.

Typically the units of $A$ are composed of single neurons in parallel, like below.

![Units of $A$ are usually just neurons in parallel [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_04.png)

But each unit in $A$ could also be a mini-network of its own (i.e. have multiple layers of neurons, or a "network-in-network"), as depicted below.

![Units of $A$ could be mini-NNs of their own [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_05.png)

### Convolutions

CNNs are based off of _convolutions_ from mathematics.

Say you have a ball which you kick once, then kick it again once it stops. On the first kick, the ball moves a distance $a$ with some probability; on the second kick, it moves a distance $b$ with some probability. You want the ball's total distance to be $c$.

What's the probability that you get the distance $c$?

Well say $c = 3$. How can $a + b = c$? You could have $a=1,b=2$, and the probability of this happening is $f(a) \cdot g(b)$. But this isn't the only combination of $a + b$ which leads to 3. You could have $a=0,b=3$ or $a=0.5,b=2.5$ and so on. To find the _total likelihood_ of the ball reaching a distance $c$, you have to sum the probabilities of each of these combinations:

$$
(f * g)(c) = \sum_{a+b=c} f(a) \cdot g(b)
$$

This is a convolution - specifically, it is the convolution of $f$ and $g$, evaluated at $c$.

We know that $b = c - a$ so we can re-write this as:

$$
(f * g)(c) = \sum_{a+b=c} f(a) \cdot g(c-a)
$$

Convolutions can be applied to any number of dimensions, using the same equation as above - it's just that $a, b, c$ become vectors.

For example, in two dimensions:

$$
(f * g)(c_1, c_2) = \sum_{\begin{aligned}a_1+b_1&=c_1\\a_2+b_2&=c_2\end{aligned}} f(a_1, a_2) \cdot g(b_1, b_2)
$$


For CNNs, convolutions allow us to describe which weights are identical across a layer.

A layer of neurons is typically described in aggregate with some weight matrix $W$, i.e.

$$
y = \sigma(Wx + b)
$$

In a convolutional layer, many of the weights are repeated in different positions:

![A simple CNN [Source](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)](assets/conv_06.png)

So a typical weight matrix for a layer might look like:

$$
W = \left[\begin{array}{ccccc}
W_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\
W_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\
W_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\
W_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\
...     &   ...   &   ...   &  ...    & ...\\
\end{array}\right]
$$

But for a convolutional layer, many weights repeat in different positions. And there are also many zeros since each unit in $A$ is not connected to every input:

$$
W = \left[\begin{array}{ccccc}
w_0 & w_1 &  0  &  0  & ...\\
 0  & w_0 & w_1 &  0  & ...\\
 0  &  0  & w_0 & w_1 & ...\\
 0  &  0  &  0  & w_0 & ...\\
... & ... & ... & ... & ...\\
\end{array}\right]
$$

Multiplying with weight matrix is the same thing as convolving with $[...0, w_1, w_0, 0...]$.

### Convolution kernels

CNNs learn a _convolution kernel_ and (for images) apply it to every pixel across the image:

![Convolution kernel example [source](https://developer.apple.com/library/ios/documentation/Performance/Conceptual/vImage/ConvolutionOperations/ConvolutionOperations.html)](assets/kernel_convolution.jpg)


### References

- <https://colah.github.io/posts/2014-07-Conv-Nets-Modular/>
- <https://colah.github.io/posts/2014-07-Understanding-Convolutions/>
- Composing Music with Recurrent Neural Networks. Daniel Johnson. August 3, 2015. <http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/>


## Recurrent Neural Networks (RNN)

With machine learning, data is typically represented in vector form. This works for certain kinds of data, such as numerical data, but not necessarily for other kinds of data, like text. We usually end up coercing text into some vector representation (e.g. TF-IDF) and end up losing much of its structure (such as the order of words). This is ok for some tasks (such as topic detection), but for many others we are throwing out important information. We could use bigrams or trigrams or so on to preserve some structure but this becomes unmanageably large (we end up with very high-dimension vectors).

Recurrent neural networks are able to take _sequences_ as input, i.e. iterate over a sequence, instead of fixed-size vectors, and as such can preserve the sequential structure of things like text and have a stronger concept of "context".

Basically, an RNN takes in each item in the sequence and updates its hidden representation based on that item and the previous hidden representation. If there is no previous hidden representation (i.e. we are looking at the first item in the sequence), we can initialize it as either all zeros or treat the initial hidden representation as another parameter to be learned.

The input item can be represented with _one-hot encoding_, i.e. each term is to a vector of all zeroes and one 1. For example, if we had the vocabulary $\\{\text{the}, \text{mad}, \text{cat} \\}$, the terms might be respectively represented as $[1,0,0], [0,1,0], [0,0,1]$.

Another way to represent these terms is with an _embedding matrix_, in which each term is mapped to some index of the matrix which points to some $n$-dimensional vector representation. So the RNN learns vector representations for each term.


 Convolutional neural networks, and feed-forward neural networks in general, treat an input the same no matter when they are given it. For RNNs, the hidden representation is like (short-term) "memory" for the network, so context is taken into account for inputs; that is, an input will be treated differently depending on what the previous input(s) was/were.

RNNs may incorporate __long short term memory__ (LSTM) units, which can handle longer-term context. These units have a few gates:

- _write_ - controls the amount of current input to be remembered
- _read_ - controls the amount of memory given as output to the next stage
- _erase_ - controls what part of the memory is erased or kept in the current time step

![A LSTM unit [source](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)](assets/lstm_unit.png)


### More on RNNs

In the most basic RNN, the hidden layer have two inputs: the input from the previous layer, and the layer's own output from the previous time step (so it loops back onto itself):

![Simple RNN network, with hidden nodes looping [source](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)](assets/rnn_1.png)

This simple network can be visualized over time as well:

![Simple RNN network, with hidden nodes looping over time [source](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)](assets/rnn_2.png)

Say we have a hidden layer $L_1$ of size 3 and another hidden layer $L_2$ of size 2. In a regular NN, the input to $L_2$ is of size 3 (because that's the output size of $L_1$). In an RNN, $L_2$ would have 3+2 inputs, 3 from $L_1$, and 2 from its own previous output.

This simple feedback mechanism offers a kind of short-term memory - the network "remembers" the output from the previous time step.

It also allows for variable-sized inputs and outputs - the inputs can be fed in one at a time and combined by this feedback mechanism.

This short-term memory may be _too_ short, however. Many RNNs incorporate _long short-term memory_ (LSTM) instead, where we have memory stored and passed through a longer number of steps. This memory is modified in each step, with something being added and something being removed at each step.

### Caveats

- RNNs train very slowly on CPUs; they train significantly faster on GPUs.
- Seems to perform less well than simpler models on small datasets; benefits don't show up until larger amounts of training data are used

### References

- General Sequence Learning using Recurrent Neural Networks, Alec Radford <https://www.youtube.com/watch?v=VINCQghQRuM>
- <http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/>
- Composing Music with Recurrent Neural Networks. Daniel Johnson. August 3, 2015. <http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/>


## Word Embeddings

A word embedding $W : \text{words} \to \mathbb R^n$ is a parameterized function that maps words to high-dimensional vectors (typically 200-500 dimensions).

This function is typically a lookup table parameterized by a matrix $\theta$, where each row represents a word. That is, the function is often $W_{\theta}(w_n) = \theta_n$. $\theta$ is initialized with random vectors for each word.

So given a task involving words, we want to learn $W$ so that we have good representations for each word.

You can visualize a word embedding space using t-SNE (a technique for visualizing high-dimensional data):

![Visualizing a word embedding space with t-SNE ([Turian et al (2010)](http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf))](assets/Turian-WordTSNE.png)

As you can see, words that are similar in meaning tend to be closer together. Intuitively this makes sense - if words have similar meaning, they are somewhat interchangeable, so we expect that their vectors be similar too.

We'll also see the vectors capture notions of analogy, for example "Paris" is to "France" as "Tokyo" is to "Japan". These kinds of analogies can be represented as vector addition: "Paris" - "France" + "Japan" = "Tokyo".

The best part is the neural network is not explicitly told to learn representations with these properties - it is just a side effect. This is one of the remarkable properties of neural networks - they learn good ways of representing the data more or less on their own.

And these representations can be portable. That is, maybe you learn $W$ for one natural language task, but you may be able to re-use $W$ for another natural language task (provided it's using a similar vocabulary). This practice is sometimes called "pretraining" or "transfer learning" or "multi-task learning".

You can also map multiple words to a single representation, e.g. if you are doing a multilingual task. For example, the English and French words for "dog" could map to the same representation since they mean the same thing (in which case we could call this a "bilingual word embedding").

Here's an example visualization of a Chinese and English bilingual word embedding:

![A Chinese and English word embedding ([Socher et al (2013a)](http://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf))](assets/Socher-BillingualTSNE.png)

You can even go a step further and learn image and word representations together, so that vectors representing images of horses are close to the vector for the word "horse".

Two main techniques for learning word embeddings are:

- CBOW: predicting the probability of context words given a word
- Skip-gram: predicting the probability of a word given context words

### Modular Neural Networks

So say we have trained a neural net which has learned our function $W$, and given a word input, it outputs us the word's high-dimensional vector representation.

We can re-use this network in a modular fashion so that we construct a larger neural net which can take a fixed-size set of words as input. For example, the following network takes in five words, from which we get their representations, which are then passed into another network $R$ to yield some output $s$.

![A modular neural network ([Bottou (2011)](http://arxiv.org/pdf/1102.1808v3.pdf))](assets/Bottou-WordSetup.png)

### Recursive Neural Networks

Using modular neural networks like above is limiting in the fact that we can only accept a fixed number of inputs.

We can get around this by adding an association module $A$, which takes two representations and merges them.

![Using association modules ([Bottou (2011)](http://arxiv.org/pdf/1102.1808v3.pdf))](assets/Bottou-Afold.png)

As you can see, it can take either a reputation from a word (via a $W$ module) or from a phrase (via another $A$ module).

We probably don't want to merge words linearly though. Instead we might want to group words in some way:

![A recursive neural network ([Bottou (2011)](http://arxiv.org/pdf/1102.1808v3.pdf))](assets/Bottou-Atree.png)

This kind of model is a "recursive neural network" (sometimes "tree-structured neural network") because it has modules feeding into modules of the same type.

## Nonlinear neural nets

In typical NNs, the architecture of the network is specified before hand and is static - neurons don't change connections. In a nonlinear neural net, however, the connections between neurons becomes dynamic, so that new connections may form and old connections may break. This is more like how the human brain operates. But so far at least, these are very complex and difficult to train.

## Transfer learning

The practice of transfer learning involves taking a neural net trained for another task and applying it to a different task. For instance, if using an image classification net trained for one classification task, you can use that same network for another, truncating the output layer, that is, take the vectors from the second-to-last layer and use those as feature vectors for other tasks.

## Recursive Neural Tensor Networks

~ to do ~

### References

- <https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/>
- <https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw>
i- <http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html>

## Choosing the network architecture

How do you decide how many layers to use, and what size each layer should be?

As the network grows in number of layers and size, the network _capacity_ increases, which is to say it is capable of representing more complex functions.

![More complex network, more complex functions ](assets/layer_sizes.svg)

Simpler networks have fewer local minima, but they are easier to converge to and tend to perform worse (they have higher loss). There is a great deal of variance across these local minima, so the outcome is quite sensitive to the random initialization - some times you land in a good local minima, sometimes not. More complex networks have more local minima, but they tend to perform better, and there is less variance across how these local minima perform.

Higher-capacity networks run a greater risk of overfitting, but this overfitting can be (preferably) mitigated by other methods such as L2 regularization, dropout, and input noise. So don't let overfitting be the sole reason for going with a simpler network if a larger one seems appropriate.

Here are regularization examples for the same data from the previous image, with the neural net for 20 hidden neurons:

![Regularization strength](assets/reg_strengths.svg)

As you can see, regularization is effective at counteracting overfitting.

### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Neural Networks Part 1: Setting up the Architecture. Andrej Karpathy. <https://cs231n.github.io/neural-networks-1/>

## Weight initialization

Given normalized data, we estimate that roughly half the weights will be negative and roughly half will be positive.

It may seem intuitive to initialize all weights to zero, but you should not, since this causes every neuron to have the same output, which causes them to have the same gradients during backpropagation, which causes them to all have the same parameter updates. Thus none of the neurons will differentiate.

So we can set each neuron's initial weights to be a random vector from a standard multidimensional normal distribution, scaled by some value, e.g. `0.001` so that they are kept very small, but still non-zero. This process is known as _symmetry breaking_. The random initializations allow the neurons to differentiate themselves during training.

Note that, however, small initial weights can be problematic for deep networks, since they may reduce the gradient signal that flows backwards by too much (in a weaker version of the gradient "killing" effect mentioned earlier).

As the number of inputs to a neuron grows, so too will its output's variance. This can be controlled for (calibrated) by scaling its weight vector by the square root of its "fan-in" (its number of inputs), so you should divide the standard multidimensional distribution sampled random vector by $\sqrt{n}$, where $n$ is the number of the neuron's inputs. For ReLUs, it is recommended you instead divide by $\sqrt{2/n}$. ([Karpathy's CS231n notes](https://cs231n.github.io/neural-networks-2/) provides more detail on why this is.)

An alternative to this fan-in scaling for the uncalibrated variances problem is _sparse initialization_, which is to set all weights to 0, and then break symmetry by randomly connecting every neuron to some fixed number (e.g. 10) of neurons below it by setting those weights to ones randomly sampled from the standard normal distribution like mentioned previously.

Biases are commonly initialized to be zero, though if using ReLUs, then you can set them to a small value like 0.01 so all the ReLUs fire at the start and are included in the gradient backpropagation update.

### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 2: Neural Networks Part 2: Setting up the Data and the Loss. Andrej Karpathy. <https://cs231n.github.io/neural-networks-2/>

## Regularization

Regularization techniques are used to prevent neural networks from overfitting.

### L2 Regularization

The most common form of regularization. Penalize the squared magnitude of all parameters (weights) as part of the objective function, i.e. we add $\sum \lambda w^2$ to the objective function. It is common to include $\frac{1}{2}$, i.e. use $\frac{1}{2} \sum \lambda w^2$, so the gradient of this term wrt to $w$ is just $\lambda w$ instead of $2 \lambda w$$. This avoids the network relying heavily on a few weights and encourages it to use all weights a little.

### L1 Regularization

Similar to L2 regularization, except that the term added to the objective function is $\sum \lambda |w|$. L1 regularization has the effect of causing weight vectors to become sparse, such that neurons only use a few of their inputs and ignore the rest as "noise". Generally L2 regularization is preferred to L1.

### Elastic net regularization

This is just the combination of L1 and L2 regularization, such that the term introduced to the objective function is $\sum \lambda_1 |w| + \lambda_2 w^2$.

### Max norm constraints

This involves setting an absolute upper bound on the magnitude of the weight vectors; that is, after updating the parameters/weights, clamp every weight vector so that it satisfies $||w||_2 < c$, where $c$ is some constant (the maximum magnitude).

### Dropout

Dropout is a regularization method which works well with the others mentioned so far (L1, L2, maxnorm). During training, we specify a probability $p$, and we only keep a neuron active with that probability $p$, otherwise we set its output to zero. If the neuron's output is set to 0, that has the effect of temporarily "removing" that neuron for that training iteration. This dropout is applied only at training time and applied per-layer (that is, it is applied after each layer, see the code example below).

One way to think about this is that, for each training step, a sub-network is sampled from the full network, and only those parameters are updated. Then on the next step, a different sub-sample is taken and updated, and so on.

![A network after dropout is applied to each layer in a training iteration [source](https://cs231n.github.io/neural-networks-2/)](assets/dropout.jpeg)

At test time, all neurons are active (i.e. we don't use dropout at test time). However, we must scale the activation functions by $p$ to maintain the same expected output for each neuron. Say $x$ is the output of a neuron without dropout. With dropout, the neuron's output has a chance $p$ of being set to 0, so its expected output becomes $px$ (more verbosely, it has $1-p$ chance of becoming 0, so its output is $px + (1-p)0$, which simplifies to $px$). Thus we must scale the outputs (i.e. the activation functions) by $p$ to keep the expected output consistent.

This scaling can be applied at training time, which is more efficient - this technique is called _inverted dropout_.

For comparison, here is an implementation of regular dropout and an implementation of inverted dropout (source from: <https://cs231n.github.io/neural-networks-2/>)

    # Dropout
    p = 0.5 # probability of keeping a unit active. higher = less dropout

    def train_step(X):
      """ X contains the data """

      # forward pass for example 3-layer neural network
      H1 = np.maximum(0, np.dot(W1, X) + b1)
      U1 = np.random.rand(*H1.shape) < p # first dropout mask
      H1 *= U1 # drop!
      H2 = np.maximum(0, np.dot(W2, H1) + b2)
      U2 = np.random.rand(*H2.shape) < p # second dropout mask
      H2 *= U2 # drop!
      out = np.dot(W3, H2) + b3

      # backward pass: compute gradients... (not shown)
      # perform parameter update... (not shown)

    def predict(X):
      # ensembled forward pass
      H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
      H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
      out = np.dot(W3, H2) + b3

    # Inverted dropout
    p = 0.5 # probability of keeping a unit active. higher = less dropout

    def train_step(X):
      # forward pass for example 3-layer neural network
      H1 = np.maximum(0, np.dot(W1, X) + b1)
      U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
      H1 *= U1 # drop!
      H2 = np.maximum(0, np.dot(W2, H1) + b2)
      U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
      H2 *= U2 # drop!
      out = np.dot(W3, H2) + b3

      # backward pass: compute gradients... (not shown)
      # perform parameter update... (not shown)

    def predict(X):
      # ensembled forward pass
      H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
      H2 = np.maximum(0, np.dot(W2, H1) + b2)
      out = np.dot(W3, H2) + b3

### Recommendations

> It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of $p=0.5$ is a reasonable default, but this can be tuned on validation data. <https://cs231n.github.io/neural-networks-2/>

### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 2: Neural Networks Part 2: Setting up the Data and the Loss. Andrej Karpathy. <https://cs231n.github.io/neural-networks-2/>

## Training

Start training with small, unequal weights to avoid _saturating_ the network w/ large weights. If all the weights start equal, the network won't learn anything.


## Hopfield Nets

### Recurrent NNs and stability

RNNs typically work by feeding in input, taking the output that results and feeding it in as new input, and so on until the output stabilizes. It is possible, however, that the output never stabilizes; such an RNN is said to be _unstable_.

In a binary Hopfield net, a simple threshold function is used as the activation function, such that each neuron outputs 0 or 1. The _state_ of the network is the set of values the network outputs, which effectively is a binary number, e.g. `1001` is a possible state for a network with four outputs. These outputs are fed back into the network as inputs which may change the state of the network - the hope is that eventually it stabilizes.

A Hopfield net with two outputs has four possible states: `00`, `01`, `10`, `11`, which can be conceptualized as the vertices of a two-dimensional polygon (a square). More generally, a Hopfield net with $n$ outputs will have $2^n$ states, and can be represented by an $n$-dimensional hypercube.

If we represent the weights of the network as a matrix $W$, then we can prove that the recurrent network is stable given that $w_{ii} = 0$ for all $i$ and that $w_{ij} = w_{ji}$ for all $j \neq i$; that is, if the matrix is symmetrical and as a zero diagonal. (see pp97) This can be similarly proven for continuous recurrent neural networks (i.e. with continuous activation functions).

However, it is possible for a recurrent network that does not satisfy these properties to be stable (which is to say, these are sufficient but not necessary requirements).

Note that a stable Hopfield net does not necessarily stabilize on the global minimum. Here, statistical training methods such as Boltzmann or Cauchy training can be used to guarantee a global minimum.

For the binary Hopfield net, the following strategy can be used:

$$
\begin{aligned}
E_k &= \text{NET}_k - \theta_k \\
p_k &= \frac{1}{1+\exp(-\delta \frac{E_k}{T})}
\end{aligned}
$$

where:

- $\text{NET}_k$ = the $\text{NET}$ output of neuron $k$
- $\theta_k$ = the threshold of neuron $k$
- $T$ = the artificial temperature

Set $T$ to a high value and set the neurons to an initial state by passing in an input vector. Then repeat:

1. For each neuron, set the state to one with probability $p_k$, otherwise, set to zero
2. Gradually reduce the artificial temperature and repeat step 1 until equilibrium is reached

---


backprop:

a single neuron:

$$
\text{OUT} = F(\text{NET})
$$

where:

- $\text{NET} = XW$ = net input to the activation function
- $X$ = input vector
- $W$ = weight vector
- $F$ = activation function
- $\text{OUT}$ = output of the neuron

if $F$ is the sigmoid function:

$$
\frac{\partial \text{OUT}}{\partial \text{NEXT}} = (\text{OUT})(1-\text{OUT})
$$

The total error of the network is: $\text{ERROR}_{\text{total}} = y_{\text{true}} - y_{\text{pred}}$.

We'll call the error for a particular layer $\text{ERROR}$.

For the penultimate layer (the layer just before the output), $\text{ERROR} = \text{ERROR}_{\text{total}}$.

For every preceding layer:

$$
\text{ERROR} = W_{L+1} \delta_{L+1}
$$

That is, it is equal to the product of the weight (before updating it) and error for the layer that comes after it (forward-wise).

The error for a particular neuron is:

$$
\delta = \frac{\partial \text{OUT}}{\partial \text{NEXT}}(\text{ERROR})
$$

The weight update for a particular neuron is:

$$
\Delta W = \delta \eta \text{OUT}
$$

So the new weight is just:

$$
W += \Delta W
$$

$\eta$ is the learning rate - it is preferable to $\alpha$ for notation because $\alpha$ can be used to represent a "momentum coefficient" or a "smoothing coefficient".

Momentum: keep track of the previous weight change and incorporate it in the next weight updates, such that:

$$
\Delta = \eta \delta \text{OUT} + \alpha [\Delta W_{t-1}]
$$

Where $\alpha \in [0,1]$ is the _momentum coefficient_, usually $0.9$, and the final bracketed term, $\Delta W_{t-1}$, is the weight change from the previous update.

But most often, momentum is not helpful.

Exponential smoothing:

$$
\Delta W = \eta(\alpha[\Delta W_{t-1}] + (1-\alpha)\delta \text{OUT})
$$

where $\alpha \in [0,1]$. Exponential smoothing can be better than momentum.

---

Using the sigmoid activation function is problematic because the outputs ($\text{OUT}$) have a good chance of being around 0, in which case the weight is not updated, because $\Delta W = \delta \eta \text{OUT}$, that is, if $\text{OUT} \approx 0$, then $\Delta W \approx 0$  Thus the network does not learn (the weights don't change). You can continue to use the sigmoid function if you adjust it:

$$
\text{OUT} = -\frac{1}{2} + \frac{1}{e^{-\text{NET}} + 1}
$$

but, as written elsewhere, you should just use a different activation function, like $\tanh$.

---

backprop is _not_ a guarantee of training at all nor of quick training.

possible issues:

- network paralysis: if the weights become very large, the neurons' $\text{OUT}$s may become very large, where the derivative of the activation function is very small, so weights are not really updated and get "stuck" at large values.
- local minima: statistical training methods can be used (such as simulated annealing), but increase training time
- step size: if it is too small, training is too slow, if it is too large, paralysis or instability (no convergence) are possible
- stability: that the network does not mess up its learning of something else to learn another thing. for instance, say it learns good weights for one input, but to learn good weights for another input, it "overwrites" or "forgets" what it learned about the prior input.

---

statistical training methods:

statistical (or "stochastic") training methods, contrasted with _deterministic_ training methods, involve some randomness to avoid local minima. They generally work by randomly leaving local minima to possibly find the global minimum. The severity of this randomness decreases over time so that a solution is "settled" upon (this gradual "cooling" of the randomness is the key part of _simulated annealing_).

simulated annealing applied as a training method to a neural network is called _Boltzmann training_ (neural networks trained in this way are called _Boltzmann machines_):

1. set $T$ (the artificial temperature) to a large value
2. apply inputs, calculate outputs and objective function
3. make random weight changes, recalculate network output and change in objective function
4a. if objective function improves, keep weight changes
4b. if the objective function worsens, accept the change according to the probability drawn from Boltzmann distribution, $P(c)$, select a random variable $r$ from a uniform distribution in $[0, 1]$; if $P(c) > r$, keep the change, otherwise, don't.

$$
P(c) = exp(\frac{-c}{kT})
$$

where:

- $c$ the change in the objective function
- $k$ a constant analogous to the Boltzmann's constant in simulated annealing, specific for the current problem
- $T$ the artificial temperature
- $P(c)$ the probability of the change $c$ in the objective function


Steps 3 and 4 are repeated for each of the weights in the network as $T$ is gradually decreased.

The random weight change can be selected in a few ways, but one is just choosing it from a Gaussian distribution, $P(w) = exp(\frac{-w^2}{T^2})$, where $P(w)$ is the probability of a weight change of size $w$ and $T$ is the artificial temperature. Then you can use Monte Carlo simulation to generate the actual weight change, $\Delta w$.

Boltzmann training uses the following cooling rate, which is necessary for convergence to a global minimum:

$$
T(t) = \frac{T_0}{log(1+t)}
$$

Where $T_0$ is the initial temperature, and $t$ is the artificial time.

The problem with Boltzmann training is that it can be very slow (the cooling rate as computed above is very low).

This can be resolved by using the Cauchy distribution instead of the Boltzmann distribution; the former has fatter tails so has a higher probability of selecting large step sizes. Thus the cooling rate can be much quicker:

$$
T(t) = \frac{T_0}{1+t}
$$

The Cauchy distribution is:

$$
P(x) = \frac{T(t)}{T(t)^2 + x^2}
$$

where $P(x)$ is the probability of a step of size $x$.

This can be integrated, which makes selecting random weights much easier:

$$
x_c = \rho T(t) \tan(P(x))
$$

Where $\rho$ is the learning rate coefficient and $x_c$ is the weight change.

Here we can just select a random number from a uniform distribution in $(-\frac{\pi}{2}, \frac{\pi}{2})$, then substitute this for $P(x)$ and solve for $x$ in the above, using the current temperature.

Cauchy training still may be slow so we can also use a method based on _artificial specific heat_ (in annealing, there are discrete energy levels where phase changes occur, at which abrupt changes in the "specific heat" occur). In the context of artificial neural networks, we define the (pseudo)specific heat to be the average rate of change of temperature with the objective function. The idea is that there are parts where the objective function is sensitive to small changes in temperature, where the average value of the objective function makes an abrupt change, so the temperature must be changed slowly here so as not to get stuck in a local minima. Where the average value of the objective function changes little with temperature, large changes in temperature can be used to quicken things.

Still, Cauchy training may be much slower than backprop, and can have issues of network paralysis (because it is possible to have very large random weight changes), esp. if a nonlinearity is used as the activation function (see the bit on network paralysis and the sigmoid function above).

Cauchy training may be combined with backprop to get the best of both worlds - it simply involves computing both the backprop and Cauchy weight updates and applying their weighted sum as the update. Then, the objective function's change is computed, and like with Cauchy training, if there is an improvement, the weight change is kept, otherwise, it is kept with a probability determined by the Boltzmann distribution.

The weighted sum of the individual weight updates is controlled by a coefficient $\eta$, such that the sum is $\eta [\alpha \Delta W_{t-1} + (1-\alpha)\delta \text{OUT}] + (1 - \eta) x_c$, so that if $\eta=0$, the training is purely Cauchy, and if $\eta=1$, it becomes purely backprop.

There is still the issue of the possibility of retaining a massive weight change due to the Cauchy distribution's infinite variance, which creates the possibility of network paralysis. The recommended approach here is to detect saturated neurons by looking at their $\text{OUT}$ values - if it is approaching the saturation point (positive or negative), apply some squashing function to its weights (note that this squashing function is not restricted to the range $[-1, 1]$ and in fact may work better with a larger range). This potently reduces large weights while only attenuating smaller ones, and maintains symmetry across weights.



## References

- _Neural Computing: Theory and Practice_ (1989). Philip D. Wasserman.
- MIT 6.034 (Fall 2010): Artificial Intelligence. Patrick H. Winston.
