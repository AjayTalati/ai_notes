
# Neural Nets

When it comes down to it, a neural net is just a very sophisticated way of fitting a curve, so you shouldn't be dazzled just by the mention of it - it's not a magic solution. A lot of a neural net's effectiveness depends on, for instance, how you represent your input parameters.

Neural networks with at least one hidden layer are __universal approximators__, which means that they can approximate _any_ (continuous) function. This approximation can be improved by increasing the number of hidden neurons in the network.

## Biological basis

Artificial neural networks (ANNs) are based off of biological neural networks such as the human brain.
Neural networks are composed of _neurons_ which send signals to each other in response to certain inputs.

A single neuron takes in one or more inputs (via dendrites), processes it, and fires one output (via its axon).

![Source: <http://ulcar.uml.edu/~iag/CS/Intro-to-ANN.html>](assets/neuron.png)

Note that the term "unit" is often used instead of "neuron" when discussing artificial neural networks to dissociate these from the biological version - while there is some basis in biological neural networks, there are vast differences, so it is a deceit to present them as analogous.


## Perceptron: a simple artificial neuron

A _perceptron_, first described by Frank Rosenblatt in 1957, is an artificial neuron (a computational model of a biological neuron, first introduced in 1943 by Warren McCulloch and Walter Pitts).
It too has multiple inputs, processes them, and returns one output.

Each input has a weight associated with it.

In the simplest artificial neuron, a "binary" or "classic spin" neuron, the neuron "fires" an output of "1" if the weighted sum of these inputs is above some _threshold_, or "-1" if otherwise.

A single-layer perceptron can't learn XOR:

![XOR](assets/xor.svg)

A line can't be drawn to separate the $A$s from the $B$s; that is, this is not a linearly separable problem. Single-layer perceptrons cannot represent linearly inseparable functions.

### Activation functions

The function that determines the output is known as the _activation function_. In the binary/classic spin case, it might look like:

    weights = [...]
    inputs  = [...]
    sum_w = sum([weights[i] * inputs[i] for i in range(len(inputs))])

    def activate(sum_w, threshold):
        return 1 if sum_w > threshold else -1

Or:

$$
\begin{aligned}
  \mbox{output} & = & \left\{ \begin{array}{ll}
      -1 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
      1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
      \end{array} \right.
\end{aligned}
$$

Note that $w \cdot x = \sum_j w_j x_j$, so it can be notated as a dot product where the weights and inputs are vectors.

In some interpretations, the "binary" neuron returns "0" or "1" instead of "-1" or "1".

An activation function can generally be described as some function:

$$\text{output} = f(w \cdot x + b)$$

where $b$ is the bias (see below).

#### Common activation functions

![Sigmoid activation function](assets/sigmoid.svg)

A common activation function is the _sigmoid_ function, which takes input and squashes it to be in $[0,1]$, it has the form:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

However, the sigmoid activation function has some problems. If the activation yields values at the tails of 0 or 1, the gradient ends up being almost 0. In backpropagation, this local gradient is multiplied with the gradient of the node's output against the total error - if this local gradient is near 0, it "kills" the gradient preventing any signal from going further backwards in the network. For this reason, when using the sigmoid activation function you must be careful of how you initialize the weights - if they are too large, you will "saturate" the network and kill the gradient in this way.

Furthermore, sigmoid outputs are not zero-centered:

> This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. x>0 elementwise in f=wTx+b)), then the gradient on the weights w will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression f). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above. [source](https://cs231n.github.io/neural-networks-1/) (TODO revisit this/clarify)

![tanh activation function](assets/tanh.svg)

The _tanh_ activation function is another option; it squishes values to be in $[-1, 1]$. However, while its output is zero-centered, it suffers from the same activation saturation issue that the sigmoid does.

$$
\tanh(x)  \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$

Note that $\tanh$ is really just a rescaled sigmoid function:

$$
\sigma(x) = \frac{1 + \tanh(\frac{x}{2})}{2}
$$


![ReLU activation function](assets/relu.svg)

The Rectified Linear Unit (ReLU) is $f(x) = \max(0,x)$, that is, it just thresholds at 0. Compared to the sigmoid/tanh functions, it converges with stochastic gradient descent quickly. Though there is not the same saturation issue as with the sigmoid/tanh functions, ReLUs can still "die" in a different sense - their weights can be updated such that the neuron never activates again, which causes the gradient through that neuron to be zero from then on, thus resulting in the same "killing" of the gradient as with sigmoid/tanh. In practice, lowering the learning rate can avoid this.

Leaky ReLUs are an attempt to fix this problem. Rather than outputting 0 when $x < 0$, there will instead be a small negative slope ($\sim 0.01$) when $x < 0$. However, it does not always work well.

There are also some units which defy the conventional activation form of $\text{output} = f(w \cdot x + b)$. One is the _Maxout_ neuron. It's function is $\max(w_1^Tx+b_1, w_2^Tx + b_2)$, which is a generalization of the ReLU and the leaky ReLU (both are special forms of Maxout). It has the benefits of ReLU but does not suffer the dying ReLU problem, but it's main drawback is that it doubles the number of parameters for each neuron (since there are two weight vectors and two bias units).

Karpathy suggests:

> Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of "dead" units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout. [source](https://cs231n.github.io/neural-networks-1/)


##### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Neural Networks Part 1: Setting up the Architecture. Andrej Karpathy. <https://cs231n.github.io/neural-networks-1/>

### Bias

The activation threshold is often expressed instead as a perceptron's _bias_ $b$, where `bias == -threshold`:

$$
\begin{aligned}
  \mbox{output} & = & \left\{ \begin{array}{ll}
      -1 & \mbox{if } w \cdot x + b \leq 0 \\
      1 & \mbox{if } w \cdot x + b > 0
      \end{array} \right.
\end{aligned}
$$

A lower bias means that a stronger signal is necessary for the perceptron to fire.

## Multilayered Perceptron (MLP, Feed-Forward ANN)

A _multilayered perceptron_ (MLP) is a simple neural network with an input layer, and output layer, and one or more intermediate layers of neurons.

When we describe the network in terms of layers as a "$N$-layer" neural network, we leave out the input layer (i.e. a 1-layer neural network has an input and an output layer, a 2-layer one has an input, a hidden, and an output layer.). ANNs may also be described by its number of nodes (units), or, more commonly, by the number of parameters in the entire network. (CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Neural Networks Part 1: Setting up the Architecture. Andrej Karpathy. <https://cs231n.github.io/neural-networks-1/>)



Two layer networks are limited to convex functions (TODO how are layers counted? are input and outputs counted in Neural Computing: Theory and Practice?), whereas 3+ layer networks have no such limitation.

This model is often called _feed-forward_ because values go into the input layer and are fed into subsequent layers.

Different _learning algorithms_ (such as backpropagation, detailed below) can train such a network so that its weights are adjusted appropriately for a given task. It's worth emphasizing that the _structure_ of the network is distinct from the _learning algorithm_ which tunes its weights and biases.

### Training a perceptron via "backpropagation"

The most common algorithm for adjusting a perceptron's weights and biases is the _back-propagation of error_:

- The initial NN state does not matter; the weights and biases may be random
- Training data is input into the NN to the output neurons, in feed-forward style
- The error of the output is then propagated backwards (from the output layer back to the input layer).
- As the error is propagated, weights and biases are adjusted to minimize the remaining error between the actual and desired outputs
- The amount weights and biases are adjusted is determined by a _delta rule_ or _delta function_. This often involves some constant which manages the "momentum" of learning. This learning constant can help "jiggle" the network out of local optima, but you want to take care that it isn't set so high that the network will also jiggle out of the global optima. As a simple example:

        # LEARNING_CONSTANT is defined elsewhere
        def adjust_weight(weight, error, input):
            return weight + error * input * LEARNING_CONSTANT

    - In some cases, a _simulated annealing_ approach is used, where the learning constant may be tempered (made smaller, less drastic) as the network evolves, to avoid jittering the network out of the global optima.

#### Notes on backpropagation:
- a backpropagation NN (BPNN), like most machine learning models, is capable of overfitting (becoming tuned to a very specific training data; lacking any ability to generalize to new input data).
- BPNNs are very much a "black box" in that you don't really know what's going on in the intermediary layers
- training can be slow, but it usually isn't too bad on fast machines


## Backpropagation

Backpropagation is just the calculation of partial derivatives (the gradient) by moving backwards through the network (from output to input), accumulating them by applying the chain rule. "Backpropagation" is almost just a special term for the chain rule in the context of training neural networks. This is because a neural network can be thought of as a composition of functions, in which case to compute the derivative of the overall function, you just apply the chain rule for computing derivatives.

Side note: composition of functions as in, each layer represents a function taking in the inputs of the previous layer's output, e.g. if the previous layer is a function that outputs a vector, $g(x)$, then the next layer, if we call it a function $f$, is $f(g(x))$.

Backpropagation is key because it is how deep neural networks (multilayer perceptrons) learn. Backpropagation computes the gradient of the loss function with respect to the weights in the network (i.e. the derivatives of the loss function with respect to each weight in the network) in order to update the weights.

We compute the total error for the network on the training data and then want to know how a change in an individual weight within the network affects this total error (i.e. the result of our cost function), e.g. $\frac{\partial E_{\text{total}}}{\partial w_i}$.

Consider the following simple neural net:

![Simple neural network](assets/simplenn.svg)

Here's a single neuron expanded:

![Single sigmoid neuron](assets/single_neuron.svg)

Remember that a neuron processes its inputs by computing the dot product of its weights and inputs (i.e. the sum of its weight-input products) and then passes this resulting _net input_ into its activation function (in this case, it is the sigmoid function).

Say we have passed some training data through the network and computed the total error as $E_\text{total}$. To update the weight $w_{2,1}$, for example, we are looking for the partial derivative $\frac{\partial E_{\text{total}}}{\partial w_{2,1}}$, which by the chain rule is equal to:

$$
\frac{\partial E_{\text{total}}}{\partial w_{2,1}} = \frac{\partial E_\text{total}}{\partial o_{2,1}} \times \frac{\partial o_{2,1}}{\partial i_{2,1}} \times \frac{\partial i_{2,1}}{\partial w_{2,1}}
$$

Then we take this value and subtract it, multiplied by a learning rate $\eta$ (sometimes notated $\alpha$), from the current weight $w_{2,1}$ to get $w_{2,1}$'s updated weight, though updates are only actually applied after these update values have been computed for all of the network's weights.

If we wanted to calculate the update value for $w_{1,1}$, we do something similar:

$$
\frac{\partial E_{\text{total}}}{\partial w_{1,1}} = \frac{\partial E_\text{total}}{\partial o_{1,1}} \times \frac{\partial o_{1,1}}{\partial i_{1,1}} \times \frac{\partial i_{1,1}}{\partial w_{1,1}}
$$

Any activation function can be used with backprop, it just must be differentiable anywhere.

### References

- A Step by Step Backpropagation Example. Matt Mazur. March 17, 2015. <http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/>
- Gradient Descent with Backpropagation. July 31, 2015. <http://outlace.com/Beginner-Tutorial-Backpropagation/>

### Alternate explanation of the chain rule

(adapted from the CS231n notes cited below)

Refresher on derivatives: say you have a function $f(x,y,z)$. The derivative of $f$ with respect to $x$ is called a _partial derivative_, since it is only with respect to one of the variables, is notated $\frac{\partial f}{\partial x}$ and is just a function that tells you how much $f$ changes due to $x$ at any point. The gradient is just a vector of these partial derivatives, so that there is a partial derivative for each variable (i.e. here it would be a vector of the partial derivative of $f$ wrt $x$, and then wrt $y$, and then wrt $z$).

As a simple example, consider the function $f(x,y) = xy$. The derivatives here are just $\frac{\partial f}{\partial x} = y, \frac{\partial f}{\partial y} = x$ What does this mean? Well, take $\frac{\partial f}{\partial x} = y$. This means that, at any given point, increasing $x$ by a infinitesimal amount will change the output of the function by $y$ times the amount that $x$ changed. So if $y = -3$, then any small change in $x$ will decrease $f$ by that amount times $-3$.

Now consider the function $f(x,y,z) = (x+y)z$. We can derive this by declaring $q = x+y$ and then re-writing $f$ to be $f=qz$. We can compute the gradient of $f$ in this form (note that it is the same as $f(x,y) = xy$ from before): $\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q$. The gradient of $q$ is also simple: $\frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1$. We can combine these gradients to get the gradient of $f$ wrt to $x,y,z$ instead of wrt to $q,z$ as we have now. We can get the missing partial derivatives wrt to $x$ and $y$ by using the chain rule, which just requires that we multiply the appropriate paritals:

$$
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}, \frac{\partial f}{\partial y} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial y}
$$

In code (adapted from the CS231 notes cited below)

    # set some inputs
    x = -2; y = 5; z = -4

    # perform the forward pass
    q = x + y # q becomes 3
    f = q * z # f becomes -12

    # perform the backward pass (backpropagation) in reverse order:
    # first backprop through f = q * z
    dfdz = q # df/dz = q, so gradient on z becomes 3
    dfdq = z # df/dq = z, so gradient on q becomes -4
    dqdx = 1.
    dqdy = 1.
    # now backprop through q = x + y
    dfdx = dqdx * dfdq # dq/dx = 1. And the multiplication here is the chain rule!
    dfdy = dqdy * dfdq # dq/dy = 1

So essentially you can decompose any function into smaller, simpler functions, compute the gradients for those, then use the chain rule to aggregate them into the original function's gradient.

#### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Backpropagation, Intuitions. Andrej Karpathy. <https://cs231n.github.io/optimization-2/>


## Choosing the network configuration

The general structure of a neural network is input layer -> 0 or more hidden layers -> output layer.

Neural networks always have one input layer, and the size of that input layer is equal to the input dimensions (i.e. one node per feature), though sometimes you may have an additional bias node.

Neural networks always have one output layer, and the size of that output layer depends on what you're doing. For instance, if your neural network will be a regressor (i.e. for a regression problem), then you'd have a single output node (unless you're doing multivariate regression). Same for binary classification. However with softmax (more than just two classes) you have one output node per class label, with each node outputting the probability the input is of the class associated with the node.

How do you know how many hidden layers to have? How do you know what size each hidden layer should be?

If your data is linearly separable, then you don't need any hidden layers (and you probably don't need a neural network either and a linear or generalized linear model may be plenty).

Neural networks with additional hidden layers become difficult to train; networks with multiple hidden layers are the subject of deep learning. For many problems, one hidden layer suffices, and you may not see any performance improvement from adding additional hidden layers.

A rule of thumb for deciding the size of the hidden layer is that the size should be between the size between the input size and output size (for example, the mean of their sizes).


## Recurrent neural networks (Feed-Back ANN)

A _recurrent neural network_ is a _feed-back_ neural network, that is, it is an ANN where the outputs of neurons are fed back into their inputs, continuing until stopped externally (or just continuing for a specific duration). They are less common than feedforward ANNs but have properties which may give them advantages over feedforward ANNs for certain problems.


Feedfoward networks are contrasted to recurrent networks (recurrent networks are just feedforward networks with some feedback loops).


## Sigmoid neurons (aka logistic neurons)

A sigmoid neuron is another artificial neuron, similar to a perceptron. However, while the perceptron has a binary output, the sigmoid neuron has a continuous output, $\sigma(w \cdot x+b)$, defined by a special activation function known as the _sigmoid function_ $\sigma$ (also known as the _logistic function_):

$$
\begin{aligned}
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
\end{aligned}
$$

which can also be written:

$$
\begin{aligned}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.
\end{aligned}
$$

Note that if $z = w \cdot x+b$ is a large positive number, then $e^{-z} \approx 0$ and thus $\sigma(z) \approx 1$. If $z$ is a large negative number, then $e^{-z} \rightarrow \infty$ and thus $\sigma(z) \approx 0$. So at these extremes, the sigmoid neuron behaves like a perceptron.

Here is the sigmoid function visualized:

![The sigmoid function](assets/sigmoid.svg)


Which is a smoothed out step function (which is how a perceptron operates):

![The step function](assets/step.svg)


Sigmoid neurons are useful because small changes in weights and biases will only produce small changes in output from a given neuron (rather than switching between binary output values as is the case with the step function, which is typically too drastic).

## Cost/loss/objective/error functions

To determine the error, a cost (or "loss" or "objective" or "error") function is used. A common one is the _quadratic_ cost function, also known as _mean squared error_ (MSE):

$$
\begin{aligned}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\end{aligned}
$$

where

- $w$ = all weights in the network
- $b$ = all biases in the network
- $n$ = the number of training inputs
- $x$ = an input vector
- $a$ = the vector of outputs from the network when $x$ is input
- $y(x)$ = the desired output vector

When $C(w,b) \approx 0$, then $a \approx y(x)$.

You want to optimize (minimize) $C(w,b)$. See [Optimization](Optimization.md) for some approaches. A very common one is gradient descent.

Note that this also averages the costs by including $\frac{1}{n}$. Sometimes just the sum is used, e.g. in cases where the number of training inputs are not known or if more training data is being provided in real-time.

Note that this cost function takes into account _all_ training inputs. If using gradient descent, you may want to opt for the stochastic variant (stochastic gradient descent) in which the cost function considers only a random subset (a "mini-batch") of the training examples.

## RBF (neural) network

You can base your activation function off of [Radial Basis Functions (RBFs)](Radial Basis Functions (RBF).md):

$$
f(X) = \sum_{i=1}^N a_i p(||b_i X - c_i||)
$$

where

- $X$ = input vector of attributes
- $p$ = the RBF
- $c$ = vector center (peak) of the RBF
- $a$ = the vector coefficient/weight for each RBF
- $b$ = the vector coefficient/weight for each input attribute

### Radial Basis Functions (RBF)

A radial basis function (RBF) is a function which is:

- symmetric about its center, which is its peak (with a value of 1)
- can be in $n$ dimensions, but always returns a single scalar value $r$, the distance (usually Euclidean) b/w the input vector and the RBF's peak:

    $$r = ||x - x_i||$$

$\phi$ is used to denote a RBF.

#### Examples

##### 1D Gaussian RBF:

![1D Gaussian RBF](assets/gaussian_rbf.svg)

Defined as:

$$
\phi(r) = e^{-r^2}
$$

###### The Ricker Wavelet

![The Ricker Wavelet](assets/ricker_rbf.svg)

Defined as:

$$
\phi(r) = (1-r^2) \cdot e^{-\frac{r^2}{2}}
$$


## Deep neural networks

Many problems can be broken down into subproblems, each of which can be addressed by a separate neural network.

Say for example we want to know whether or not a face is in an image. We could break that down (_decompose_ it) into subproblems like:

- is there an eye?
- is there an ear?
- is there a nose?
- etc.

We could train a neural network on each of these subproblems. We could even break these subproblems further (e.g. "Is there an eyelash?", "Is there an iris?", etc) and train neural networks for those, and so on.

Then if we want to identify a face, we can aggregate these networks into a larger network.

This kind of multi-layered neural net is a _deep neural network_.

Multilayer nns must have nonlinear activation functions, otherwise they are equivalent to a single layer network aggregating its weights.

That is, a 2 layer network has weight vectors $W_1$ and $W_2$ and input X. The network computes $(XW_1)W_2$, which is equivalent to $X(W_1W_2)$, so the network is equivalent to a single layer network with weight vectors $W_1W_2$

Deep networks are harder to train - a simple stochastic gradient descent + backpropagation approach is not as effective or quick.

Newer techniques (2006 onward) based on SGD and backpropagation allow deeper (5-10 hidden layers) and larger networks to be effectively trained.

These deep networks can perform much better than shallow networks (networks with just one hidden layer) because they can embody a complex hierarchy of concepts.


## Convolutional Neural Networks (CNN)

A _convolutional neural network_ is one in which multiple copies of the same neuron (i.e. "copies" meaning they share the same weights) as used to form _convolutional layers_, which allows for many neurons which share parameters, thus keeping the number of parameters relatively small.

A convolutional layer computes features which then may be fed into further convolutional layers or to a fully-connected layer (known as __affine__ layers).

![A 2D convolutional neural net [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_01.png)

The figure shows a 2D convolutional neural net with inputs layer $x$ and two convolutional layers $A$ and $B$, consisting of copied neurons (different for each layer), which then feed into a fully-connected layer $F$.

A convolutional layer operates on a _window_ of inputs from its preceding layer; that is, it takes as input multiple neurons from the preceding layer, and these windows overlap. The figure below shows a 1D neural net with a window size of 3 for the $A$ convolutional layer.

![$A$ is a convolutional layer with a window size of 3 [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_02.png)

Often there may be _pooling layers_ between convolutional layers. The intuition here is that you may not need a fine-grained resolution of your inputs, so you can group them in some way into a single representative input. For instance, with image recognition - you may not care if something is one or two pixels off, so you might pool those features together.

A popular pooling layer is the _max-pooling layer_, which output the maximum of features in its window. This allows convolutional layers to look at larger sections of data (i.e. patches) and more invariant to small transformations of the data.

![A 1D CNN with a max-pooling layer [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_03.png)

In this figure there is a max-pooling layer between the convolutional layers $A$ and $B$. The maximum of each pair of $A$ fed into the max-pooling layer will be passed along to $B$.

Typically the units of $A$ are composed of single neurons in parallel, like below.

![Units of $A$ are usually just neurons in parallel [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_04.png)

But each unit in $A$ could also be a mini-network of its own (i.e. have multiple layers of neurons, or a "network-in-network"), as depicted below.

![Units of $A$ could be mini-NNs of their own [Source](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)](assets/conv_05.png)

### Convolutions

CNNs are based off of _convolutions_ from mathematics.

Say you have a ball which you kick once, then kick it again once it stops. On the first kick, the ball moves a distance $a$ with some probability; on the second kick, it moves a distance $b$ with some probability. You want the ball's total distance to be $c$.

What's the probability that you get the distance $c$?

Well say $c = 3$. How can $a + b = c$? You could have $a=1,b=2$, and the probability of this happening is $f(a) \cdot g(b)$. But this isn't the only combination of $a + b$ which leads to 3. You could have $a=0,b=3$ or $a=0.5,b=2.5$ and so on. To find the _total likelihood_ of the ball reaching a distance $c$, you have to sum the probabilities of each of these combinations:

$$
(f * g)(c) = \sum_{a+b=c} f(a) \cdot g(b)
$$

This is a convolution - specifically, it is the convolution of $f$ and $g$, evaluated at $c$.

We know that $b = c - a$ so we can re-write this as:

$$
(f * g)(c) = \sum_{a+b=c} f(a) \cdot g(c-a)
$$

Convolutions can be applied to any number of dimensions, using the same equation as above - it's just that $a, b, c$ become vectors.

For example, in two dimensions:

$$
(f * g)(c_1, c_2) = \sum_{\begin{aligned}a_1+b_1&=c_1\\a_2+b_2&=c_2\end{aligned}} f(a_1, a_2) \cdot g(b_1, b_2)
$$


For CNNs, convolutions allow us to describe which weights are identical across a layer.

A layer of neurons is typically described in aggregate with some weight matrix $W$, i.e.

$$
y = \sigma(Wx + b)
$$

In a convolutional layer, many of the weights are repeated in different positions:

![A simple CNN [Source](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)](assets/conv_06.png)

So a typical weight matrix for a layer might look like:

$$
W = \left[\begin{array}{ccccc}
W_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\
W_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\
W_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\
W_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\
...     &   ...   &   ...   &  ...    & ...\\
\end{array}\right]
$$

But for a convolutional layer, many weights repeat in different positions. And there are also many zeros since each unit in $A$ is not connected to every input:

$$
W = \left[\begin{array}{ccccc}
w_0 & w_1 &  0  &  0  & ...\\
 0  & w_0 & w_1 &  0  & ...\\
 0  &  0  & w_0 & w_1 & ...\\
 0  &  0  &  0  & w_0 & ...\\
... & ... & ... & ... & ...\\
\end{array}\right]
$$

Multiplying with weight matrix is the same thing as convolving with $[...0, w_1, w_0, 0...]$.

### Convolution kernels

CNNs learn a _convolution kernel_ and (for images) apply it to every pixel across the image:

![Convolution kernel example [source](https://developer.apple.com/library/ios/documentation/Performance/Conceptual/vImage/ConvolutionOperations/ConvolutionOperations.html)](assets/kernel_convolution.jpg)


### References

- <https://colah.github.io/posts/2014-07-Conv-Nets-Modular/>
- <https://colah.github.io/posts/2014-07-Understanding-Convolutions/>
- Composing Music with Recurrent Neural Networks. Daniel Johnson. August 3, 2015. <http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/>


## Recurrent Neural Networks (RNN)

With machine learning, data is typically represented in vector form. This works for certain kinds of data, such as numerical data, but not necessarily for other kinds of data, like text. We usually end up coercing text into some vector representation (e.g. TF-IDF) and end up losing much of its structure (such as the order of words). This is ok for some tasks (such as topic detection), but for many others we are throwing out important information. We could use bigrams or trigrams or so on to preserve some structure but this becomes unmanageably large (we end up with very high-dimension vectors).

Recurrent neural networks are able to take _sequences_ as input, i.e. iterate over a sequence, instead of fixed-size vectors, and as such can preserve the sequential structure of things like text and have a stronger concept of "context".

Basically, an RNN takes in each item in the sequence and updates its hidden representation based on that item and the previous hidden representation. If there is no previous hidden representation (i.e. we are looking at the first item in the sequence), we can initialize it as either all zeros or treat the initial hidden representation as another parameter to be learned.

The input item can be represented with _one-hot encoding_, i.e. each term is to a vector of all zeroes and one 1. For example, if we had the vocabulary $\\{\text{the}, \text{mad}, \text{cat} \\}$, the terms might be respectively represented as $[1,0,0], [0,1,0], [0,0,1]$.

Another way to represent these terms is with an _embedding matrix_, in which each term is mapped to some index of the matrix which points to some $n$-dimensional vector representation. So the RNN learns vector representations for each term.


 Convolutional neural networks, and feed-forward neural networks in general, treat an input the same no matter when they are given it. For RNNs, the hidden representation is like (short-term) "memory" for the network, so context is taken into account for inputs; that is, an input will be treated differently depending on what the previous input(s) was/were.

RNNs may incorporate __long short term memory__ (LSTM) units, which can handle longer-term context. These units have a few gates:

- _write_ - controls the amount of current input to be remembered
- _read_ - controls the amount of memory given as output to the next stage
- _erase_ - controls what part of the memory is erased or kept in the current time step

![A LSTM unit [source](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)](assets/lstm_unit.png)


### More on RNNs

In the most basic RNN, the hidden layer have two inputs: the input from the previous layer, and the layer's own output from the previous time step (so it loops back onto itself):

![Simple RNN network, with hidden nodes looping [source](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)](assets/rnn_1.png)

This simple network can be visualized over time as well:

![Simple RNN network, with hidden nodes looping over time [source](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)](assets/rnn_2.png)

Say we have a hidden layer $L_1$ of size 3 and another hidden layer $L_2$ of size 2. In a regular NN, the input to $L_2$ is of size 3 (because that's the output size of $L_1$). In an RNN, $L_2$ would have 3+2 inputs, 3 from $L_1$, and 2 from its own previous output.

This simple feedback mechanism offers a kind of short-term memory - the network "remembers" the output from the previous time step.

It also allows for variable-sized inputs and outputs - the inputs can be fed in one at a time and combined by this feedback mechanism.

This short-term memory may be _too_ short, however. Many RNNs incorporate _long short-term memory_ (LSTM) instead, where we have memory stored and passed through a longer number of steps. This memory is modified in each step, with something being added and something being removed at each step.

### Caveats

- RNNs train very slowly on CPUs; they train significantly faster on GPUs.
- Seems to perform less well than simpler models on small datasets; benefits don't show up until larger amounts of training data are used

### more

Basically, the core difference of an RNN from a regular feedforward network is that the output of a neuron is a function of its inputs _and_ of its past state, e.g.

$$
\text{OUT}_t = f(\text{OUT}_{t-1} W_r + X_t W_x)
$$

Where $W_r$ are the __recursive weights__.

RNNs are trained using a variant of backpropagation called __backpropagation through time__, which just involves unfolding the RNN a certain number of time steps, which results in what is essentially a regular feedforward network, and then applying backpropagation:

$$
\frac{\partial E}{\partial \text{OUT}_{t-1}} = \frac{\partial E}{\partial \text{OUT}_t} \frac{\partial \text{OUT}_t}{\partial \text{OUT}_{t-1}} = \frac{\partial E}{\partial \text{OUT}_t}W_r
$$

which starts with:

$$
\frac{\partial E}{\partial y} = \frac{\partial E}{\partial \text{OUT}_n}
$$

Where $\text{OUT}_n$ is the output of the last layer.

The gradients of the cost function wrt to the weights is computed by summing the weight gradients in each layer:

$$
\begin{aligned}
\frac{\partial E}{\partial W_x} &= \sum^n_{k=0} \frac{\partial E}{\partial \text{OUT}_t} X_t \\
\frac{\partial E}{\partial W_r} &= \sum^n_{k=1} \frac{\partial E}{\partial \text{OUT}_t} \text{OUT}_{t-1}
\end{aligned}
$$

This summing of the weight gradients at each time step is the main difference from regular feedforward networks, aside from that BPTT is basically just backpropagation on an RNN unrolled up to some time step $t$.

However, if working with long sequences, this is effectively like training a deep network with many hidden layers (i.e. this is equivalent to an unrolled RNN), which can be difficult (due to vanishing or exploding gradients). In practice, it's common to truncate the backpropagation by running it for only to a few time steps back.

The vanishing gradient problem in RNNs means long-term dependencies won't be learned - the effect of earlier steps "vanish" over time steps (this is essentially the same problem of vanishing gradients in deep feedforward networks).

Exploding gradients are more easily dealt with - it's obvious when they occur (you'll see `NaN`s, for instance), and you can clip them at some maximum value, which can be quite effective (refer to [this paper](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf))

Some strategies for dealing with vanishing gradients:

- vanishing gradients are sensitive to weight initialization, so proper weight initialization can help avoid them
- ReLUs can work better as the nonlinear activation functions since they are not bounded by 1 as the sigmoid and $\tanh$ nonlinearities are

Generally, however, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures are used instead of vanilla RNNs, which were designed for mitigating vanishing gradients (for the purpose of better learning long-range dependencies).

LSTM and GRU RNNs just compute hidden states in a different way than in a vanilla RNN.

TODO include Chris Olah's LSTM diagrams: <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>

The LSTM architecture includes an LSTM unit, which is what computes the current hidden state based on the previous hidden state and the current input.

The LSTM unit includes three _gates_ - input, output, and forget - which are sigmoid functions. They are called gates because they tune how much of their input is passed on (i.e. sigmoids give a value in $[0,1]$, which can be thought as the percent of input to pass on).

The input gate determines how much of the input is let through, the forget gate determines how much of the previous state is let through. We compute a new "memory" (i.e. the LSTM unit's internal state) from the outputs of these gates. The output gate determines how much of this new memory to output as the hidden state.

An RNN is can be thought of as an LSTM in which all input and output gates are 1 and all forget gates are 0, with an additional activation function (e.g. $\tanh$) afterwards (LSTMs do not have this additional activation function).

There are many variations of LSTMs (see [this paper](http://arxiv.org/pdf/1503.04069.pdf) for empirical comparisons between some of them).

A GRU is a simpler LSTM unit; it includes only two gates (also both sigmoid functions) - the reset gate and the update gate. The reset gate determines how to mix the current input and the previous state and the update gate determines how much of the previous state to retain. A vanilla RNN is a GRU architecture in which all reset gates are 1 and all update gates are 0 (with an additional activation function; like LSTMs don't have this additional nonlinearity). GRUs don't have internal states like LSTM units do; there is no output gate so there is no need for an internal state.


### BI-RNNs

__Bidirectional RNNs__ (BI-RNNs) are a variation on RNNs in which the RNN can not only look into the past, but it can also look into the "future". The BI-RNN has two states, $s_i^f$ (the forward state) and $s_i^b$ (the backward state). The forward state $s_i^f$ is based on $x_1, x_2, \dots, x_i$, whereas the backward state $s_i^b$ is based on $x_n, x_{n-1}, \dots, x_i$. These states are managed by two different RNNs, one which is given the sequence $x_{1:n}$ and the other is fed $x_{n:1}$ (that is, the input in reverse).

The output at position $i$ is the concatenation of these RNNs' output vectors, i.e. $y_i = [y_i^f; y_i^b]$.

### References

- General Sequence Learning using Recurrent Neural Networks, Alec Radford <https://www.youtube.com/watch?v=VINCQghQRuM>
- <http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/>
- Composing Music with Recurrent Neural Networks. Daniel Johnson. August 3, 2015. <http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/>
- Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients. Denny Britz. October 8, 2015. <http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/>
- Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano. Denny Britz. October 27, 2015. <http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/>



## Word Embeddings

A word embedding $W : \text{words} \to \mathbb R^n$ is a parameterized function that maps words to high-dimensional vectors (typically 200-500 dimensions).

This function is typically a lookup table parameterized by a matrix $\theta$, where each row represents a word. That is, the function is often $W_{\theta}(w_n) = \theta_n$. $\theta$ is initialized with random vectors for each word.

So given a task involving words, we want to learn $W$ so that we have good representations for each word.

You can visualize a word embedding space using t-SNE (a technique for visualizing high-dimensional data):

![Visualizing a word embedding space with t-SNE ([Turian et al (2010)](http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf))](assets/Turian-WordTSNE.png)

As you can see, words that are similar in meaning tend to be closer together. Intuitively this makes sense - if words have similar meaning, they are somewhat interchangeable, so we expect that their vectors be similar too.

We'll also see the vectors capture notions of analogy, for example "Paris" is to "France" as "Tokyo" is to "Japan". These kinds of analogies can be represented as vector addition: "Paris" - "France" + "Japan" = "Tokyo".

The best part is the neural network is not explicitly told to learn representations with these properties - it is just a side effect. This is one of the remarkable properties of neural networks - they learn good ways of representing the data more or less on their own.

And these representations can be portable. That is, maybe you learn $W$ for one natural language task, but you may be able to re-use $W$ for another natural language task (provided it's using a similar vocabulary). This practice is sometimes called "pretraining" or "transfer learning" or "multi-task learning".

You can also map multiple words to a single representation, e.g. if you are doing a multilingual task. For example, the English and French words for "dog" could map to the same representation since they mean the same thing (in which case we could call this a "bilingual word embedding").

Here's an example visualization of a Chinese and English bilingual word embedding:

![A Chinese and English word embedding ([Socher et al (2013a)](http://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf))](assets/Socher-BillingualTSNE.png)

You can even go a step further and learn image and word representations together, so that vectors representing images of horses are close to the vector for the word "horse".

Two main techniques for learning word embeddings are:

- CBOW: predicting the probability of context words given a word
- Skip-gram: predicting the probability of a word given context words

### Modular Neural Networks

So say we have trained a neural net which has learned our function $W$, and given a word input, it outputs us the word's high-dimensional vector representation.

We can re-use this network in a modular fashion so that we construct a larger neural net which can take a fixed-size set of words as input. For example, the following network takes in five words, from which we get their representations, which are then passed into another network $R$ to yield some output $s$.

![A modular neural network ([Bottou (2011)](http://arxiv.org/pdf/1102.1808v3.pdf))](assets/Bottou-WordSetup.png)

### Recursive Neural Networks

Using modular neural networks like above is limiting in the fact that we can only accept a fixed number of inputs.

We can get around this by adding an association module $A$, which takes two representations and merges them.

![Using association modules ([Bottou (2011)](http://arxiv.org/pdf/1102.1808v3.pdf))](assets/Bottou-Afold.png)

As you can see, it can take either a reputation from a word (via a $W$ module) or from a phrase (via another $A$ module).

We probably don't want to merge words linearly though. Instead we might want to group words in some way:

![A recursive neural network ([Bottou (2011)](http://arxiv.org/pdf/1102.1808v3.pdf))](assets/Bottou-Atree.png)

This kind of model is a "recursive neural network" (sometimes "tree-structured neural network") because it has modules feeding into modules of the same type.

## Nonlinear neural nets

In typical NNs, the architecture of the network is specified before hand and is static - neurons don't change connections. In a nonlinear neural net, however, the connections between neurons becomes dynamic, so that new connections may form and old connections may break. This is more like how the human brain operates. But so far at least, these are very complex and difficult to train.

## Transfer learning

The practice of transfer learning involves taking a neural net trained for another task and applying it to a different task. For instance, if using an image classification net trained for one classification task, you can use that same network for another, truncating the output layer, that is, take the vectors from the second-to-last layer and use those as feature vectors for other tasks.

## Recursive Neural Tensor Networks

~ to do ~

### References

- <https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/>
- <https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw>
i- <http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html>

## Choosing the network architecture

How do you decide how many layers to use, and what size each layer should be?

As the network grows in number of layers and size, the network _capacity_ increases, which is to say it is capable of representing more complex functions.

![More complex network, more complex functions ](assets/layer_sizes.svg)

Simpler networks have fewer local minima, but they are easier to converge to and tend to perform worse (they have higher loss). There is a great deal of variance across these local minima, so the outcome is quite sensitive to the random initialization - some times you land in a good local minima, sometimes not. More complex networks have more local minima, but they tend to perform better, and there is less variance across how these local minima perform.

Higher-capacity networks run a greater risk of overfitting, but this overfitting can be (preferably) mitigated by other methods such as L2 regularization, dropout, and input noise. So don't let overfitting be the sole reason for going with a simpler network if a larger one seems appropriate.

Here are regularization examples for the same data from the previous image, with the neural net for 20 hidden neurons:

![Regularization strength](assets/reg_strengths.svg)

As you can see, regularization is effective at counteracting overfitting.

### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 1: Neural Networks Part 1: Setting up the Architecture. Andrej Karpathy. <https://cs231n.github.io/neural-networks-1/>

## Weight initialization

Given normalized data, we estimate that roughly half the weights will be negative and roughly half will be positive.

It may seem intuitive to initialize all weights to zero, but you should not, since this causes every neuron to have the same output, which causes them to have the same gradients during backpropagation, which causes them to all have the same parameter updates. Thus none of the neurons will differentiate.

So we can set each neuron's initial weights to be a random vector from a standard multidimensional normal distribution, scaled by some value, e.g. `0.001` so that they are kept very small, but still non-zero. This process is known as _symmetry breaking_. The random initializations allow the neurons to differentiate themselves during training.

Note that, however, small initial weights can be problematic for deep networks, since they may reduce the gradient signal that flows backwards by too much (in a weaker version of the gradient "killing" effect mentioned earlier).

As the number of inputs to a neuron grows, so too will its output's variance. This can be controlled for (calibrated) by scaling its weight vector by the square root of its "fan-in" (its number of inputs), so you should divide the standard multidimensional distribution sampled random vector by $\sqrt{n}$, where $n$ is the number of the neuron's inputs. For ReLUs, it is recommended you instead divide by $\sqrt{2/n}$. ([Karpathy's CS231n notes](https://cs231n.github.io/neural-networks-2/) provides more detail on why this is.)

An alternative to this fan-in scaling for the uncalibrated variances problem is _sparse initialization_, which is to set all weights to 0, and then break symmetry by randomly connecting every neuron to some fixed number (e.g. 10) of neurons below it by setting those weights to ones randomly sampled from the standard normal distribution like mentioned previously.

Biases are commonly initialized to be zero, though if using ReLUs, then you can set them to a small value like 0.01 so all the ReLUs fire at the start and are included in the gradient backpropagation update.

Elsewhere it is recommended that ReLU weights should be sampled from zero-mean Gaussian distribution with standard deviation of $\sqrt{\frac{2}{d_\text{in}}}$.

### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 2: Neural Networks Part 2: Setting up the Data and the Loss. Andrej Karpathy. <https://cs231n.github.io/neural-networks-2/>

## Regularization

Regularization techniques are used to prevent neural networks from overfitting.

### L2 Regularization

The most common form of regularization. Penalize the squared magnitude of all parameters (weights) as part of the objective function, i.e. we add $\sum \lambda w^2$ to the objective function. It is common to include $\frac{1}{2}$, i.e. use $\frac{1}{2} \sum \lambda w^2$, so the gradient of this term wrt to $w$ is just $\lambda w$ instead of $2 \lambda w$$. This avoids the network relying heavily on a few weights and encourages it to use all weights a little.

### L1 Regularization

Similar to L2 regularization, except that the term added to the objective function is $\sum \lambda |w|$. L1 regularization has the effect of causing weight vectors to become sparse, such that neurons only use a few of their inputs and ignore the rest as "noise". Generally L2 regularization is preferred to L1.

### Elastic net regularization

This is just the combination of L1 and L2 regularization, such that the term introduced to the objective function is $\sum \lambda_1 |w| + \lambda_2 w^2$.

### Max norm constraints

This involves setting an absolute upper bound on the magnitude of the weight vectors; that is, after updating the parameters/weights, clamp every weight vector so that it satisfies $||w||_2 < c$, where $c$ is some constant (the maximum magnitude).

### Dropout

Dropout is a regularization method which works well with the others mentioned so far (L1, L2, maxnorm). During training, we specify a probability $p$, and we only keep a neuron active with that probability $p$, otherwise we set its output to zero. If the neuron's output is set to 0, that has the effect of temporarily "removing" that neuron for that training iteration. This dropout is applied only at training time and applied per-layer (that is, it is applied after each layer, see the code example below). This prevents the network from relying too much on certain neurons.

One way to think about this is that, for each training step, a sub-network is sampled from the full network, and only those parameters are updated. Then on the next step, a different sub-sample is taken and updated, and so on.

![A network after dropout is applied to each layer in a training iteration [source](https://cs231n.github.io/neural-networks-2/)](assets/dropout.jpeg)

At test time, all neurons are active (i.e. we don't use dropout at test time). However, we must scale the activation functions by $p$ to maintain the same expected output for each neuron. Say $x$ is the output of a neuron without dropout. With dropout, the neuron's output has a chance $p$ of being set to 0, so its expected output becomes $px$ (more verbosely, it has $1-p$ chance of becoming 0, so its output is $px + (1-p)0$, which simplifies to $px$). Thus we must scale the outputs (i.e. the activation functions) by $p$ to keep the expected output consistent.

This scaling can be applied at training time, which is more efficient - this technique is called _inverted dropout_.

For comparison, here is an implementation of regular dropout and an implementation of inverted dropout (source from: <https://cs231n.github.io/neural-networks-2/>)

    # Dropout
    p = 0.5 # probability of keeping a unit active. higher = less dropout

    def train_step(X):
      """ X contains the data """

      # forward pass for example 3-layer neural network
      H1 = np.maximum(0, np.dot(W1, X) + b1)
      U1 = np.random.rand(*H1.shape) < p # first dropout mask
      H1 *= U1 # drop!
      H2 = np.maximum(0, np.dot(W2, H1) + b2)
      U2 = np.random.rand(*H2.shape) < p # second dropout mask
      H2 *= U2 # drop!
      out = np.dot(W3, H2) + b3

      # backward pass: compute gradients... (not shown)
      # perform parameter update... (not shown)

    def predict(X):
      # ensembled forward pass
      H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
      H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
      out = np.dot(W3, H2) + b3

    # Inverted dropout
    p = 0.5 # probability of keeping a unit active. higher = less dropout

    def train_step(X):
      # forward pass for example 3-layer neural network
      H1 = np.maximum(0, np.dot(W1, X) + b1)
      U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
      H1 *= U1 # drop!
      H2 = np.maximum(0, np.dot(W2, H1) + b2)
      U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
      H2 *= U2 # drop!
      out = np.dot(W3, H2) + b3

      # backward pass: compute gradients... (not shown)
      # perform parameter update... (not shown)

    def predict(X):
      # ensembled forward pass
      H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
      H2 = np.maximum(0, np.dot(W2, H1) + b2)
      out = np.dot(W3, H2) + b3

### Recommendations

> It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of $p=0.5$ is a reasonable default, but this can be tuned on validation data. <https://cs231n.github.io/neural-networks-2/>

### References

- CS231n Convolutional Neural Networks for Visual Recognition, Module 2: Neural Networks Part 2: Setting up the Data and the Loss. Andrej Karpathy. <https://cs231n.github.io/neural-networks-2/>

## Training

Start training with small, unequal weights to avoid _saturating_ the network w/ large weights. If all the weights start equal, the network won't learn anything.


## Hopfield Nets

### Recurrent NNs and stability

RNNs typically work by feeding in input, taking the output that results and feeding it in as new input, and so on until the output stabilizes. It is possible, however, that the output never stabilizes; such an RNN is said to be _unstable_.

In a binary Hopfield net, a simple threshold function is used as the activation function, such that each neuron outputs 0 or 1. The _state_ of the network is the set of values the network outputs, which effectively is a binary number, e.g. `1001` is a possible state for a network with four outputs. These outputs are fed back into the network as inputs which may change the state of the network - the hope is that eventually it stabilizes.

A Hopfield net with two outputs has four possible states: `00`, `01`, `10`, `11`, which can be conceptualized as the vertices of a two-dimensional polygon (a square). More generally, a Hopfield net with $n$ outputs will have $2^n$ states, and can be represented by an $n$-dimensional hypercube.

If we represent the weights of the network as a matrix $W$, then we can prove that the recurrent network is stable given that $w_{ii} = 0$ for all $i$ and that $w_{ij} = w_{ji}$ for all $j \neq i$; that is, if the matrix is symmetrical and as a zero diagonal. (see pp97) This can be similarly proven for continuous recurrent neural networks (i.e. with continuous activation functions).

However, it is possible for a recurrent network that does not satisfy these properties to be stable (which is to say, these are sufficient but not necessary requirements).

Note that a stable Hopfield net does not necessarily stabilize on the global minimum. Here, statistical training methods such as Boltzmann or Cauchy training can be used to guarantee a global minimum.

For the binary Hopfield net, the following strategy can be used:

$$
\begin{aligned}
E_k &= \text{NET}_k - \theta_k \\
p_k &= \frac{1}{1+\exp(-\delta \frac{E_k}{T})}
\end{aligned}
$$

where:

- $\text{NET}_k$ = the $\text{NET}$ output of neuron $k$
- $\theta_k$ = the threshold of neuron $k$
- $T$ = the artificial temperature

Set $T$ to a high value and set the neurons to an initial state by passing in an input vector. Then repeat:

1. For each neuron, set the state to one with probability $p_k$, otherwise, set to zero
2. Gradually reduce the artificial temperature and repeat step 1 until equilibrium is reached

---


backprop:

a single neuron:

$$
\text{OUT} = F(\text{NET})
$$

where:

- $\text{NET} = XW$ = net input to the activation function
- $X$ = input vector
- $W$ = weight vector
- $F$ = activation function
- $\text{OUT}$ = output of the neuron

if $F$ is the sigmoid function:

$$
\frac{\partial \text{OUT}}{\partial \text{NET}} = (\text{OUT})(1-\text{OUT})
$$

The total error of the network is: $\text{ERROR}_{\text{total}} = y_{\text{true}} - y_{\text{pred}}$.

We'll call the error for a particular layer $\text{ERROR}$.

For the penultimate layer (the layer just before the output), $\text{ERROR} = \text{ERROR}_{\text{total}}$.

For every preceding layer:

$$
\text{ERROR} = W_{L+1} \delta_{L+1}
$$

That is, it is equal to the product of the weight (before updating it) and error for the layer that comes after it (forward-wise).

The error for a particular neuron is:

$$
\delta = \frac{\partial \text{OUT}}{\partial \text{NET}}(\text{ERROR})
$$

The weight update for a particular neuron is:

$$
\Delta W = \delta \eta \text{OUT}
$$

So the new weight is just:

$$
W += \Delta W
$$

$\eta$ is the learning rate - it is preferable to $\alpha$ for notation because $\alpha$ can be used to represent a "momentum coefficient" or a "smoothing coefficient".

Momentum: keep track of the previous weight change and incorporate it in the next weight updates, such that:

$$
\Delta = \eta \delta \text{OUT} + \alpha [\Delta W_{t-1}]
$$

Where $\alpha \in [0,1]$ is the _momentum coefficient_, usually $0.9$, and the final bracketed term, $\Delta W_{t-1}$, is the weight change from the previous update.

But most often, momentum is not helpful.

Exponential smoothing:

$$
\Delta W = \eta(\alpha[\Delta W_{t-1}] + (1-\alpha)\delta \text{OUT})
$$

where $\alpha \in [0,1]$. Exponential smoothing can be better than momentum.

---

Using the sigmoid activation function is problematic because the outputs ($\text{OUT}$) have a good chance of being around 0, in which case the weight is not updated, because $\Delta W = \delta \eta \text{OUT}$, that is, if $\text{OUT} \approx 0$, then $\Delta W \approx 0$  Thus the network does not learn (the weights don't change). You can continue to use the sigmoid function if you adjust it:

$$
\text{OUT} = -\frac{1}{2} + \frac{1}{e^{-\text{NET}} + 1}
$$

but, as written elsewhere, you should just use a different activation function, like $\tanh$.

---

backprop is _not_ a guarantee of training at all nor of quick training.

possible issues:

- network paralysis: if the weights become very large, the neurons' $\text{OUT}$s may become very large, where the derivative of the activation function is very small, so weights are not really updated and get "stuck" at large values.
- local minima: statistical training methods can be used (such as simulated annealing), but increase training time
- step size: if it is too small, training is too slow, if it is too large, paralysis or instability (no convergence) are possible
- stability: that the network does not mess up its learning of something else to learn another thing. for instance, say it learns good weights for one input, but to learn good weights for another input, it "overwrites" or "forgets" what it learned about the prior input.

---

statistical training methods:

statistical (or "stochastic") training methods, contrasted with _deterministic_ training methods, involve some randomness to avoid local minima. They generally work by randomly leaving local minima to possibly find the global minimum. The severity of this randomness decreases over time so that a solution is "settled" upon (this gradual "cooling" of the randomness is the key part of _simulated annealing_).

simulated annealing applied as a training method to a neural network is called _Boltzmann training_ (neural networks trained in this way are called _Boltzmann machines_):

1. set $T$ (the artificial temperature) to a large value
2. apply inputs, calculate outputs and objective function
3. make random weight changes, recalculate network output and change in objective function
4a. if objective function improves, keep weight changes
4b. if the objective function worsens, accept the change according to the probability drawn from Boltzmann distribution, $P(c)$, select a random variable $r$ from a uniform distribution in $[0, 1]$; if $P(c) > r$, keep the change, otherwise, don't.

$$
P(c) = exp(\frac{-c}{kT})
$$

where:

- $c$ the change in the objective function
- $k$ a constant analogous to the Boltzmann's constant in simulated annealing, specific for the current problem
- $T$ the artificial temperature
- $P(c)$ the probability of the change $c$ in the objective function


Steps 3 and 4 are repeated for each of the weights in the network as $T$ is gradually decreased.

The random weight change can be selected in a few ways, but one is just choosing it from a Gaussian distribution, $P(w) = exp(\frac{-w^2}{T^2})$, where $P(w)$ is the probability of a weight change of size $w$ and $T$ is the artificial temperature. Then you can use Monte Carlo simulation to generate the actual weight change, $\Delta w$.

Boltzmann training uses the following cooling rate, which is necessary for convergence to a global minimum:

$$
T(t) = \frac{T_0}{log(1+t)}
$$

Where $T_0$ is the initial temperature, and $t$ is the artificial time.

The problem with Boltzmann training is that it can be very slow (the cooling rate as computed above is very low).

This can be resolved by using the Cauchy distribution instead of the Boltzmann distribution; the former has fatter tails so has a higher probability of selecting large step sizes. Thus the cooling rate can be much quicker:

$$
T(t) = \frac{T_0}{1+t}
$$

The Cauchy distribution is:

$$
P(x) = \frac{T(t)}{T(t)^2 + x^2}
$$

where $P(x)$ is the probability of a step of size $x$.

This can be integrated, which makes selecting random weights much easier:

$$
x_c = \rho T(t) \tan(P(x))
$$

Where $\rho$ is the learning rate coefficient and $x_c$ is the weight change.

Here we can just select a random number from a uniform distribution in $(-\frac{\pi}{2}, \frac{\pi}{2})$, then substitute this for $P(x)$ and solve for $x$ in the above, using the current temperature.

Cauchy training still may be slow so we can also use a method based on _artificial specific heat_ (in annealing, there are discrete energy levels where phase changes occur, at which abrupt changes in the "specific heat" occur). In the context of artificial neural networks, we define the (pseudo)specific heat to be the average rate of change of temperature with the objective function. The idea is that there are parts where the objective function is sensitive to small changes in temperature, where the average value of the objective function makes an abrupt change, so the temperature must be changed slowly here so as not to get stuck in a local minima. Where the average value of the objective function changes little with temperature, large changes in temperature can be used to quicken things.

Still, Cauchy training may be much slower than backprop, and can have issues of network paralysis (because it is possible to have very large random weight changes), esp. if a nonlinearity is used as the activation function (see the bit on network paralysis and the sigmoid function above).

Cauchy training may be combined with backprop to get the best of both worlds - it simply involves computing both the backprop and Cauchy weight updates and applying their weighted sum as the update. Then, the objective function's change is computed, and like with Cauchy training, if there is an improvement, the weight change is kept, otherwise, it is kept with a probability determined by the Boltzmann distribution.

The weighted sum of the individual weight updates is controlled by a coefficient $\eta$, such that the sum is $\eta [\alpha \Delta W_{t-1} + (1-\alpha)\delta \text{OUT}] + (1 - \eta) x_c$, so that if $\eta=0$, the training is purely Cauchy, and if $\eta=1$, it becomes purely backprop.

There is still the issue of the possibility of retaining a massive weight change due to the Cauchy distribution's infinite variance, which creates the possibility of network paralysis. The recommended approach here is to detect saturated neurons by looking at their $\text{OUT}$ values - if it is approaching the saturation point (positive or negative), apply some squashing function to its weights (note that this squashing function is not restricted to the range $[-1, 1]$ and in fact may work better with a larger range). This potently reduces large weights while only attenuating smaller ones, and maintains symmetry across weights.


## Unsupervised neural networks

>  The most basic one is probably the autoencoder, which is a feed-forward neural net which tries to predict its own input. While this isn’t exactly the world’s hardest prediction task, one makes it hard by somehow constraining the network. Often, this is done by introducing a bottleneck, where one or more of the hidden layers has much lower dimensionality than the inputs. Alternatively, one can constrain the hidden layer activations to be sparse (i.e. each unit activates only rarely), or feed the network corrupted versions of its inputs and make it reconstruct the clean ones (this is known as a denoising autoencoder). [https://www.metacademy.org/roadmaps/rgrosse/deep_learning]

## In high dimensions, local minima are not problems

More typically, there are saddle points, which slow down training but can be escaped in time. This is because with many dimensions, it is unlikely that a point is a minimum is _all_ dimensions (if we consider that a point is a minimum in one dimensions with probability $p$, then it has probability $p^n$ to be a minimum in all $n$ dimensions); it is, however, likely that it is a local minimum in some of the dimensions.

As training nears the global minimum, $p$ increases, so if you do end up at a local minimum, it will likely be close enough to the global minimum.

## Training on adversarial examples

Adding noise to input, such as in the accompanying figure, can throw off a classifier. Few strategies are robust against these tricks, but one approach is to generate these adversarial examples and include them as part of the training set.

![Adversarial example [source](http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/)](assets/adversarial.png)

## Training neural nets tips

- Normalize real-valued data (subtract mean, divide by standard deviation (see part on data preprocessing))
- Decrease the learning rate during training
- Use minibatches for a more stable gradient
- Use momentum to get through plateaus

## Neural net weight initialization

Sample your weights uniformly from $[-b, b]$, where:

$$
b = \sqrt{\frac{6}{H_k + H_{k+1}}}
$$

where $H_k$ and $H_{k+1}$ are the sizes of the hidden layers before and after the weight matrix.

## Activation functions

| Activation Function | Propagation                   | Backpropagation                                                                                           |
|---------------------|-------------------------------|-----------------------------------------------------------------------------------------------------------|
| Sigmoid             | $y_s = \frac{1}{1+e^{-x_2}}$  | $[\frac{\partial E}{\partial x}]_s = [\frac{\partial E}{\partial y}]_s \frac{1}{(1+e^{x_2})(1+e^{-x_2})}$ |
| Tanh                | $y_s = \tanh(x_s)$            | $[\frac{\partial E}{\partial x}]_s = [\frac{\partial E}{\partial y}]_s \frac{1}{\cosh^2 x_s}$             |
| ReLu                | $y_s = \max(0, x_s)$          | $[\frac{\partial E}{\partial x}]_s = [\frac{\partial E}{\partial y}]_s \mathbb I \{x_s > 0\}$             |
| Ramp                | $y_s = \min(-1, \max(1,x_s))$ | $[\frac{\partial E}{\partial x}]_s = [\frac{\partial E}{\partial y}]_s \mathbb I \{1- < x_s < 1\}$        |

There is also the __hard-tanh__ activation function, which is an approximation of $\tanh$ that is faster to compute and take derivatives of:

$$
\text{hardtanh}(x) =
\begin{cases}
-1 & x < -1 \\
1 & x > 1 \\
x & \text{otherwise}
\end{cases}
$$

## Loss functions

| Loss Function             | Propagation                                 | Backpropagation                                                                                                       |
|---------------------------|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| Square                    | $y = \frac{1}{2}(x-d)^2$                    | $\frac{\partial E}{\partial x} = (x-d)^T \frac{\partial E}{\partial y}$                                               |
| Log, $c = \pm 1$          | $y = \log(1 + e^{-cx})$                     | $\frac{\partial E}{\partial x} = \frac{-c}{1+e^{cx}} \frac{\partial E}{\partial y}$                                   |
| Hinge, $c = \pm 1$        | $y = \max(0, m-cx)$                         | $\frac{\partial E}{\partial x} = -c \mathbb I \{cx < m \} \frac{\partial E}{\partial y}$                              |
| LogSoftMax, $c=1 \dots k$ | $y = \log(\sum_k e^{x_k}) - x_c$            | $[\frac{\partial E}{\partial x}]_s = (\frac{e^{x_s}}{\sum_k} e^{x_k} - \delta_{sc})\frac{\partial E}{\partial y}$     |
| MaxMargin, $c=1 \dots k$  | $y = [\max_{k \neq c} \{x_k + m\} - x_c]_+$ | $[\frac{\partial E}{\partial x}]_s = (\delta_{sk^*} - \delta_{sc}) \mathbb I \{E > 0\} \frac{\partial E}{\partial y}$ |


## LSTM networks

A type of RNN.

Conventional RNNs do not support learning long-term dependencies; LSTMs are a variation on the conventional RNN which includes a mechanism for learning long-term dependencies. Generally, LSTM networks perform better than regular RNNs.

The conventional RNN has a single activation function which processes its input. An LSTM network instead has four interacting layers in the activation function's place. These layers are themselves learned neural network layers. This entire LSTM unit is sometimes called an LSTM cell.

The cell has a _state_ which is modified based on _gates_, which manage what information modifies the state. The gates are composed of a sigmoid neural net layer and a pointwise multiplication operation. The output of this layer (which, due to the sigmoid function, is in $[0, 1]$) is the amount of information passed through to modify the state. So it can be nothing (0), all of it (1) or somewhere in between.

Three of the cell's four neural network layers are these sigmoid gates.

One gate is the _forget gate_ (sometimes called an _erase gate_), which controls what is removed ("forgotten") from the cell state. The input to the forget gate is the concatenation of the cell's output from the previous step, $\text{OUT}_{t-1}$ and the current input to the cell, $X_t$. The gate computes a value in $[0,1]$ (with the sigmoid function) for _each_ value in the previous cell state $C_{t-1}$; the resulting value determines how much of that value to keep (1 means keep it all, 0 means forget all of it). So we are left with a vector of values in $[0,1]$, which we then pointwise multiply with the existing cell state to get the updated cell state.

The output of a forget gate $f$ at step $t$ is:

$$
f_t = \text{sigmoid}(W_f \dot [\text{OUT}_{t-1}, X_t] + b_f)
$$

Then our intermediate value of $C_t$ is $C_t' = f_t C_{t-1}$.

Where $W_f, b_f$ are the forget gate's weight vector and bias, respectively.

One of the other gates is the _input gate_ (sometimes called a _write gate_), which controls what information gets stored in the cell state. This gate also takes as input the concatenation of $\text{OUT}_{t-1}$ and $X_t$. We will denote its output at step $t$ as $i_t$. Like the forget gate, this is a vector of values in $[0, 1]$ which determine how much information gets through - 0 means none, 1 means all of it.

A $\tanh$ function takes the same input and outputs a vector of candidate values, $\tilde C_t$.

We pointwise multiple this candidate value vector with the input gate's output vector to get the vector that is passed to the cell state. This resulting vector is pointwise added to the updated cell state.

$$
\begin{aligned}
i_t &= \text{sigmoid}(W_i \dot [\text{OUT}_{t-1}, X_t] + b_i) \\
\tilde C_t &= \tanh(W_C \dot [\text{OUT}_{t-1}, X_t] + b_C)
\end{aligned}
$$

Thus our final updated value of $C_t$ is $C_t = C_t' + i_t \tilde C_t$.

We don't output this cell state $C_t$ directly. Rather, we have yet another gate, the _output gate_ (sometimes called a _read gate_) that outputs another vector with values in $[0, 1]$, $o_t$, which determines how much of the cell state is outputted. This gate again takes in as input the concatenation of $\text{OUT}_{t-1}$ and $X_t$.

So the output of the output gate is just:

$$
o_t = \text{sigmoid}(W_o [\text{OUT}_{t-1}, X_t] + b_o)
$$

To get the final output of the cell, we pass the cell state $C_t$ through $\tanh$ and then pointwise multiply that with the output of the output gate:

$$
\text{OUT}_t = o_t \tanh(C_t)
$$

### Variations on LSTM

#### Peephole connections

This variant just passes on the previous cell state, $C_{t-1}$, to the forget and input gates, and the new cell state, $C_t$, to the output gate, that is, all that is changed is that:

$$
\begin{aligned}
f_t &= \text{sigmoid}(W_f \dot [C_{t-1}, \text{OUT}_{t-1}, X_t] + b_f) \\
i_t &= \text{sigmoid}(W_i \dot [C_{t-1}, \text{OUT}_{t-1}, X_t] + b_i) \\
o_t &= \text{sigmoid}(W_o \dot [C_{t-1}, \text{OUT}_{t-1}, X_t] + b_o)
\end{aligned}
$$

#### Update gates

In this variant, the forget and input gates are combined into a single _update gate_. The value $f_t$ is computed the same, but $i_t$ is instead just:

$$
i_t = 1 - f_t
$$

Essentially, we just update enough information to replace what was forgotten.

#### Gated Recurrent Unit (GRU)

This is a popular variant, which also combines the forget and input gates into an update gate (denoted as $r$ here) and replaces the output gate with another gate, denoted $z$. The cell state and its output are also merged as its hidden state, $h_t$. This GRU is described as:

$$
\begin{aligned}
h_{t-1} &= \text{OUT}_{t-1} \\
z_t &= \text{sigmoid}(W_z \dot [h_{t-1}, X_t]) \\
r_t &= \text{sigmoid}(W_r \dot [h_{t-1}, X_t]) \\
\tilde h_t &= \tanh(W \dot [r_t h_{t-1}, X_t]) \\
h_t &= (1 - z_t) h_{t-1} + z_t \tilde h_t \\
\text{OUT}_t &= h_t \\
\end{aligned}
$$

## Distributed Representations

Say we have animals of some type and color. We could learn representations for each of them, e.g. with a neuron for a red cat, one for a blue dog, one for a blue cat, etc. But this would mean learning many, many representations (for instance, with three types and three colors, we learn nine representations). We could instead learn __distributed representations__, where we have neurons that learn the different colors and other neurons which learn the different types (for instance, with three types and three colors, we have six neurons). Thus the representation of a given case, such as a blue dog, is distributed across the neurons.

## Neural Turing Machines

This research is still in its infancy, but a Neural Turing Machine is a neural network capable of learning entire programs; i.e. it is a self-programming network.

## Unstable gradients

Certain neural networks, such as RNNs, can have __unstable gradients__, in which gradients may grow exponentially (an __exploding gradient__) or shrink exponentially until it reaches zero (a __vanishing gradient__).

With exploding gradients, the minimum is not found because, with such a large gradient, the steps don't effectively search the space.

With vanishing gradients, the minimum is not found because a gradient of zero means the space isn't searched at all.

Unstable gradients can occur as a result of drastic changes in the cost surface, as illustrated in the accompanying figure (from [Pascanu et al](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf) via <http://peterroelants.github.io/posts/rnn_implementation_part01/>).

![](assets/exploding_gradient.png)

In the figure, the large jump in cost leads to a large gradient which causes the optimizer to make an exaggerated step.

There are methods for dealing with unstable gradients, including:

- Gradient clipping (e.g. limiting $g$ to $g = \frac{t}{||g||_2}$ if $||g||_2 > t$, where $t$ is some clipping threshold)
- Hessian-Free Optimization
- Momentum
- Resilient backpropagation (Rprop)

### Resilient backpropgation (Rprop)

Normally, weights are updated by the size of the gradient (typically scaled by some learning rate). However, as demonstrated above, this can lead to an unstable gradient.

Resilient backpropagation ignores the size of the gradient and only considers its sign and then uses two hyperparameters, $\eta^-, \eta^+$ ($\eta^+ > 1$) to determine the size of the update. $\eta^-$.

If the sign of the gradient changes in an iteration, the weight update $\Delta$ is multiplied by $\eta^-$, i.e. $\Delta = \Delta \eta^-$. If the gradient's sign doesn't change, the weight update $\Delta$ is multiplied by $\eta^+$, i.e. $\Delta = \Delta \eta^+$.

If the gradient's sign changes, this usually indicates that we have passed through a local minima.

Then the weight is updated by this computed value in the opposite direction of its gradient:

$$
W \to W' = W - \text{sign}(\frac{\partial J}{\partial W}) \Delta
$$

Typically, $\eta^+ = 1.2, \eta^- = 0.5$.

### Rmsprop

__Rmsprop__ is the mini-batch version of Rprop. It computes a moving average, $\text{MA}$, of the squared gradient for each parameter:

$$
\text{MA} = \lambda \text{MA} + (1 - \lambda)(\frac{\partial J}{\partial W})^2
$$

Then normalizes the gradient by dividing by the square root of this moving average:

$$
\frac{\partial J}{\partial W} \frac{1}{\text{MA}}
$$

Rmsprop can be used with momentum as well (i.e. update the velocity with this modified gradient).

> The basic idea behind rmsprop is to adjust the learning rate per-parameter according to the a (smoothed) sum of the previous gradients. Intuitively this means that frequently occurring features get a smaller learning rate (because the sum of their gradients is larger), and rare features get a larger learning rate. <http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/>

## backprop re-write

#### the following is an old explanation for backprop, which may still have some useful parts

We will use the following notation:

$$
N^{\text{layer number}}_{\text{neuron number}}
$$

So, for instance, $N^2_4$ is the fourth neuron in the second layer.

When we drop the neuron number, we are notating a vector of values for the entire layer. So, for example, $\text{OUT}^l$ is the output for the entire layer $l$.

We'll say that we have $m$ training examples and $n$ layers. So $L^n$ is the last layer of the network (i.e. the output layer).

An individual neuron $N^l_i$, has an activation function $f^l_i$, has weights $W^l_i$, has bias $b^l_i$, and outputs $\text{OUT}^l_i$. It's output is calculated:

$$
\begin{aligned}
\text{NET}^l_i &= W^l_i \text{OUT}^{l-1} + b^l_i \\
\text{OUT}^l_i &= f(\text{NET}^l_i)
\end{aligned}
$$

Where $\text{NET}^l_i$ is the net input to $N^l_i$.

We can think of each layer in the network as a function $g_l$. Each layer, then, is a function $g_i$ of its previous layer $g_{i-1}$, such that the entire network represents the function $h_{\theta}(X) = g_n(g_{n-1}(g_{n-2})(\dots(X)))$.

When training the network, we want to compute its error and distributed across the network so that we know how to update each neuron's parameters (weights and bias). We use __backpropagation__ to do this.

The output of the entire network is the output of its final layer, $L^n$, so we notate that output as $\text{OUT}^n$. We compute the error of each individual layer, notating this error as $\delta^l$.

Since we want to relate the weights to the error, we ultimately want to compute $\frac{\partial J}{\partial W^i}$, that is, how the error changes with the weights of a layer.

But first, it is easier to compute just the error for a layer.

We start by computing the error of the output layer, $\delta^n$, and then propagate it backwards through the network. This is accomplished at its core by applying the chain rule of derivation.

$$
\begin{aligned}
\delta^n &= \frac{\partial J}{\partial \text{NET}^n} \\
&= \frac{\partial J}{\partial \text{OUT}^n} \frac{\partial \text{OUT}^n}{\partial \text{NET}^n}
\end{aligned}
$$

Note that the term $\frac{\partial \text{OUT}^n}{\partial \text{NET}^n}$ is just the derivative of the activation function $f^n$, i.e. $(f^n)'(\text{NET}^n)$.

In a sense, $\delta^n$ is the error that the output layer is "responsible" for.

We want to do something similar and compute the error for every other layer as well. The error for some layer $i$ is $\delta^i$.

First, we need to introduce the $\odot$ operation, which is just elementwise multiplication (it is sometimes known as the Hadamard product or the Schur product).

$\delta^i$, where $i \neq n$, is derived similarly to $\delta^n$:

$$
\delta^i = \frac{\partial J}{\partial \text{NET}^i}
$$

We know that:

$$
\delta^{i+1} = \frac{\partial J}{\partial \text{NET}^{i+1}}
$$

So we can redefine $\delta^i$ with that term (that is, by propagating the error from the next layer backwards to this layer):

$$
\begin{aligned}
\delta^i &= \delta^{i+1} \frac{\partial \text{NET}^{i+1}}{\partial \text{NET}^i} \\
&= \frac{\partial J}{\partial \text{NET}^{i+1}} \frac{\partial \text{NET}^{i+1}}{\partial \text{NET}^i} \\
\end{aligned}
$$

We also know that:

$$
\begin{aligned}
\text{NET}^{i+1} &= W^{i+1} \text{OUT}^i + b^{i+1} \\
&= W^{i+1} f^i(\text{NET}^i) + b^{i+1} \\
\frac{\partial \text{NET}^{i+1}}{\partial \text{NET}^i} &= W^{i+1} (f^i)'(\text{NET}^i)
\end{aligned}
$$

Thus:

$$
\delta^i = ((w^{i+1})^T \delta^{i+1}) \odot (f^i)'(\text{NET}^i)
$$

Now let's consider how we would compute $\frac{\partial J}{\partial W^i}$.

Expanding this with the chain rule:

$$
\frac{\partial J}{\partial W^i} = \frac{\partial J}{\partial \text{OUT}^i} \frac{\partial \text{OUT}^i}{\partial \text{NET}^i} \frac{\partial \text{NET}^i}{\partial W^i}
$$

Note that we have already computed $\frac{\partial J}{\partial \text{OUT}^i} \frac{\partial \text{OUT}^i}{\partial \text{NET}^i}$ as $\delta^i$ (and as $\delta^n$ for the output layer), so we can re-write this as:

$$
\frac{\partial J}{\partial W^i} = \delta^i \frac{\partial \text{NET}^i}{\partial W^i}
$$

The last term, $\frac{\partial \text{NET}^i}{\partial W^i}$ is derived as follows:

$$
\begin{aligned}
\text{NET}^i &= W^i \text{OUT}^{i-1} + b^i \\
\frac{\partial \text{NET}^i}{\partial W^i} &= \text{OUT}^{i-1}
\end{aligned}
$$

(This is derived the same way for a function $f = mx + b$, where $f' = m$)

So the gradient of the cost function (for the weights) is:

$$
\frac{\partial J}{\partial W^i} = \delta^i \text{OUT}^{i-1}
$$

In a similar fashion, we can compute $\frac{\partial J}{\partial b^i}$, which ends up just being:

$$
\frac{\partial J}{\partial b^i} = \delta^i
$$

Remember that $\delta^n$ includes the term $\frac{\partial J}{\partial \text{OUT}^n}$. This is the derivative of the cost function. As a reminder, $h_{\theta}(X) = \text{OUT}^n$.

In the case of MSE, the cost function derivative is:

$$
\frac{2}{m} \sum^m (h_{\theta}(X^{(i)}) - y^{(i)})
$$

## Gradient Checking

When you write code to compute the gradient, it can be very difficult to debug. Thus it is often useful to _check the gradient_ by numerically approximating the gradient and comparing it to the computed gradient.

Say our implemented gradient function is $g(\theta)$. We want to check that $g(\theta) = \frac{\partial J(\theta)}{\partial \theta}$.

We choose some $\epsilon$, e.g. $\epsilon = 0.0001$. It should be a small value, but not so small that we run into floating point precision errors.

Then we can numerically approximate the gradient at some scalar value $\theta$:

$$
\frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2 \epsilon}
$$

When $\theta$ is a vector, as is more often the case, we instead compute:

$$
\frac{J(\theta^{(i+)} - J(\theta^{(i-)})}{2 \epsilon}
$$

Where:

- $\theta^{(i+)} = \theta + (\epsilon \times e_i)$
- $\theta^{(i-)} = \theta - (\epsilon \times e_i)$
- $e_i$ is the $i$th is the basis vector (i.e. it is 0 everywhere except at the $i$th element, where it is 1)

## Autoencoders

An __autoencoder__ is a multilayer perceptron which tries to output a reproduction of its input. The hidden layer was originally designed to be smaller than the input, so that it functions like compression - the network needs to find some way of representing the input with less information. In some sense, we do this already with language, where we may represent a photo with a word (or a thousand words with a photo).

However, using a smaller hidden layer made training difficult, since encoding a lot of information into fewer bits is quite challenging.

Rather counterintuitively, a larger hidden layer helps, where some hidden units are randomly turned off during a training iteration - that way, the output isn't a mere copy of the input, and learning is easier since there is more "room" to represent the input. Such an autoencoder is called a __sparse autoencoder__.

In effect, what an autoencoder is learning is some higher-level representation of its input. In the case of an image, it may go from pixels to edges.

We can stack these sparse autoencoders on top of each other, so that higher and higher-level representations are learned. The sparse autoencoder that goes from pixels to edges can go into another one that learns how to go from edges to shapes, for example.

## Backpropagation

### Notation

![A neural network](assets/neural_network.svg)

Where:

- $L_i$ is layer $i$
- $N^i_j$ is the $j$th node in layer $i$
- $N$ is the number of layers
- $k$ is the number of outputs, e.g. classes, or $k=1$ for regression
- $n_i$ is the number of nodes in layer $i$

Given training data:

$$
\begin{aligned}
(X^{(1)}&, Y^{(1)}) \\
(X^{(2)}&, Y^{(2)}) \\
&\vdots \\
(X^{(m)}&, Y^{(m)})
\end{aligned}
$$

Where $X^{(i)} \in \mathbb R^{1 \times d}$ and

- $d$ is the dimensionality of the input
- $m$ is the number of training examples

Thus $X$ is the input matrix, $X \in \mathbb R^{m \times d}$.

For a node $N^i_j$:

- $b^i_j$ is the bias for the node (scalar), i.e. $b^i_j \in \mathbb R$
- $w^i_j$ is the weights for the node, $w^i_j \in \mathbb R^{1 \times n_{i-1}}$
- $f^i_j$ is the activation function for the node
- $\text{NET}^i_j$ is the net input for the node, $\text{NET}^i_j = W^i_j \cdot \text{OUT}^{i-1} + b^i_j$, $\text{NET}^i_j \in \mathbb R$
- $\text{OUT}^i_j$ is the output for the node, $\text{OUT}^i_j = f^i_j(\text{NET}^i_j)$, $\text{OUT}^i_j \in \mathbb R$

For a layer $L^i$:

- $b^i$ is the bias vector for the layer, $b^i \in \mathbb R^{n_i \times 1}$, i.e.

$$
b^i = \begin{bmatrix} b^i_1 \\ b^i_2 \\ \vdots \\ b^i_{n_i} \end{bmatrix}
$$

- $W^i$ is the weight matrix for the layer, $W^i \in \mathbb R^{n_i \times n_{i-1}}$, i.e.

$$
W^i = \begin{bmatrix} W^i_1 \\ W^i_2 \\ \vdots \\ W^i_{n_i} \end{bmatrix}
$$

- $f^i$ is the activation function for the layer, since generally $f^i = f^i_1 = f^i_2 = \dots = f^i_{n_i}$
- $\text{NET}^i$ is the net input for the layer:

$$
\text{NET}^i = W^i \cdot \text{OUT}^{i-1} + b^i, \text{NET}^i \in \mathbb R^{n_i \times 1}
$$

- $\text{OUT}^i$ is the output for the layer:

$$
\text{OUT}^i = f^i(\text{NET}^i), \text{OUT}^i \in \mathbb R^{n_i \times 1}
$$

Note that $\text{OUT}^N = h_{\theta}(X)$.

### Feedforward

A straightforward algorithm:

```
OUT^0 = X
for i in 1...N
    OUT^i = f^i(W^i \cdot OUT^{i-1}+b^i)
```

### Backpropagation

Backpropagation is an algorithm for computing the gradient necessary for using gradient descent to train a neural network. For each iteration, we are interested in the gradient $\nabla J$ which includes the components $\frac{\partial J}{\partial W^i_j}$ and $\frac{\partial J}{\partial b^i_j}$ (that is, how the cost function changes with respect to the weights and biases in the network).

The main advantage of backpropagation is that it allows us to compute this gradient _efficiently_. There are other ways we could do it. For instance, we could manually calculate the partial derivatives of the cost function with respect to each individual weight, which, if we had $w$ weights, would require computing the cost function $w$ times, which requires $w$ forward passes. Naturally, given a complex network with many, many weights, this becomes extremely costly to compute. The beauty of backpropagation is that we can compute these partial derivatives (that is, the gradient), with just a single forward pass.

Some notation:

- $J$ is the cost function for the network (what it is depends on the use case)
- $\delta^i$ is the error for layer $i$
- $\odot$ is the elementwise product ("Hadamard" or "Schur" product)

The error for a layer $i$ is how much the cost function changes wrt to that layer's net input, i.e. $\delta^i = \frac{\partial J}{\partial \text{NET}^i}$.

For the output layer, this is straightforward (by applying the chain rule):

$$
\delta^N = \frac{\partial J}{\partial \text{NET}^N} = \frac{\partial J}{\partial \text{OUT}^N} \frac{\partial \text{OUT}^N}{\partial \text{NET}^N}
$$

Since $\text{OUT}^N = f^N(\text{NET}^N)$, then $\frac{\partial \text{OUT}^N}{\partial \text{NET}^N} = (f^N)'(\text{NET}^N)$.

Thus we have:

$$
\delta^N = \frac{\partial J}{\partial \text{OUT}^N} (f^N)'(\text{NET}^N)
$$

Note that for $\frac{\partial J}{\partial \text{OUT}^N}$, we are computing the derivative of the cost function $J$ with respect to each training example's corresponding output, and then we average them (in some situations, such as when the total number of training examples is not fixed, their sum is used). It is costly to do this across all training examples if you have a large training set, in which case, the minibatch stochastic variant of gradient descent may be more appropriate. (TODO this may need clarification/revision)

For the hidden layer prior to the output, $L^{N-1}$, we would need to connect that layer's net input, $\text{NET}^{N-1}$, to the cost function $J$:

$$
\delta^{N-1} = \frac{\partial J}{\partial \text{NET}^{N-1}} = \frac{\partial J}{\partial \text{OUT}^N} \frac{\partial \text{OUT}^N}{\partial \text{NET}^N} \frac{\partial \text{NET}^N}{\partial \text{OUT}^{N-1}} \frac{\partial \text{OUT}^{N-1}}{\partial \text{NET}^{N-1}}
$$

We have already calculated the term $\frac{\partial J}{\partial \text{OUT}^N} \frac{\partial \text{OUT}^N}{\partial \text{NET}^N}$ as $\delta^N$, so this can be restated:

$$
\delta^{N-1} = \delta^N \frac{\partial \text{NET}^N}{\partial \text{OUT}^{N-1}} \frac{\partial \text{OUT}^{N-1}}{\partial \text{NET}^{N-1}}
$$

Since $\text{OUT}^{N-1} = f^{N-1}(\text{NET}^{N-1})$, then $\frac{\partial \text{OUT}^{N-1}}{\partial \text{NET}^{N-1}} = (f^{N-1})'(\text{NET}^{N-1})'$.

Similarly, since $\text{NET}^N = W^N \cdot OUT^{N-1} + b^N$, then $\frac{\partial \text{NET}^N}{\partial \text{OUT}^{N-1}} = W^N$.

Thus:

$$
\delta^{N-1} = W^N \delta^N \odot (f^{N-1})'(\text{NET}^{N-1})
$$

This is how we compute $\delta^i$ for all $i \neq N$, i.e. we push back (backpropagate) the next ("next" in the forward sense) layer's error, $\delta^{i+1}$, to $L^i$ to get $\delta^i$. So we can generalize the previous equation:

$$
\delta^i = W^{i+1} \delta^{i+1} \odot (f^i)'(\text{NET}^i), i \neq N
$$

We are most interested in updating weights and biases, rather than knowing the errors themselves. That is, we are most interested in the quantities:

$$
\frac{\partial J}{\partial W^i}, \frac{\partial J}{\partial b^i}
$$

For any layer $L_i$.

These are relatively easy to derive.

We want to update the weights such that the error is lowered with the new weights. Thus we compute the gradient of the error with respect to the weights and biases to learn in which way the error is increasing and by how much. Then we move in the opposite direction by that amount (typically weighted by a learning rate).

$$
\begin{aligned}
\frac{\partial J}{\partial b^i} &= \frac{\partial J}{\partial \text{NET}^i} \frac{\partial \text{NET}^i}{\partial b^i} \\
\frac{\partial J}{\partial W^i} &= \frac{\partial J}{\partial \text{NET}^i} \frac{\partial \text{NET}^i}{\partial W^i}
\end{aligned}
$$

We previously showed that $\delta^i = \frac{\partial J}{\partial \text{NET}^i}$, so here we have:

$$
\begin{aligned}
\frac{\partial J}{\partial b^i} &= \delta^i \frac{\partial \text{NET}^i}{\partial b^i} \\
\frac{\partial J}{\partial W^i} &= \delta^i \frac{\partial \text{NET}^i}{\partial W^i}
\end{aligned}
$$

Then, knowing that $\text{NET}^i = W^i \cdot \text{OUT}^{i-1} + b^i$, we get:

$$
\begin{aligned}
\frac{\partial \text{NET}^i}{\partial b^i} &= 1 \\
\frac{\partial \text{NET}^i}{\partial W^i} &= \text{OUT}^{i-1}
\end{aligned}
$$

Thus:

$$
\begin{aligned}
\frac{\partial J}{\partial b^i} &= \delta^i \\
\frac{\partial J}{\partial W^i} &= \delta^i \text{OUT}^{i-1}
\end{aligned}
$$

Then we can use these for gradient descent.

A quick bit of notation: $\delta^{j,i}$ refers to layer $i$'s error for the $j$th training example; similarly, $\text{OUT}^{j,i}$ refers to layer $i$'s output for the $j$th training example.

$$
\begin{aligned}
W^i &\to W^i - \frac{\eta}{m} \sum_j \delta^{j,i} (\text{OUT}^{j, i-1})^T \\
b^i &\to b^i - \frac{\eta}{m} \sum_j \delta^{j,i}
\end{aligned}
$$

So, to clarify, we are computing $\frac{\partial J}{\partial b^i} = \delta^i, \frac{\partial J}{\partial W^i} = \delta^i \text{OUT}^{i-1}$ for each training example and computing their average. As mentioned before, in some cases you may only take their sum, which just involves the removal of the $\frac{1}{m}$ term, so you are effectively just scaling the change.

### Cost functions

We have some cost function, which is a function of our parameters, typically notated $J(\theta)$.

For regression, this is often the mean squared error (MSE), also known as the quadratic cost:

$$
J(\theta) = \frac{1}{m} \sum^m (y^{(i)} - h_{\theta}(X^{(i)}))^2
$$

Note that $h_{\theta}$ represents the output of the entire network.

So for a single example, the cost function is:

$$
(y^{(i)} - h_{\theta}(X^{(i)}))^2
$$

For deriving convenience, we'll include a $\frac{1}{2}$ term. Including this term just scales the cost function, which doesn't impact the outcome, and for clarity, we'll substitute $f^N(\text{NET}^N)$ for $h_{\theta}(X^{(i)})$, since they are equivalent.

$$
\frac{(y^{(i)} - f^N(\text{NET}^N))^2}{2}
$$

Deriving with respect to $W^N$ and $b^N$ gives us the following for individual examples:

$$
\begin{aligned}
\frac{\partial J}{\partial W^N} &= (f^N(\text{NET}^N) - y) (f^N)'(\text{NET}^N)X^{(i)} \\
\frac{\partial J}{\partial b^N} &= (f^N(\text{NET}^N) - y) (f^N)'(\text{NET}^N)
\end{aligned}
$$

Note that these are dependent on the derivative of the output layer's activation function, $(f^N)'(NET^N)$. This can cause training to become slow in the case of activation functions like the sigmoid function. This is because the derivative near the sigmoid's tails (i.e. where it outputs values close to 0 or 1) is very low (the sigmoid flattens out at its tails). Thus, when the output layer's function has this property, and outputs values near 0 and 1 (in the case of sigmoid), this reduces the entire partial derivative, leading to small updates, which has the effect of slow learning. When slow learning of this sort occurs (that is, the kind caused by the activation functions outputting at their minimum or maximum), it is called _saturation_, and it is a common problem with neural networks.

For binary classification, a common cost function is the cross-entropy cost, also known as "log loss" or "logistic loss":

$$
J(\theta) = -\frac{1}{m} \sum_i^m \sum_j^k [y^{(i)} \ln h_{\theta}(X^{(i)}) + (1 - y^{(i)}) \ln(1 - h_{\theta}(X^{(i)}))]
$$

where $m$ is the total number of training examples and $k$ is the number of output neurons.

The partial derivatives of the cross-entropy cost with respect to $W^N$ and $b^N$ are (for brevity, we'll notate $f^N(\text{NET}^N)$ as simply $f(n)$):

$$
\begin{aligned}
\frac{\partial J}{\partial W^N} &= \sum_j^k \frac{(y-f(n))}{f(n)(1-f(n))} f'(n) X \\
\frac{\partial J}{\partial b^N} &= \sum_j^k \frac{(y-f(n))}{f(n)(1-f(n))} f'(n)
\end{aligned}
$$

This has the advantage that for some activation functions $f$, such as the sigmoid function, the activation function's derivative $f'$ cancels out, thus avoiding the training slowdown that can occur with the MSE.

However, as mentioned before, this saturation occurs with only some activation functions (like the sigmoid function). This isn't a problem, for instance, with linear activation functions, in which case quadratic cost is appropriate (though neural nets with linear activation functions are limited in what they can learn).

Thus we have:

$$
\delta^N = \frac{\partial J}{\partial \text{OUT}^N} (f^N)'(\text{NET}^N)
$$

### Softmax function

The __softmax function__ (called such because it is like a "softened" maximum function) may be used as the output layer's activation function. It takes the form:

$$
f^N_i(\text{NET}^N_i) = \frac{e^{\text{NET}^N_i}}{\sum_j^k e^{\text{NET}^N_j}}
$$

To clarify, we are summing over all the output neurons in the denominator.

This function has the properties that it sums to 1 and that all of its outputs are positive, which are useful for modeling probability distributions.

### Log-likelihood cost function

The __log-likelihood cost function__ is defined as, for a single training example:

$$
-\ln f^N_y(\text{NET}^N_y)
$$

That is, given an example that belongs to class $y$, we take the natural log of the value outputted by the output node corresponding to the class $y$ (typically this is the $y$th node, since you'd have an output node for each class). If $f^N_y(\text{NET}^N_y)$ is close to 1, then the resulting cost is low; the further it is from 1, the larger the value is.

This is assuming that the output node's activation function outputs probability-like values (such as is the case with the softmax function).

This cost function's partial derivatives with respect to $W^N$ and $b^N$ work out to be:

$$
\begin{aligned}
\frac{\partial J}{\partial W^N} &= f^{N-1}(n) (f^N(n) - y) \\
\frac{\partial J}{\partial b^N} &= f^N(n) - y
\end{aligned}
$$

For brevity, we've notated $f^N(\text{NET}^N)$ as simply $f^N(n)$, and the same for $f^{N-1}(n)$; for the latter $n=\text{NET}^{N-1}$. (TODO clean this notation up)

Note that for softmax activation functions, we avoid the saturation problem with this cost function. Thus softmax output activations and the log-likelihood cost functions are a good pairing for problems requiring probability-like outputs (such as with classification problems).

### Overfitting

Because neural networks can have so many parameters, it can be quite easy for them to overfit. Thus it is something to always keep an eye out for. This is especially a problem for large neural networks, which have huge amounts of parameters.

A simple, but possibly expensive way of reducing overfitting is by increasing the amount of training data - it's unlikely to overfit many, many examples. However, this is seldom a practical option.

Regularization is a common approach. A common regularization method is __L2 regularization__, sometimes called __weight decay__, which simply involves adding an additional term, the __regularization term__, to the cost function, such that large weights are penalized.

This term looks like:

$$
\frac{\lambda}{2m} \sum_w w^2
$$

We are summing all the network's weights (note that biases are not included), where $\lambda$ is the __regularization parameter__ and sets the strength of regularization, which in effect controls the trade off of lowering the cost function vs having large weights.

So a regularized cost function $J$, from the original unregularized cost function $J_0$, is simply:

$$
J = J_0 + \frac{\lambda}{2m} \sum_w w^2
$$

This affects the partial derivative of the cost function with respect to weights in a simple way (again, biases are not included, so it does not change that partial derivative):

$$
\frac{\partial J}{\partial w} = \frac{\partial J_0}{\partial w} + \frac{\lambda}{m} w
$$

So your update rule would be:

$$
w \to w' = \frac{\eta \lambda}{m} - \frac{\eta}{m} \sum_i^m \frac{\partial J_i}{\partial w}
$$

Empirically, regularization helps with neural network generalization, though there is not yet a solid theoretical understanding as to _why_.

Biases are typically not included by convention; regularizing them usually does not have an impact on the network's generalizability.

Other common regularization approaches include:

#### L1 regularization

Add the regularization term $\frac{\lambda}{m} \sum_w |w|$ to the cost function; that is, the sum of the absolute values of the weights, with a regularization parameter. The main difference is that L1 regularization shrinks weights by a constant amount, whereas L2 regularization shrinks weights by an amount proportional to the weights themselves. This is made clearer by considering the derived update rules from gradient descent.

For L1, this partial derivative of the cost function wrt the weights is:

$$
\frac{\partial J}{\partial W^N} = \frac{\partial J_0}{\partial W^N} + \frac{\lambda}{m} \text{sign}(W^N)
$$

This ends up leading to the following update rule:

$$
w \to w' = w - \frac{\eta \lambda}{m}\text{sign}(w) - \frac{\eta}{m} \sum_i^m \frac{\partial J_i}{\partial w}
$$

Note that we say that $\text{sign}(0) = 0$.

Compare this with the update rule for L2 regularization:

$$
w \to w' = w - \frac{\eta \lambda}{m}w - \frac{\eta}{m} \sum_i^m \frac{\partial J_i}{\partial w}
$$

In L2 regularization, we subtract a term weighted by $w$, whereas in L1 regularization, the subtracted term is affected only by the sign of $w$.

### Dropout

__Dropout__ is a regularization method which does not involve modifying cost functions. Rather, the network itself is modified. At the start of each epoch, a random half of the hidden neurons are disabled (input and output neurons remain the same), and the epoch precedes normally (forward, backpropagation, then gradient descent). At the next epoch, all neurons are restored, and another random half of hidden neurons are disabled, and so on.

When using the network, there will be twice as many hidden neurons active as there were in training, so all weights are halved to compensate.

Dropping out neurons in this way has the effect of training multiple neural networks simultaneously. If we have multiple networks overfit to different training data, they are unlikely to all overfit in the same way. So their average should provide better results.

This has the additional advantage that neurons must learn to operate in the absence of other neurons, which can have the effect of the network learning more robust features. That is, the neurons of the network should be more resilient to the absence of some information.

Dropout is usually combined with another regularization method, e.g. L2 regularization.

### Artificially expanding the training set

The training set can be artificially expanded by taking existing training data and modifying it in a way we'd expect to see in the real world.

For instance, if we were training a network to recognize handwritten digits, we may take our examples and rotate them slightly, since this could plausibly happen naturally.

A related technique is training on adversarial examples (detailed elsewhere), in which training examples are modified to be deliberately hard for the network to classify, so that it can be trained on more ambiguous/difficult examples.

## Weight initialization

What are the best values to initialize weights and biases to?

A good starting point is to draw weights and biases from a normal distribution with mean 0 and standard deviation 1.

However, this can become problematic.

Consider that the net input to a neuron is:

$$
\text{NET} = W \cdot X + b
$$

The following extends to the general case, but for simplicity, consider an input $X$ that is all ones, with dimension $d$.

Then $\text{NET}$ is a sum of $d+1$ (plus one for the bias) standard normally distributed independent random variables.

The sum of $n$ normally distributed independent random variables is:

$$
N(\sum_i^n \mu_i, \sum_i^n \sigma^2_i)
$$

That is, it is also a normal distribution.

Thus $\text{NET}$ will still have a mean of 0, but it's standard deviation will be $\sqrt{d+1}$.

If for example, $d=100$, this leaves us with a standard deviation of $\sim 10$. This is quite large, and implies that $\text{NET}$ may take on large values due to how we initialized our weights. If $\text{NET}$ takes on large values, we may run into saturation problems given an activation function such as sigmoid, which then leads to slow training. Thus, poor weight initialization can lead to slow training.

To avoid this, it is recommended that you instead initialize weights using a normal distribution with mean 0 and a standard deviation of $\frac{1}{\sqrt n_{\text{in}}}$, where $n_{\text{in}}$ is the number of inputs to that layer.

## Tweaking hyperparameters

A big challenge in designing a neural network is calibrating its hyperparameters. From the start, it may be difficult to intuit what hyperparameters need tuning. There are so many to choose from: network architecture, number of epochs, cost function, weight initialization, learning rate, etc.

There are a few heuristics which may help.

When the learning rate $\eta$ is set too high, you typically see constant oscillation in the error rate as the network trains. This is because with too large a learning rate, you may miss the minimum in the error surface by "jumping" too far. Thus once you see this occurring, it's a hint to try a lower learning rate.

Learning rates which are too low tend to have a slow decrease in error over training. You can try higher learning rates if this seems to be the case.

The learning rate does not need to be fixed. When starting out training, you may want a high learning rate to quickly get close to a minimum. But once you get closer, you may want to decrease the learning rate to carefully identify the best minimum.

The specification of how the learning rate decreases is called the __learning rate schedule__.

Some places recommend using a learning rate in the form:

$$
\eta_t = \eta_0 (1 + \eta_0 \lambda t)^{-1}
$$

Where $\eta_0$ is the initial learning rate, $\eta_t$ is the learning rate for the $t$th example, and $\lambda$ is another hyperparameter.

For the number of epochs, we can use a strategy called "early stopping", where we top once some performance metric (e.g. classification accuracy) appears to stop improving. More precisely, "stop improving" can mean when the performance metric doesn't improve for some $n$ epochs.

However, neural networks sometimes plateau for a little bit and then keep on improving. In which case, adopting an early stopping strategy can be harmful. You can be somewhat conservative and set $n$ to a higher value to play it safe.

## Momentum

__Momentum__ is a technique that can be combined with gradient descent to improve its performance.

Conceptually, it applies the idea of velocity and friction to the error surface (imagine a ball rolling around the error surface to find a minimum).

We incorporate a matrix of velocity values $V$, with the same shape as the matrix of weights and biases for the network (for simplicity, we will roll the weights and matrices together into a matrix $W$).

To do so, we break our gradient descent update rule ($W \to W' = W - \eta \nabla J$) into two separate update rules; one for updating the velocity matrix, and another for updating the weights and biases:

$$
\begin{aligned}
V \to V' &= \mu V - \eta \nabla J \\
W \to W' &= W + V'
\end{aligned}
$$

Another hyperparameter, $\mu \in [0, 1]$, is also introduced - this controls the "friction" of the system ($\mu = 1$ is no friction). It is known as the __momentum coefficient__, and it is tuned to prevent "overshooting" the minimum.

You can see that if $\mu$ is set to 0, we get the regular gradient descent update rule.

### Nesterov momentum

A variation on regular momentum is __Nesterov momentum__:

$$
\begin{aligned}
V \to V' &= \mu V - \eta \nabla J(\theta + \mu V) \\
W \to W' &= W + V'
\end{aligned}
$$

Here we add the velocity to the parameters before computing the gradient.

Using gradient descent with this momentum is also called the _Nesterov accelerated gradient_.

## Deep Learning

Training __deep__ neural networks (that is, neural networks with more than one hidden layer) is not as straightforward as it is with a single hidden layer.

This is because of __unstable gradients__. This has two ways of showing up:

- __Vanishing gradients__, in which the gradient gets smaller moving backwards through the hidden layers, such that earlier layers learn very slowly (and may not learn at all).
- __Exploding gradients__, in which the gradient gets much larger moving backwards through the hidden layers, such that earlier layers cannot find good parameters.

These unstable gradients occur because gradients in earlier layers are the products of the later layers (refer to backpropagation for details, but remember that the $\delta^i$ for layer $i$ is computed from $\delta^{i+1}$). Thus if these later terms are mostly $< 1$, we will have a vanishing gradient. If these later terms are $> 1$, they can get very large and lead to an exploding gradient.

### Convolutional nets

In a regular neural network, the relationship between a pixel and one that is next to it is the same as its relationship with a pixel far away - the structural information of the image is totally lost. Convolutional nets are especially effective with image-based tasks, since they are capable of encoding this structural information about the image.

Convolutional nets are based on three ideas:

- local receptive fields
- shared weights
- pooling

#### Local receptive fields

A regular neural network is _fully-connected_ in that every node from a layer $i$ is connected to each node in the layer $i+1$.

This is not the case with convolutional nets.

Typically we think of a layer as a line of neurons. With convolutional nets, it is more useful to think of the neurons arranged in a grid.

(Note: the following images are from <http://neuralnetworksanddeeplearning.com/chap6.html> TODO replace the graphics)

![A layer as a grid](assets/conv_net_01.png)

We do _not_ fully connect this input layer to the hidden layer (which we'll call a __convolutional layer__). Rather, we connect regions of neurons to neurons in the hidden layer. These regions are __local receptive fields__, local to the neuron at their center (they may more simply be called _windows_).

![Local receptive fields map to a neuron in the hidden layer](assets/conv_net_02.png)

We can move across local receptive fields one neuron at a time, or in greater movements. These movements are called the __stride length__.

![Moving across fields at a stride length of 1](assets/conv_net_03.png)

These windows end up learning to detect salient features, but are less sensitive to where exactly they occur. For instance, for recognizing a human face, it may be important that we see an eye in one region, but it doesn't have to be in a particular exact position. A __filter__ (also called a __kernel__) function is applied to each window to transform it into another vector (which is then passed to a pooling layer, see below).

One architectural decision with CNNs is the use of __wide convolution__ or __narrow convolution__. When you reach the edges of your input (say, the edges of an image), do you stop there or do you pad the input with zeros (or some other value) so we can fit another window? Padding the input is wide convolution, not padding is narrow convolution.

#### Shared weights

Another change here is that the hidden layer has one set of weights and a bias that is shared across the entire layer (these weights and biases are accordingly referred to as _shared weights_ and the _shared bias_, and together, they define a __filter__ or a __kernel__).

As a result, if we have receptive fields of $m \times m$ size, the output of the $i,j$th neuron in the hidden layer looks like:

$$
f(b + \sum^{m-1}_{k=0} \sum^{m-1}_{l=0} W_{k,l} \text{OUT}^0_{i+k,j+l})
$$

Where $W \in \mathbb R^{m \times m}$ is the array of shared weights and $\text{OUT}^0_{x,y}$ is the output of the input neuron at position $x,y$.

Another way of writing the above is:

$$
f(b + W * \text{OUT}^0)
$$

Where $*$ is the _convolution operator_.

The consequence of this sharing of weights and biases is that this layer detects the same feature across different receptive fields. For example, this layer could detect vertical edges anywhere in the image. If an edge shows up in the upper-right part of the image, the corresponding input neuron for that receptive field will fire. If an edge shows up in the lower-left part of the image, the corresponding input neuron for that receptive field will also fire, due to the fact that they all share weights and a bias.

As a result of this property, this mapping between layers is often called a __feature map__ (sometimes the terms "kernel" and "filter" refer to the feature map as well).

We may include multiple feature maps, i.e. have the input connect to many hidden layers of this kind. Each layer would learn to detect a different feature.

Another benefit to sharing weights and biases across the layer is that it introduces some resilience to overfitting - the sharing of weights means that the layer cannot favor peculiarities in particular parts of the training data; it must take the whole example into account. As a result, regularization methods are seldom necessary for these layers.

#### Pooling layers

In addition to convolutional layers there are also __pooling layers__, which often accompany convolutional layers (one per convolutional layer) and follow after them. Pooling layers produced a condensed version of the feature map they are given (e.g. a $2 \times 2$ neuron region of the feature map may be represented with only one neuron in the pooling layer).

![Mapping from feature map to a pooling layer](assets/conv_net_04.png)

There are a few different strategies for how this compression works. A common one is __max-pooling__, in which the pooling neuron just outputs the maximum value of its inputs. In some sense, max-pooling asks its region: was your feature present? And activates if it was. It isn't concerned with where in that region the feature was, since in practice, its precise location doesn't matter so much as its relative positioning to other features (especially with images).

Another pooling technique is __L2 pooling__. Say a pooling neuron has an $m \times m$ input region of neurons coming from the layer $i$. Then it's output is:

$$
\sqrt{\sum^{m-1}_{j=0} \sum^{m-1}_{k=0} (\text{OUT}^i_{j,k})^2}
$$

Another pooling technique is __average-pooling__ in which the average value of the input is output.

There is also the __$k$-max pooling__ method, which takes the top $k$ values in each dimension, instead of just the top value as is with max-pooling. The result is a matrix rather than a vector.


#### The network architecture

Generally, we have many feature maps (convolutional layers) and pooling layer pairs grouped together; conceptually it is often easier to think of these groups themselves as layers (called "convolutional-pooling layers").

![An example convolutional network](assets/conv_net_05.png)

The output layer is fully-connected (i.e. every neuron from the convolutional-pooling layer are connected to every neuron in the output layer).

Often it helps to include another (or more) fully-connected layer just prior to the output layer. This can be thought of as aggregating and considering all the features coming from the convolutional-pooling layer.

It is also possible to insert additional convolutional-pooling layers (this practice is called __hierarchical pooling__). Conceptually, these take the features output by the previous convolutional-pooling layer and extract higher-level features. The way these convolutional-pooling layers connect to each other is a little different. Each of this new layer's input neurons (that is, the neurons in its first set of convolutional layers) takes as its input _all_ of the outputs (within its local receptive field) from the preceding convolutional-pooling layer

For example, if the preceding convolutional-pooling layer has 20 layers in it, and we have receptive fields of size $5 \times 5$, then each of the input neurons for the new convolutional-pooling layer would have $20 \times 5 \times 5$ inputs.

#### Backpropagation

Backpropagation is slightly different for a convolutional net because the typical backpropagation assumes fully-connected layers.

TODO add this

## Deep Belief Nets

__Deep belief networks__ are a _generative_ neural network. Given some feature values, a deep belief net can be run "backwards" and generate plausible inputs. For example, if you train a DBN on handwritten digits, it can be used to generate new images of handwritten digits.

Deep belief nets are also capable of unsupervised and semi-supervised learning. In an unsupervised setting, DBNs can still learn useful features.

---

Recommended to use stochastic gradient descent (SGD)

## References

- _Neural Computing: Theory and Practice_ (1989). Philip D. Wasserman.
- MIT 6.034 (Fall 2010): Artificial Intelligence. Patrick H. Winston.
- <http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/>
- Fundamentals of Deep Learning (2015). Nikhil Buduma.
- Understanding LSTM Networks. Chris Olah. August 27, 2015. <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>
- Deep Learning. Yoshua Bengio, Ian Goodfellow, Aaron Courville. <http://www-labs.iro.umontreal.ca/~bengioy/dlbook/>
- <http://peterroelants.github.io/posts/rnn_implementation_part01/>
- <http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/>
- _Crash Introduction to Artificial Neural Networks_, Ivan Galkin: <http://ulcar.uml.edu/~iag/CS/Intro-to-ANN.html>
- <http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html>
- _The Nature of Code_, Daniel Shiffman: <http://natureofcode.com/book/chapter-10-neural-networks></http:>
- _Neural Networks and Deep Learning_, Michael A Nielsen. Determination Press, 2015. <http://neuralnetworksanddeeplearning.com>
- _Neural Networks_, Christos Stergiou & Dimitrios Siganos: <http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html>
- <http://www.ai-junkie.com/ann/evolved/nnt1.html>
A Primer on Neural Network Models for Natural Language Processing. Yoav Goldberg. October 5, 2015. <http://arxiv.org/abs/1510.00726>
