Planning is tricky because:

- environmental properties:
    - systems may be _stochastic_
    - there may be _multiple agents_ in the system
    - there may be _partial observability_ (that is, the state of the system may not be fully known)
- agent properties:
    - some information may be _unknown_
    - plans are hierarchical (high level to low level parts)

In planning we represent the world in _belief states_, in which multiple states are possible (because of incomplete information, we are not certain what the true state of the world is). Actions that are taken can either increase or decrease the possible states, in some cases down to one state.

Sequences of actions can be defined as trees, where each branch is an action and each node is a state or is an observation of the world. Then we search this tree for a plan which will satisfy the goal.


## References

- Logical Foundations of Artificial Intelligence (1987) (Chapter 12: Planning)

---

Broadly within planning, there are two kinds:

- __domain-specific planning__, in which the representations and techniques are tailored for a particular problem (e.g. path and motion planning, perception planning, manipulation planning, communication planning)
- __domain-independent planning__, in which generic representations and techniques are used

## Planning research areas and techniques:

- Domain modeling (HTN, SIPE)
- Domain description (PDDL, NIST PSL)
- Domain analysis (TIMS)
- Search methods (Heuristics, A\*)
- Graph planning algorithms (GraphPlan)
- Partial-order planning (Nonlin, UCPOP)
- Hierarchical planning (NOAH, Nonlin, O-Plan)
- Refinement planning (Kambhampati)
- Opportunistic search (OPM)
- Constraint satisfaction (CSP, OR, TMMS)
- Optimization method (NN, GA, ant colony optimization)
- Issue/flaw handling (O-Plan)
- Plan analysis (NOAH, Critics)
- Plan simulation (QinetiQ)
- Plan qualitative modeling (Excalibur)
- Plan repair (O-Plan)
- Re-planning (O-Plan)
- Plan monitoring (O-Plan, IPEM)
- Plan generalization (Macrops, EBL)
- Case-based planning (CHEF, PRODIGY)
- Plan learning (SOAR, PRODIGY)
- User interfaces (SIPE, O-Plan)
- Plan advice (SRI/Myers)
- Mixed-initiative plans (TRIPS/TRAINS)
- Planning web services (O-Plan, SHOP2)
- Plan sharing & comms (I-X, I-N-C-A)

## Representing plans and systems

We can use a __state-transition system__ as a conceptual model for planning. Such a system is described by a 4-tuple $\Sigma = (S,A,E,\gamma)$, where:

- $S = \{s_1, s_2, \dots \}$ is a finite or recursively enumerable set of states
- $A = \{a_1, a_2, \dots \}$ is a finite or recursively enumerable set of actions
- $E = \{e_1, e_2, \dots \}$ is a finite or recursively enumerable set of events
- $\gamma: S \times (A \cup E) \to 2^S$ is a state transition function (note $2^S$ is the power set of all states, that is an element of the set is itself a set of world states)

If $a \in A$ and $\gamma(s,a) \neq \emptyset$, then $a$ is _applicable_ in $s$. Applying $a$ in $s$ will take the system to $s' \in \gamma(s,a)$.

We can also represent such a state-transition system as a directed labelled graph $G=(N_G, E_G)$, where:

- the nodes correspond to the states in $S$, i.e. $N_G = S$
- there is an arc from  $s \in N_G$ to $s' \in N_G$, i.e. $s \to s' \in E_G$, with label $u \in (A \cup E)$ if and only if $s' \in \gamma(s,u)$.

A _plan_ is a structure that gives us appropriate actions to apply in order to achieve some _objective_ when starting from a given state.

The objective can be:

- a _goal state_ $s_g$ or a set of goal states $S_g$
- to satisfy some conditions over the sequence of states
- to optimize a utility function attached to states
- a task to be performed

Generally we have a _planner_ which generates a plan and the passes the plan to a _controller_ which _executes_ the actions in the plan. The execution of the action then changes the state of the system. The system, however, changes not only via the controller's actions but also through external events. So the controller must _observe_ the system using an observation function $\eta: S \to O$ and generate the appropriate action.

Sometimes, however, there may be parts of the system which we cannot observe, so, given the observations that could be collected, there may be many possible states of the world - this is the _belief state_ of the controller.

The system as it actually is is often different from how it was described to the planner (as $\Sigma$, which, as an abstraction, loses some details). In _dynamic planning_, planning and execution are more closely linked to compensate for this scenario (which is more the rule than the exception). That is the controller must _supervise_ the plan, i.e. it must detect when observations differ from expected results. The controller can pass this information to the planner as an _execution status_, and then the planner can _revise_ its plan to take into account the new state.

Planning approaches are often presented on _toy problems_, which can be quite different from real-world problems. Namely, toy problems have a concise and exact description, but real-world problems seldom, if ever, have an agreed-upon or unambiguous description. They also have important consequences, whereas toy problems do not. But toy problems provide a standard way of comparing approaches.

Some example toy problems:

- the farmers, wolves, and the river
- the sliding-block puzzle
- the $n$-queens problem
- the Dock-Worker Robots (DWR) domain (i.e. the container/block stacking problem)

## Multi-agent planning

Planners may need to work in concert with other planners or other agents (e.g. people); such scenarios introduce a few new concerns, such as how plans and outcomes are represented and communicated across agents.

## Search Problems

Consists of:

- an _initial state_
- a set of possible _actions_/applicability conditions
  - a _successor function_: from a state to a set of (action, state)
  - the successor function plus the initial state is the _state space_ (which is a directed graph as described before)
  - a _path_ (i.e. a solution)
- a _goal_ (a goal state or a goal test function)
- a _path cost_ function (for optimality, generally it is the sum of the step costs)

_Problem formulation_ can itself be a problem, as it typically is with real-world problems. We have to consider how granular/abstract we want to be  and what actions and states to include. To make this a bit easier, we typically make the following assumptions about the environment:

- finite and discrete
- fully observable
- deterministic
- static (no events)

And other assumptions are typically included as well:

- restricted goals
- sequential plans (no parallel activity in plans)
- implicit time (activities do not have a duration)
- offline planning (the state transition system is not changing while we plan)

We use _search nodes_ in search problems (nodes in the search tree), with the following data structure:

- _state_ - a state in the state space
- _parent node_ - the immediate predecessor in the search tree (only the root node has no parent)
- _action_ - the action that, when performed in the parent node's state, leads to this node's state
- _path cost_ - the path cost leading to this node
- _depth_ - the depth of this node in the search tree

The _general tree search_ algorithm is as follows:

- initialize the _fringe_ (the set of open nodes, i.e. the nodes we have not yet explored, conversely, the set of closed nodes are the nodes we have explored) with a search node for the initial state
- iteratively:
  - if the fringe is empty, return a failure
  - otherwise, select a node from the fringe based on the current search strategy
  - if this node's state passes the goal test (or is the goal state), return the path to this node
  - otherwise, _expand_ the fringe with this node's children (successors)

The particulars vary depending on the _search strategy_ used.

The _search strategy_ (also called _control strategy_), is a method for deciding which node to be expanded from the fringe (that is, it determines the order we expand nodes). The goal of the search strategy is to select nodes to reach the goal as quickly as possible.

Figuring out what node to expand next is often the key concern of a search strategy.

Some simple search strategies might be:

- LIFO (that is, select the node that has last been added, i.e. depth-first search)
- FIFO (that is, select the node that was first added, i.e. breadth-first search)


The Dock-Worker Robots problem:

- we have some containers
- we have some locations, connected by paths
- containers can be stacked onto pallets (no limit to the height of these stacks)
- we have robots (vehicles) that can have a container loaded onto them
  - each location can only have one robot at a time
- we have cranes which can pick up and stack containers (one at a time)

The available actions are:

- `move` robot $r$ from location $l$ to some adjacent and unoccupied location $l'$
- `take` container $c$ with empty crane $k$ from the top of pile $p$, all located at the same location $l$
- `put` down container $c$ held by crane $k$ on top of pile $p$, all located at location $l$
- `load` container $c$ held by crane $k$ onto unloaded robot $r$, all located at location $l$
- `unload` container $c$ with empty crane $k$ from loaded robot $r$, all located at location $l$

## Heuristic search strategies

Heuristic search strategies use information about fringe nodes to decide the node to explore next.

We have a _heuristic function_ $h: \text{state space} \to \mathbb R$. $h(n)$ is the _estimated_ (hence "heuristic") cost of the cheapest path from node $n$ to a goal node. If $n$ is a goal node, then $h(n) = 0$.

The specific heuristic function is defined depending on the particular problem (i.e. we estimate the distance to the goal state differently in different problems, for instance, with a travel route, we might estimate the cost with linear distance to the target city).

The typical trade-off with heuristics is between simplicity/efficiency and accuracy.

The question of finding good heuristics, and doing so automatically, has been a big topic in AI planning recently.

_Best-first search_ algorithms are general tree (or graph) search algorithms in which we choose as the next node: $\argmin_n f(n)$, where $f(n)$ is some _evaluation function_.

One example is _greedy best-first search_, in which $f(n) = h(n)$. However this approach often finds suboptimal solutions.

_A\* search_, on the other hand, always gives us optimal solutions. It is a refinement of best-first search, where the evaluation function $f(n) = h(n) = g(n)$. The additional function $g(n)$ computes the cost to reach the node $n$.

This evaluation function ends up giving us the estimated cost of the cheapest solution through the node $n$ (since $g(n)$ covers the path up to $n$, and $h(n)$ provides an estimate from $n$ to the goal state).

As a result, A\* may backtrack if another path looks more appealing than the current path.

A\* search is optimal if $h(n)$ is _admissible_; that is, it never overestimates the distance to the goal. It is complete as well; i.e. if a solution exists, A\* will find it.

A\* is also _optimally efficient_ (with respect to the number of expanded nodes) for a given heuristic function. That is, no other optimal algorithm is guaranteed to expand fewer nodes than A\*.


If we use A\* search where the heuristic always returns 0 (i.e. $f(n) = g(n)$), we have _uniform cost search_, also known as _Dijkstra's algorithm_.

We are often searching graphs rather than trees. Thus we need to adapt A\* for graph search.

We can adapt the general tree search algorithm to graph search with a few additions: we keep a hash table of all the fringe nodes, then, before we add a new successor, we first check if is already in the hash table. If not, we add the successor to the fringe as well as to the hash table.

However - if the given heuristic is admissible, this graph search version no longer guarantees an optimal solution. This is because when we generate a successor we have already seen, it is possible that it this time it's through a shorter path, but we don't know because we discard it. A way around this is to check when generating a redundant successor whether or not the resulting path would be shorter than the existing path through this successor; if so, then swap those paths.

The downside of A\* vs greedy best-first search is that it can be slower since it explores the space more thoroughly - it has worst case time and space complexity of $O(B^l)$, where $b$ is the branching factor (the number of successors per node on average) and $l$ is the length of the path we're looking for.

Typically we are dealing with the worst case; the fringe usually grows exponentially. Sometimes the time complexity is permissible, but the space complexity is problematic because there may simply not be enough memory for some problems.

There is a variation of A\* called _iterative deepening A\*_ (IDA\*) which uses significantly less memory.

## Permutations

A _permutation_ of a solution is a case in which some actions in the path to the solution can have their order changed without affecting the success of the path (that is, the permuted path still leads to the solution with the same cost). In this case, the actions are said to be _independent_.


## STRIPS (Stanford Research Institute Problem Solver)

The STRIPS representation gives us an internal structure to our states, which up until now have been left as black boxes. It is based on first order predicate logic; that is, we have objects in our domain, represented by symbols and grouped according to type, and these objects are related (such relationships are known as _predicates_) to each other in some way.

For example, in the Dock-Worker Robot domain, one type of object is _robot_ and each robot would be represented with a unique symbol, e.g. `robot1`, `robot2`, etc.

We must specify all of this in a syntax that the planner can understand. The most common syntax is PDDL (Planning Domain Definition Language). For example:

```
(define (domain dock-worker-robot))

  (:requirements :strips :typing)

  (:types
    location  ;there are several connected locations
    pile      ;is attached to a location,
              ;it holds a pallet and a stack of containers
    robot     ;holds at most 1 container,
              ;only 1 robot per location
    crane     ;belongs to a location to pickup containers
    container
  )

  (:predicates
    (adjacent ?l1 ?l2 - location)       ;location ?l1 is adjacent to ?l2
    (attached ?p - pile ?l - location)  ;pile ?p attached to location ?l
    (belong ?k - crane ?l - location)   ;crane ?k belongs to location ?l
    (at ?r - robot ?l - location)       ;robot ?r is at location ?l
    (occupied ?l - location)            ;there is a robot at location ?l
    (loaded ?r - robot ?c - container)  ;robot ?r is loaded with container ?c
    (unloaded ?r - robot)               ;robot ?r is empty
    (holding ?k - crane ?c - container) ;crane ?k is holding a container ?c
    (empty ?k - crane)                  ;crane ?k is empty
    (in ?c - container ?p - pile)       ;container ?c is within pile ?p
    (top ?c - container ?p - pile)      ;container ?c on top of pile ?p
    (on ?c1 ?c2 - container)            ;container ?c1 is on container ?c2
  )

)
```

Let $\mathcal{L}$ be a first-order language with finitely many predicate symbols, finitely many constant symbols, and no function symbols (e.g. as defined with PDDL above).

A state in a STRIPS planning domain is a set of _ground atoms_ of $\mathcal{L}$. An _atom_ is a predicate with an appropriate number of objects (e.g. those we defined above). An atom is _ground_ if all its objects are real objects (rather than variables).

- (ground) atom $p$ _holds_ in state $s$ if and only if $p \in s$ (this is the _closed world assumption_); i.e. it is "true"
- $s$ satisfies a set of (ground) literals (a literal is an atom that is either positive or negative, e.g. an atom or a negated atom) $g$ (denoted $s \vDash g$) if:
  - every positive literal in $g$ is in $s$
  - every negative literal in $g$ is not in $s$

Say we have the symbols `loc1, loc2, p1, p2, crane1, r1, c1, c2, c3, pallet`. An example state for the DWR problem:

```
state = {
  adjacent(loc1, loc2), adjacent(loc2, loc1),
  attached(p1, loc1), attached(p2, loc1),
  belong(crane1, loc1),
  occupied(loc2),
  empty(crane1),
  at(r1,loc2),
  unloaded(r1),
  in(c1,p1), in(c3,p1),
  on(c3,c1), on(c1,pallet),
  top(c3,p1),
  in(c2,p2),
  on(c2,pallet),
  top(c2,p2)
}
```

In STRIPS, a _planning operator_ is a triple $o = \text{name}(o), \text{precond}(o), \text{effects}(o)$, where:

- the name of the operator $\text{name}(o)$ is a syntactic expression of the form $n(x_1, \dots, x_k)$ where $n$ is a unique symbol and $x_1, \dots, x_k$ are all variables that appear in $o$ (i.e. it is a function signature)
- the preconditions $\text{precond}(o)$ and the effects $\text{effects(o)}$ of the operator are sets of literals (i.e. positive or negative atoms)
  - the positive effects form the _add list_
  - the negative effects form the _delete list_

An _action_ in STRIPS is a ground instance of a planning operator (that is, we substitute the variables for symbols, e.g. we are "calling" the operator, as in a function).

For example, we may have an operator named `move(r,l,m)` with the preconditions `adjacent(l,m), at(r,l), !occupied(m)` and the effects `at(r,m), occupied(m), !occupied(l), !at(r,l)`. An action might be `move(robot1, loc1, loc2)`, since we are specifying specific instances to operate on.

In the PDDL syntax, this can be written:

```
(:action move
  :parameters (?r - robot ?from ?to - location)
  :precondition (and
    (adjacent ?from ?to) (at ?r ?from)
    (not (occupied ?to)))
  :effect (and
    (at ?r ?to) (occupied ?to)
    (not (occupied ?from)) (not (at ?r ?from)) ))
```

This is a bit confusing because PDDL does not distinguish "action" from "operator".

### Applicability and state transitions

When is an action applicable in a state?

Let $L$ be the set of literals. $L^+$ is the set of atoms that are positive literals in $L$ and $L^-$ is the set of all atoms whose negations are in $L$.

Let $a$ be an action and $s$ a state. $a$ is _applicable_ in $s$ if and only if:

- $\text{precond}^+(a) \subseteq s$
- $\text{precond}^-(a) \cap s = \emptyset$

Which just says all positive preconditions must be true in the current state, and all negative preconditions must be false in the state.

The state transition function $\gamma$ for an applicable action $a$ in state $s$ is defined as:

$$
\gamma(s,a) = (s - \text{effects}^-(a)) \cup \text{effects}^+(a)
$$

That is, we apply the delete list (remove those effects from the state) and apply the add list (add those effects to the state).

Finding actions applicable for a given state is a non-trivial problem, in particular because there may be many, many available actions.

We can define an algorithm which will find the applicable actions for a given operator in a given state:

- initialize:
  - `A` is a set of actions, initially empty
  - `op` is the operator
  - `precs` is the list of remaining preconditions to be satisfied
  - `v` is the substitutions for the variables of the operator
  - `s` is the given state
- function `addApplicables(A, op, precs, v, s)`
  - if no positive preconditions remaining
    - for every negative precondition `np` in `s`
      - if the state falsifies the `np`, return
    - add `v(op)` to `A`
  - else:
    - select the next positive precondition `pp`
    - for each proposition `sp` in `s`
      - extend 'v' such that `pp` and `sp` match, the result is `v'`
      - if `v'` is valid, then:
        - `addApplicables(A, op, (precs - pp), v', s)`

We can formally define a _planning domain_ in STRIPS.

Given our function-free first-order language $\mathcal{L}$, a STRIPS planning domain on $\mathcal{L}$ is a _restricted_ (meaning there are no events) state-transition system $\Sigma = (S, A, \gamma)$ such that:

- $S$ is a set of STRIPS states, i.e. sets of ground atoms
- $A$ is a set of ground instances of some STRIPS planning operators $O$ (i.e. actions)
- $\gamma S \times A \to S$ where
  - $\gamma(s,a) = (s - \text{effects}^-(a)) \cup \text{effects}^+(a)$ if $a$ is applicable in $s$
  - $\gamma(s,a) = \text{undefined}$ otherwise
- $S$ is closed under $\gamma$

We can formally define a _planning problem_ as a triple $\mathcal{P} = (\Sigma, s_i, g)$, where:

- $\Sigma$ is the STRIPS planning domain (as described above)
- $s_i \in S$ is the initial state
- $g$ is a set of ground literals describing the goal such that the set of goal states is $S_g = \{s \in S|s \vDash g\}$ (as a reminder, $s \vDash g$ means $s$ satisfies $g$)

In PDDL syntax, we can define the initial state like so:

```
(:init
  (adjacent l1 l2)
  (adjacent l2 l1)
  ;etc
)
```

and the goal like so:

```
(:goal (and
    (in c1 p2) (in c2 p2)
    ;etc
))
```

We formally define a _plan_ as any sequence of actions $\pi = a_1, \dots, a_k$ where $k \geq 0$:

- The length of a plan $\pi$ is $|\pi|=k$, i.e. the number of actions
- If $\pi_1 = a_1, \dots, a_k$ and $\pi_2 = a_1', \dots, a_j'$ are plans, their _concatenation_ is the plan $\pi_1 \cdot \pi_2 = a_1, \dots, a_k, a_1', \dots, a_j'$
- The extended state transition function for plans is defined as follows:
  - $\gamma(s, \pi) = s$ if $k=0$ (that is, if $\pi$ is empty)
  - $\gamma(s, \pi) = \gamma(\gamma(s,a_1), a_2, \dots, a_k)$ if $k>0$ and $a_1$ is applicable in $s$
  - $\gamma(s, \pi) = \text{undefined}$ otherwise

A plan $\pi$ is a solution for a planning problem $\mathcal{P}$ if $\gamma(s_i, \pi)$ satisfies $g$.

A solution $\pi$ is _redundant_ if there is a proper subsequence of $\pi$ that is also a solution for $\mathcal{P}$.

A solution $\pi$ is _minimal_ if no other solution for $\mathcal{P}$ contains fewer actions that $\pi$.

### Other representations

Representations other than STRIPS includes:

- _propositional reprsentation_:
  - world state is a set of propositions (i.e. only symbols, no variables)
  - actions consist of precondition propositions, propositions to be added and removed (i.e. there are no operators b/c we only have symbols)
  - so the STRIPS representation is essentially propositional representation but with first-order literals instead of propositions (i.e. the preconditions of an operator can be positive or negative)
- _state-variable representation_:
  - state is a tuple of state variables $\{x_1, \dots, x_n\}$
  - an action is a partial function over states

These representations, however, can all be translated between each other.

### Forward Search

The basic idea is to apply standard search algorithms (e.g. bread-first, depth-first, A\*, etc) to the planning problem.

- search space is a subset of the state space
- nodes correspond to world states
- arcs correspond to state transitions
- path in the search space corresponds to plan

Forward search is _sound_ (if a plan is returned, it will indeed be a solution) and it is _complete_ (if a solution exists, it will be found).

### Backward Search

Alternatively, we can search backwards from a goal state to the initial state.

First we define two new concepts:

An action $a \in A$ is _relevant_ for $g$ if:

- $g \cap \text{effects}(a) \neq \emptyset$
- $g^+ \cap \text{effects}^-(a) = \emptyset$
- $g^- \cap \text{effects}^+(a) = \emptyset$

Essentially what this is says is the action must contribute the goal (the first item) and the action must not interfere with the goal (the last two items).

This is equivalent to applicability.

The _regression set_ of $g$ for a relevant action $a \in A$ is:

$$
\gamma^{-1}(g,a) = (g - \text{effects}(a)) \cup \text{precond}(a)
$$

That is, it is the inverse of the state transition function.

When searching backwards, sometimes we end up with operators rather than actions (i.e. some of the parameters are still variables). We could in theory branch out to all possible actions from this operator by just substituting all possible values for the variable, but that will increase the branching factor by a lot. Instead, we can do _lifted backward search_, in which we just stick with these _partially instantiated operators_ instead of actions.

Keeping variables in a plan, such as with lifted backward search, is called _least commitment planning_.

## Partial plans (plan-space search)

Partial plans are like plans mentioned thus far (i.e. simple sequences of actions), but we also record the rationale behind each action, e.g. to achieve the precondition of another action. In aggregate these partial plans may form the solution to the problem (i.e. they act as component plans).

We also have explicit ordering constraints (i.e. these actions must occur in this order), so we can have partial plans with _partial order_, which means that we can execute actions in parallel.

And as with lifted backward search, we may have variables in our actions as well.

We adjust the planning problem a bit - instead of achieving goals, we want to accomplish _tasks_. Tasks are high-level descriptions of some activity we want to execute - this is typically accomplished by decomposing the high-level task into lower-level subtasks. This is the approach for __Hierarchical Task Network (HTN) planning__. There is a simpler version called STN planning as well.

Rather than searching through the state-space, we search through _plan-space_ - a graph of partial plans. The nodes are partially-specified plans, the arcs are _plan refinement operations_ (which is why this is called _refinement planning_), and the solutions are _partial-order plans_.

More concretely, if a plan is a set of actions organized into some structure, then a partial plan is:

- a subset of the actions
- a subset of the organizational structure
  - temporal ordering of actions
  - rationale: what the action achieves in the plan
- a subset of variable bindings

More formally, we define a partial plan as a tuple $\pi = (A, \prec, B, L)$ where:

- $A = \{a_1, \dots, a_k\}$ is a set of partially-instantiated planning operators
- $\prec$ is a set of ordering constraints on $A$ of the form $(a_i \prec a_j)$
- $B$ is a set of binding constraints on the variables of actions in $A$ of the form $x=y, x \neq y$ or  $x \in D_x$
- $L$ is a set of causal links of the form $a_i \to [p] \to a_j$ such that:
  - $a_i, a_j$ are actions in $A$
  - the constraint $(a_i \prec a_j)$ is in $\prec$
  - the proposition $p$ is an effect of $a_i$ and a precondition of $a_j$
  - the binding constraints for variables in $a_i$ and $a_j$ appearing $p$ are in $B$

Note that for causal links, $a_i$ is the _producer_ in the causal link and $a_j$ is the _consumer_.

### Plan refinement operations

#### Adding actions

With least-commitment planning, we only want to add actions (more specifically, we are adding partially-instantiated operators) if it's justified:

- to achieve unsatisfied preconditions
- to achieve unsatisfied goal conditions

Actions can be added anywhere in the plan.

Note that each action that we add has its own set of variables, unrelated to those of other actions.

#### Adding causal links

Causal links link a provider (an effect of an action or an atom that holds in the initial state) to a consumer (a precondition of an action or a goal condition). There is an ordering constraint here as well, in that the provider must come before the consumer (but not necessarily _directly_ before).

We add causal links to prevent interference with other actions.

#### Adding variable bindings

A solution plan must have actions, not partially-instantiated operators. Variable bindings are what allow us to turn operators into actions.

Variable bindings constraints keep track of possible values for variables, and also can specify co-designation (i.e. that certain variables must or must not have the same value). For example, with causal links, there are variables in the producer that must be the same as those corresponding variables in the consumer (because they are "carried over"). When two variables must share the same value, we say they are _unified_.

#### Adding ordering constraints

Ordering constraints are just binary relations specifying the temporal order between actions in a plan.

Ordering constraints help us avoid possible interference. Causal links imply ordering constraints, and some trivial ordering constraints are that all actions must come after the initial state and before the goal.

### The Plan-Space Search Problem

The initial search state includes just the initial state and the goal as "dummy actions":

- an `init` action with no preconditions and with the initial state as its effects
- a `goal` action with the goal conditions as its preconditions and with no effects

We start with the empty plan: $\pi_0 = (\{\text{init, goal}\}, \{(\text{init} \prec \text{goal})\}, \{\}, \{\})$

It includes just the two dummy actions, one ordering constraint (`init` before `goal`), and no variable bindings or causal links.

We generate successors through one or more plan refinement operators:

- adding an action to $A$
- adding an ordering constraint to $\prec$
- adding a binding constraint to $B$
- adding a causal link to $L$

### Threats and flaws

A _threat_ in a partial plan is when we have an action that might occur in parallel with a causal link and has an effect that is complimentary to the condition we want to protect (that is, it interferes with a condition we want to protect).

We can often get around this by introducing a new causal link that requires this conflicting action to follow the causal link, instead of occurring in parallel.

More formally, an action $a_k$ in a partial plan is a thread to a causal link $a_i \to [p] \to a_j$ if and only if:

- $a_k$ has an effect $\not q$ that is possibly inconsistent with $p$, i.e. $q$ and $p$ are unifiable
- the ordering constraints $(a_i \prec a_k)$ and $(a_k \prec a_j)$ are consistent with $\prec$
- the binding constraints for the unification $q$ and $p$ are consistent with $B$

That is, if we have one action which produces the precondition $p$, which is what we want, but another action which simultaneously produces the precondition $\not q$ where $q$ and $p$ are unifiable, then we have a threat.

A _flaw_ in a partial plan is either:

- an unsatisfied subgoal, e.g. a precondition of an action in $A$ without a causal link that supports it, or
- a threat

### Partial order solutions

We consider a plan $\pi=(A, \prec, B, L)$ as partial order solution for a planning problem $\mathcal P$ if:

- its ordering constraints $\prec$ are not circular
- its binding constraints $B$ are consistent
- it is flawless

## The Plan-Space Planning (PSP) algorithm

The main principle is to refine the partial plan $\pi$ while maintaining $\prec$ and $B$ consistent until $\pi$ has no more flaws.

Basic operations:

- find the flaws of $\pi$
- select a flaw
- find a way of resolving the chosen flaw
- choose one of the resolvers for the flaw
- refine $\pi$ according to the chosen resolver

The general algorithm:

```
function PSP(plan):
  all_flaws = plan.open_goals() + plan.threats()
  if not all_flaws:
    return plan
  flaw = all_flaws.select_one()
  all_resolvers = flaw.get_resolvers(plan)
  if not all_resolves:
    return failure
  resolver = all_resolvers.choose_one()
  new_plan = plan.refine(resolver)
  return PSP(new_plan)
```

Where the initial `plan` is the empty plan as described previously.

`all_resolvers.choose_one` is a non-deterministic decision (i.e. this is something we may need to backtrack to; that is, if one resolver doesn't work out, we need to try another branch).

`all_flaws.select_one` is a deterministic selection (we don't need to backtrack to this because all flaws must be resolved). The order is not important for completeness, but it is important for efficiency.

### Implementing `plan.open_goals()`

We find unachieved subgoals incrementally:

- the goal conditions $\pi_0$ are the initial unachieved subgoals
- when adding an action: all preconditions are unachieved subgoals
- when adding a causal link, the protected proposition is no longer unachieved

### Implementing `plan.threats()`

We find threats incrementally:

- no threats in the goal conditions $\pi_0$
- when adding an action $a_{\text{new}}$ to $\pi = (A, \prec, B, L)$:
  - for every causal link $(a_i \to [p] \to a_j) \in L$
    - if $(a_{\text{new}} \prec a_i)$ or $(a_j \prec a_{\text{new}})$, then next link
    - else for every effect $q$ of $a_{\text{new}}$
      - if $\exists \sigma: \sigma(p) = \sigma(\not q))$ then $q$ of $a_{\text{new}}$ threatens $(a_i \to [p] \to a_j)$
- when adding a causal link $(a_i \to [p] \to a_j)$ to $\pi = (A, \prec, B, L)$:
  - for every action $a_{\text{old}} \in A$
    - if $(a_{\text{old}} \prec a_i)$ or $(a_j \prec a_{\text{old}})$, then next action
    - else for every effect $q$ of $a_{\text{old}}$
      - if $\exists \sigma: \sigma(p) = \sigma(\not q))$ then $q$ of $a_{\text{old}}$ threatens $(a_i \to [p] \to a_j)$

### Implementing `flaw.get_resolvers(plan)`

For an unachieved precondition $p$ of $a_g$:

- add causal links to an existing action:
  - for every action $a_{\text{old}} \in A$, see if an existing action can be a provider for this precondition
    - if $(a_g = a_{\text{old}})$ or $(a_g \prec a_{\text{old}})$ then next action (i.e. if the existing action _is_ the consumer or it comes after the consumer, then it cannot be a producer, so just move on to the next action)
    - else for every effect $q$ of $a_{\text{old}}$, check if the existing action produces a precondition that is equal to the unachieved precondition:
      - if $\exists \sigma: \sigma(p) = \sigma(q))$ then adding $a_{\text{old}} \to [\sigma(p)] \to a_g$ is a resolver
- add a new action and a causal link (i.e. create a new provider):
  - for every effect $q$ of every operator $o$
    - if $\exists \sigma: \sigma(p) = \sigma(q))$ then adding $a_{\text{new}} = $ `o.newInstance()` and $a_{\text{new}} \to [\sigma(p)] \to a_g$ is a resolver

For an effect $q$ of action $a_t$ threatening $a_i \to [p] \to a_j$:

- order action before threatened link:
  - if $(a_t = a_i)$ or $(a_j \prec a_t)$, then not a resolver
  - else adding $(a_t \prec a_i)$ is a resolver
- order threatened link before action:
  - if $(a_t = a_i)$ or $(a_t \prec a_i)$, then not a resolver
  - else adding $(a_j \prec a_t)$ is a resolver
- extend variable bindings such that unification fails:
  - for every variable $v$ in $p$ or $q$
    - if $v \neq \sigma(v)$ is consistent with $B$ then adding $v \neq \sigma(V)$ is a resolver

### Implementing `plan.refine(resolver)`

Refines a partial plan by adding elements specified in `resolver`, i.e.:

- an ordering constraint;
- one or more binding constraints;
- a causal link; and/or
- a new action

This may introduce new flaws, so we must update the flaws (i.e. `plan.open_goals()` and `plan.threats()`).

### Implementing ordering constraints

Ordering constraint management can be implemented as an independent module with two operations:

- querying whether $(a_i \prec a_j)$
- adding $(a_i \prec a_j)$

### Implementing variable binding constraints

Types of constraints:

- unary constraints: $x \in D_x$
- equality constraints: $x=y$
- inequalities: $x \neq y$

Unary and equality constraints can be dealt with in linear time, but inequality constraints cause exponential complexity here - with inequalities, this is a _general constraint satisfaction problem_ which is NP-complete. So these variable binding constraints can become problematic.

### Soundness and completeness

The PSP procedure is sound and complete whenever $\pi_0$ can be refined into a solution plan and $\text{PSP}(\pi_0)$ returns such a plan.

To clarify, soundness: if PSP returns a plan, it is a solution plan, and completeness: if there is a solution plan, PSP will return it.

Proof:

- soundness: $\prec$ and $B$ are consistent at every stage of refinement
- completeness: induction on the number of actions in the solution plan

### The UC Partial-Order Planning (UCPoP) Planner

This is slight variation to PSP as outlined above, in which threats are instead dealt with after an open goal is dealt with (that is, it deals with threats from the resolver that was used to deal with an open goal, which is to say that threats are resolved as part of the successor generation process).

UCPoP takes in a slightly different input as well. In addition to the partial plan, it also takes in an _agenda_, which is a set of $(a,p)$ pairs where $a$ is an action and $p$ is one of its preconditions (i.e. this is a list of things that still need to be dealt with, that is the remaining open goal flaws).

## Overview: state-space planning vs plan-space planning

- state-space planning
  - finite search space
  - explicit representation of intermediate states
  - action ordering reflects control strategy
  - causal structure only implicit
  - search nodes relatively simple and successors are easy to compute
- plan-space planning
  - infinite search space
  - no explicit intermediate states
  - choice of actions and organization are independent
  - explicit representation of rationale
  - search nodes are complex and successors are expensive to compute

Nowadays with efficient heuristics, state-space planning is the more efficient way for finding solutions.

## Tasks

With task network planning, we still have terms, literals, operators, actions, state transition functions, and plans.

A few new things are added:

- the tasks to be performed
- methods describing ways in which tasks can be performed
- organized collections of tasks called task networks

Formally:

- we have task symbols $T_s = \{t_1, \dots, t_n\}$ for giving unique names to tasks
  - the operator names must be $\subsetneq T_s$ (that is, it must be a subset of the task symbols and cannot be equal to the entire set). Operator names that correspond to task symbols are called _primitive tasks_.
  - non-primitive task symbols are $T_s - \text{operator names}$, i.e. task symbols with no corresponding operators.
- task: $t_i(r_1, \dots, r_k)$
  - $t_i$ is the task symbol (primitive or non-primitive)
  - $r_1, \dots, r_k$ are terms or objects manipulated by the task
  - a ground task is one in which all its parameters are ground (i.e. they are actual objects, not variables)
- action: $a=\text{op}(c_1, \dots, c_k)$, where $\text{op}$ is an operator name and $c_1, \dots, c_k$ are constants representing parameters for the action, _accomplishes_ ground primitive task $t_i(r_1, \dots, r_k)$ in state $s$ if and only if:
  - $\text{name}(a) = t_i$, i.e. the name of the action must be the task symbol, and $c_1 = r_1$ and $\dots$ and $c_k = r_k$(i.e. the parameters must be the same)
  - $a$ is applicable in $s$

### Simple Task Networks (STN)

We can group tasks into _task networks_.

A _simple task network_ $w$ is an acyclic directed graph $(U, E)$ in which:

- the node set $U = \{t_1, \dots, \t_n\}$ is a set of tasks
- the edges in $E$ define a partial ordering of the tasks in $U$

A task network $w$ is ground/primitive if all tasks $t_u \in U$ are ground/primitive, otherwise it is unground/non-primitive.

A task network may also be _totally ordered_ or _partially ordered_. We have an ordering $t_u \prec t_v$ in $w$ if there is a path from $t_u$ to $t_v$.

A network $w$ is _totally ordered_ if and only if $E$ defines a total order on $U$ (that is, that every node is ordered with respect to every other node in the network). If $w$ is totally ordered, we can represent it as a sequence of tasks $t_1, \dots, t_n$).

Let $w = t_1, \dots, t_n$ be a totally ordered, ground, primitive STN. Then the plan $\pi(w)$ is defined as:

$$
\pi(w) = a_1, \dots, a_n
$$

Where $a_i = t_i, 1 \leq i \leq n$.

Simple task networks are a simplification of the more general hierarchical task networks (HTNs).

#### Example (DWR)

Tasks:

- $t_1 = \text{take}(\text{crane}, \text{loc}, c_1, c_2, p_1)$ (primitive, because we have an operator of that same name in the DWR domain, and ground, because all arguments here are objects and not variables)
- $t_2 = \text{take}(\text{crane}, \text{loc}, c_2, c_3, p_1)$ (primitive, ground)
- $t_3 = \text{move-stack}(p_1, q)$ (non-primitive, because we do not have an operator named "move-stack" in the DWR domain, and unground because $q$ is a variable)

Task networks:

- $w_1 = (\{t_1, t_2, t_3\}, \{(t_1, t_2), (t_1, t_3)\})$ (partially-ordered, because we don't have an order specified for $t_2, t_3$, non-primitive, because the non-primitive task $t_3$ is included, and unground, because the unground task $t_3$ is included.
- $w_2 = (\{t_1, t_2\}, \{(t_1, t_2)\})$ (totally ordered, ground, primitive)
  - $\pi(w_2) = t_1, t_2$

### Methods

_Methods_ are plan refinements (i.e. they correspond to state transitions in our search space).

Let $M_S$ be a set of method symbols. A STN method is a 4-tuple $m=(\text{name}(m), \text{task}(m), \text{precond}(m), \text{network}(m))$ where:

- $\text{name}(m)$ is the name of the method
  - it is a syntactic expression of the form $n(x_1, \dots, x_k)$
    - $n \in M_S$ is a unique method symbol
    - $x_1, \dots, x_k$ are all the variable symbols that occur in $m$
- $\text{task}(m)$ is a non-primitive task (primitive tasks can just be accomplished by an operator) accomplished by this method
- $\text{precond}(m)$ is a set of literals; the method's preconditions
- $\text{network}(m)$ is a task network $(U,E)$ containing the set of _subtasks_, which is $U$, of $m$

#### Example (DWR)

Say we want to define a method which involves taking the topmost container of a stack and moving it. If you recall, we have the operators `take` and `put`, which we can use to define this method.

The name of the method could be $\text{take-and-put}(c,k,l,p_o,p_d,x_o,x_d)$. The task this method completes would be $\text{move-topmost}(p_o, p_d)$. The preconditions would be $\text{top}(c,p_0), \text{on}(c, x_o), \text{attached}(p_o,l), \text{belong}(k,l), \text{attached}(p_d,l), \text{top}(x_d,p_d)$. Finally, the subtasks would be $\text{take}(k,l,c,x_o,p_o), \text{put}(k,l,c,x_d,p_d)$.

Where:

- $c$ is the container to move
- $k$ is the crane to use
- $l$ is the location
- $p_o$ is the original pile
- $p_d$ is the destination pile
- $x_o$ is the container from which we are taking $c$
- $x_d$ is the container which we are placing $c$ on

#### Applicability and relevance

A method instance $m$ is _applicable_ in a state $s$ if:

- $\text{precond}^+(m) \subseteq s$
- $\text{precond}^-(m) \cap s = \emptyset$

A method instance $m$ is _relevant_ for a task $t$ if there is a substitution $\sigma$ such that $\sigma(t) = \text{task}(m)$

#### Decomposition of tasks

The _decomposition_ of an individual task $t$ by a relevant method $m$ under $\sigma$ is either:

- $\delta(t,m,\sigma) = \sigma(\text{network}(m))$ or
- $\delta(t,m,\sigma) = \sigma(\text{subtasks}(m))$ if $m$ is totally ordered

$\delta$ is called the decomposition method for a task given a method and a substitution.

That is, we break the task down into its subtasks.

The decomposition of tasks in a STN is as follows:

Let:

- $w = (U,E)$ be a STN
- $t \in U$ be a task with no predecessors in $w$
- $m$ be a method that is relevant for $t$ under some substitution $\sigma$ with $\text{network}(m) = (U_m, E_m)$

The decomposition of $t$ in $w$ by $m$ under $\sigma$ is the STN $\delta(w,t,m,\sigma)$ where:

- $t$ is replaced in $U$ by $\sigma(U_m)$
- edges in $E$ involving $t$ are replaced by edges to appropriate nodes in $\sigma(U_m)$

That is, we replace the task with its subtasks.

### Planning Domains and Problems

An STN planning domain is a pair $\mathcal D = (O,M)$ where:

- $O$ is a set of STRIPS planning operators
- $M$ is a set of STN methods

$\mathcal D$ is a _total-order STN planning domain_ if every $m \in M$ is totally ordered.

An STN planning problem is a 4-tuple $\mathcal P = (s_i, w_i, O, M)$ where:

- $s_i$ is the initial state (a set of ground atoms)
- $w_i$ is a task network called the _initial task network_
- $\mathcal D=(O,M)$ is an STN planning domain

So it is quite similar to a STRIPS planning problem (just the domain also includes STN methods and we have a initial task network).

$\mathcal P$ is a _total-order STN planning problem_ if $w_i$ and $\mathcal D$ are both totally ordered.

A plan $\pi = a_1, \dots, a_n$ is a solution for an STN planning problem $\mathcal P = (s_i, w_i, O, M)$ if:

- $w_i$ is empty and $\pi$ is empty (i.e. if we had no tasks, doing nothing is a solution)

or:

- there is a primitive task $t \in w_i$ that has no predecessors in $w_i$ (i.e. it is one of the first tasks)
- $a_1 = t$ is applicable in $s_i$
- $pi'=a_2, \dots, a_n$ is a solution for $\mathcal P' = (\gamma(s_i, a_1), w_i - \{t\}, O, M)$ (i.e. recurse)

or:

- there is a non-primitive task $t \in w_i$ that has no predecessors in $w_i$ (i.e. it is one of the first tasks)
- a method $m \in M$ is relevant for $t$, i.e. $\sigma(t) = \text{task}(m)$ and applicable in $s_i$
- $\pi$ is a solution for $\mathcal P' = (s_i, \delta(w_i, t, m, \sigma), O, M)$

### Planning with task networks

#### The Ground Total order Forward Decomposition (Ground-TFD) procedure

```
function GroundTFD(s1, (t1, ..., tk), O, M):
  if k=0 return []
  if t1.is_primitive() then
    actions = {(a, sigma) | a = sigma(t1) and a.applicable_in(s)}
    if not actions then return failure
    (a, sigma) = actions.choose_one() # non-deterministic choice, we may need to backtrack to here
    plan = GroundTFD(gamma(s,a), sigma((t2, ..., tk)), O, M)
    if plan = failure then return failure
    else return [a].extend(plan)
  else:
    methods = {(m, sigma) | m.relevant_for(sigma(t1) and m.applicable_in(s)}
    if not methods then return failure
    (m, sigma) = methods.choose_one()
    plan = subtasks(m).extend(sigma((t2, ..., tk)))
    return GroundTFD(s, plan, O, M)
```

TFD considers only _applicable_ actions, much like forward search, and it only considers _relevant_ actions, much like backward search.

Ground-TFD can be generalized to Lifted-TFD, giving the same advantages as lifted backward search (e.g. least commitment planning).

#### The Ground Partial order Forward Decomposition (Ground-PFD) procedure

```
function GroundPFD(s1, w, O, M):
  if w.U = {} return []
  task = {t in U| t.no_predecessors_in(w.E)}.choose_one()
  if task.is_primitive() then
    actions = {(a, sigma) | a = sigma(t1) and a.applicable_in(s)}
    if not actions then return failure
    (a, sigma) = actions.choose_one()
    plan = GroundPFD(gamma(s,a), sigma(w-{task}), O, M)
  else:
    methods = {(m, sigma) | m.relevant_for(sigma(t1) and m.applicable_in(s)}
    if not methods then return failure
    (m, sigma) = methods.choose_one()
    return GroundPFD(s, delta(w,task,m,sigma), O, M)
```

## Hierarchical Task Network (HTN) planning

HTN planning is more general than STN planning, which also means there is no single algorithm for implementing HTN planning.

In STN planning, we had two types of constraints:

- ordering constraints, which were maintained in the network
- preconditions (constraints on a state before a method or action is applied):
  - enforced as part of the planning procedure
  - must know the state to test for applicability
  - must perform forward search

HTN planning has the flexibility to use these constraints or other arbitrary constraints as needed; that is, it maintains more general constraints explicitly (in contrast, with STN planning the constraints are embedded as part of the network or the planning). For instance, we could include constraints on the resources used by the tasks.

HTN methods are different than STN methods. Formally:

Let $M_S$ be a set of method symbols. An HTN method is a 4-tuple $m=(\text{name}(m), \text{task}(m), \text{subtasks}(m), \text{constr}(m))$ where:

- $\text{name}(m), \text{task}(m)$ are the same as with STN methods
- $(\text{subtasks}(m), \text{constr}(m))$ is a hierarchical task network, with subtasks (similar to STN methods) but also arbitrary constraints.

So the main difference between HTN and STN is that HTN can handle arbitrary constraints, which makes it more powerful, but also more complex.

### HTN vs STRIPS planning

The STN/HTN formalism is more expressive; you can encode more problems with this formalism than you can with STRIPS. For example, STN/HTN planning can encode _undecidable_ problems.

However, if you leave out the recursive aspects of STN planning, you can translate such a non-recursive STN problem into an equivalent STRIPS problem. However the size of the problem may become exponentially larger.

There is also a set of STN domains called "regular" STN domains which are equivalent to STRIPS.

It is important to note that STN/HTN and STRIPS planning are meant to solve different kinds of problems - STN/HTN for task-based planning, and STRIPS for goal-based planning.

## Graphplan

Like STRIPS, a planning problem for Graphplan consists of a set of operators constituting a domain, an initial state, and set of goals that need to be achieved.

A major difference is that Graphplan works on a _propositional_ representation, i.e. the atoms that make up the world are no longer structured - they don't consist of objects and their relationships but of individual symbols (facts about the world) which can be either true or false. Actions are also individual symbols, not parameterized actions. However, note that every STRIPS planning problem can be translated into an equivalent propositional problem, so long as its operators have no negative preconditions.

The Graphplan algorithm creates a data structure called a _planning graph_. The algorithm has two major steps:

1. the planning graph is expanded with two new layers, an action layer and a proposition layer
2. the graph is searched for a plan

The initial layer is a layer of propositions that are true in the initial state. Then a layer of actions applicable given this initial layer is added, followed by a proposition layer of those propositions that would be true after these actions (including those that were true before which have not been altered by the actions).

TODO planning graph example image

This expansion step runs in polynomial time (so it is quite fast).

The search step searches backwards, searching from the last proposition layer in the plan and goes backwards to the initial state. The search itself can be accomplished with something like A\*. If a plan is not found, the algorithm goes back to the first step.

Example: Simplified DWR problem:

- location 1
  - robot `r`
  - container `a`
- location 2
  - robot `q`
  - container `b`

- robots can load and unload autonomously
- locations may contain unlimited number of robots and containers
- problem: swap locations of containers (i.e. we want container `a` at location 2 and container `b` at location 1)

Here are the STRIPS operators we could use in this domain:

- `move(r,l,l')`
  - precond: `at(r,l), adjacent(l,l')`
  - effects: `at(r,l), not at(r,l)`
- `load(c,r,l)`
  - precond: `at(r,l), in(c,l), unloaded(r)`
  - effects: `loaded(r,c), not in(c,l), not unloaded(r)`
- `unload(c,r,l)`
  - precond: `at(r,l), loaded(r,c)`
  - effects: `unloaded(r), in(c,l), not loaded(r,c)`

There are no negative preconditions here so we can translate this into a propositional representation.

Basically for each operator, we have to consider every possible configuration (based on the preconditions), and each configuration is given a symbol. For example, for `move(r,l,l')`, we have two robots and two locations. So there are four possibilities for `at(r,l)` (i.e. `at(robot r, location 1), at(robot q, location 1), at(robot r, location 2), at(robot q, location 2)`), and because we have only two locations, there is only one possibility for `adjacent(l,l')`, so we will have eight symbols for that STRIPS operator in the propositional representation.

So for instance, we could use the symbol `r1` for `at(robot r, location 1)`, the symbol `r2` for `at(robot r, location 2)` , the symbol `ur` for `unloaded(robot r)`, and so on.

Then we'd represent the initial state like so: `{r1, q2, a1, b2, ur, uq}`.

Our actions are also symbolically represented. For instance, we could use the symbol `Mr12` for `move(robot r, location 1, location 2)`.

A propositional planning problem $\mathcal P = (\Sigma, s_i, g)$ has a solution if and only if $S_g \cap \Gamma^{>}(\{s_i\}) \neq \emptyset$, where $\Gamma^{>}(\{s_i\})$ is the set of all states reachable from the initial state.

We can identify reachable states by constructing a _reachability tree_, where:

- the root is the initial state $s_i$
- the children of a node $s$ are $\Gamma(\{s\})$
- arcs are labeled with actions

All nodes in the reachability tree is denoted $\Gamma^{>}(\{s_i\})$.

All nodes up to depth $d$ are $\Gamma^d(\{s_i\})$.

These trees are usually very large: there are $O(k^d)$ nodes, where $k$ is the number of applicable actions per state. So we cannot simply traverse the entire tree.

Instead, we can construct a _planning graph_.

A planning graph is a layered (layers as mentioned earlier) directed graph $G=(N,E)$, where:

- $N = P_0 \cup A_1 \cup P_1 \cup A_2 \cup, \dots$
  - $P_0, P_1, \dots$ are state proposition layers
  - $A_1, A_2, \dots$ are action layers

The first proposition layer $P_0$ has the propositions in the initial state $s_i$.

An action layer $A_j$ has all actions $a$ where $\text{precond}(a) \subseteq P_{j-1}$.

A proposition layer $P_j$ has all propositions $p$ where $p \in P_{j-1}$ or $\exists a \in A_j: p \in \text{effects}^{+}(a)$. Note that we do not look at negative effects; we never remove negative effects from a layer.

As a result, both proposition layers and action layers increase (grow larger) monotonically as we move forward through the graph.

We create arcs throughout the graph like so:

- from proposition $p \in P_{j-1}$ to action $a \in A_j$ if $p \in \text{precond}(a)$
- from action $a \in A_j$ to layer $p \in P_j$
  - positive arc if $p \in \text{effects}^+(a)$
  - negative arc if $p \in \text{effects}^-(a)$
- no arcs between other layers

If a goal $g$ is reachable from the initial state $s_i$, then there will be some proposition layer $P_g$ in the planning graph such that $g \subseteq P_g$.

This is a necessary condition, but not sufficient because the planning graph's proposition layers contain propositions that _may_ be true depending on the selected actions in the previous action layer; furthermore, these proposition layers may contain inconsistent propositions (e.g. a robot cannot be in two different locations simultaneously). Similarly, actions in an action layer may not be applicable at the same time (e.g. a robot cannot move to two different locations simultaneously).

The advantage of the planning graph is that it is of polynomial size and we can evaluate this necessary condition in polynomial time.

### Action independence

Actions which cannot be executed simultaneously/in parallel are _dependent_, otherwise, they are _independent_.

More formally:

Two actions $a_1$ and $a_2$ are independent if and only if:

- $\text{effects}^{-}(a_1) \cap (\text{precond}(a_2) \cup \text{effects}^{+}(a_2)) = \emptyset$ and
- $\text{effects}^{-}(a_2) \cap (\text{precond}(a_1) \cup \text{effects}^{+}(a_1)) = \emptyset$

(that is, they don't interfere with each other)

A set of actions $\pi$ is independent if and only if every pair of actions $a_1, a_2 \in \pi$ is independent.

In pseudocode:

```
function independent(a1, a2):
  for each p in neg_effects(a1):
    if p in precond(a2) or p in pos_effects(a2):
      return false
  for each p in neg_effects(a2):
    if p in precond(a1) or p in pos_effects(a1):
      return false
  return true
```

A set $\pi$ of independent actions is _applicable_ to a state $s$ if and only if $\bigcup_{a \in \pi} \text{precond}(a) \subseteq s$.

The _result_ of applying the set $\pi$ in $s$ is defined as $\gamma(s, \pi) = (s - \text{effects} - (\pi)) \cup \text{effects}^{+}(\pi)$, where:

- $\text{precond}(\pi) = \bigcup_{a \in \pi} \text{precond}(a)$
- $\text{effects}^{+}(\pi) = \bigcup_{a \in \pi} \text{effects}^{+}(a)$
- $\text{effects}^{-}(\pi) = \bigcup_{a \in \pi} \text{effects}^{-}(a)$

### Independent action execution order

To turn a set of independent actions into a sequential plan:

If a set $\pi$ of independent actions is applicable in state $s$, then for any permutation $a_1, \dots, a_k$ of the elements of $\pi$:

- the sequence $a_1, \dots, a_k$ is applicable to $s$
- the state resulting from the application of $\pi$ to $s$ is the same as from the application of $a_1, \dots, a_k$, i.e. $\gamma(s, \pi) = \gamma(s, (a_1, \dots, a_k))$.

Which is to say that the execution order doesn't matter for a set of independent actions, we can execute these actions in any order we like (intuitively, this makes sense, because none of them interfere with each other, so they can happen in any order).

### Layered plans

Let $P = (A, s_i, g)$ be a statement of a propositional planning problem and let $G=(N,E)$ be the planning graph as previously defined.

A _layered plan_ over $G$ is a sequence of sets of actions $\Pi = \pi_1, \dots, \pi_k$ where:

- $\pi_i \subseteq A_i \subseteq A$
- $\pi_i$ is applicable in state $P_{i-1}$
- the actions of $\pi_i$ are independent

A layered plan $\Pi$ is a solution to a planning problem $P$ if and only if:

- $\pi_1$ is applicable in $s_i$
- for $j \in \{2, \dots, k\}, pi_j$ is applicable in state $\gamma(\dots \gamma(\gamma(s_i, \pi_1), \pi_2), \dots \pi_{j-1})$
- $g \subseteq \gamma(\dots \gamma(\gamma(s_i, \pi_1), \pi_2), \dots, \pi_k)$

### Mutual exclusivity (mutex)

In addition to the nodes and edges thus far mentioned, there are also _mutual exclusivity_ (_mutex_) propositions and actions.

Two propositions in a layer may be incompatible if:

- the only actions which produce them are dependent actions
- they are the positive and negative effects of the same action

We introduce a _No-Op_ operation for a proposition $p$, notated $Ap$. They carry $p$ from one proposition layer to the next. Thus their only precondition is $p$, and their only effect is $p$. These were implied previously when we said that all propositions are carried over to the next proposition layer; we are just making them explicit through these No-Op actions.

With these no-op actions, we can now encode the second reason for incompatible propositions (i.e. if they are positive and negative effects of the same action) as the first (i.e. they are produced by dependent actions), the dependent actions now being the no-op action and the original action.

We say that two propositions $p$ and $q$ in proposition layer $P_j$ are _mutex_ (mutually exclusive) if:

- every action in the preceding action layer $A_j$ that has $p$ as a positive effect (including no-op actions) is mutex with every action in $A_j$ that has $q$ as a positive effect
- there is no single action in $A_j$ that has both $p$ and $q$ as positive effects

Notation: $\mu P_j = \{(p, q) | p, q \in P_j \text{are mutex} \}$

In pseudocode, this would be:

```
function mutex_proposition(p1, p2, mu_a_j):
  for each a1 in p1.producers:
    for each a2 in p2.producers:
      if (a1, a2) not in mu_a_j:
        return false
  return true
```

See below for the definition `mu_a_j`.

Two actions $a_1$ and $a_2$ in action layer $A_j$ are mutex if:

- $a_1$ and $a_2$ are independent, or
- a precondition of $a_1$ is mutex with a precondition of $a_2$

Notation: $\mu A_j = \{(a_1, a_2) | a_1, a_2 \in A_j \text{are mutex} \}$

In pseudocode, this would be:

```
function mutex_action(a1, a2, mu_P):
  if not independent(a1, a2):
    return true
  for each p1 in precond(a1):
    for each p2 in precond(a2):
      if (p1, p2) not in mu_P:
        return true
  return false
```

How do mutex relations propagate through the planning graph?

If $p,q \in P_{j-1}$ and $(p, q) \notin \mu P_{j-1}$ then $(p,q) \notin \mu P_j$.

Proof:

- if $p,q \in P_{j-1}$ then $Ap, Aq \in A_j$ (reminder: $Ap, Aq$ are no-op operations for $p, q$ respectively)
- if $(p, q) \notin \mu \P_{j-1}$ then $(Ap, Aq) \notin \mu A_j$
- since $Ap, Aq \in A_j$ and $(Ap, Aq) \notin \mu A_j$, $(p,q) \in \mu P_j$ must hold

If $a_1,a_2 \in A_{j-1}$ and $(a_1, a_2) \notin \mu A_{j-1}$ then $(a_1,a_2) \notin \mu A_j$.

Proof:

- if $a_1,a_2 \in A_{j-1}$ and $(a_1, a_2) \notin \mu A_{j-1}$ then
  - $a_1, a_2$ are independent
  - their preconditions in $P_{j-1}$ are not mutex
- both properties remain true for $P_j$
- hence $a_1, a_2 \in A_j$ and $(a_1, a_2) \notin \mu A_j$

So mutex relations decrease in some sense further down the planning graph.

Actions with mutex preconditions $p$ and $q$ are _impossible_, and as such, we can remove that action from the graph.

### Forward planning graph expansion

This is the process of growing the planning graph. Theoretically, the planning graph is infinite, but we can set a limit on it given a planning problem $P=(A,s_i,g)$.

If $g$ is reachable from $s_i$, then there is a proposition layer $P_g$ such that $g \subseteq P_g$ and $\not \exists g_1, g_2 \in g: (g_1, g_2) \in \mu P_g$ (that is, there are no pairs of goal propositions that are mutually exclusive in the proposition layer $P_g$).

The basic idea behind the Graphplan algorithm:

- expand the planning graph, one action layer and one proposition layer at a time
- stop expanding when we reach the first graph for which $P_g$ is the last proposition layer such that:
  - $g \subseteq P_g$
  - $\not \exists g_1, g_2 \in g: (g_1, g_2) \in \mu P_g$
- search backwards from the last proposition layer $P_g$ for a solution

Pseudocode for the expand step:

- function $\text{expand}(G_{k-1})$
  - $A_k = \{a \in A | \text{precond}(a) \subseteq P_{k-1} \text{and} \{(p_1, p_2) | p_1, p_2 \in \text{precond}(a)\} \cap \mu P_{k-1} = \emptyset \}$
  - $\mu A_k = \{(a_1, a_2) | a_1, a_2 \in A_k, a_1 \neq a_2, \text{and} \text{mutex}(a_1, a_2, \mu P_{k-1}) \}$
  - $P_k = \{ p | \exists a \in A_k : p \in \text{effects}^{+}(a) \}$
  - $\mu P_k = \{ (p_1, p_2) | p_1, p_2 \in P_k, p_1 \neq p_2, \text{and} \text{mutex}(p_1, p_2, \mu A_k) \}$
  - for all $a \in A_k$
    - $\text{pre}_k = \text{pre}_k \cup (\{p | p \in P_{k-1} \text{and} p \in \text{precond}(a) \} \times a)$
    - $e_k^{+} = e_k^{+} \cup (a \times \{p | p \in P_k \text{and} p \in \text{effects}^{+}(a)\})$
    - $e_k^{-} = e_k^{-} \cup (a \times \{p | p \in P_k \text{and} p \in \text{effects}^{-}(a)\})$

The size of a planning graph up to level $k$ and the time required to expand it to that level are both polynomial in the size of the planning problem.

Proof: given a problem size of $n$ propositions and $m$ actions, $|P_j| \leq n, |A_j| \leq n+m$, including no-op actions. The algorithms for generating each layer and all link types are polynomial in the size of the layer and we have a linear number of layers $k$.

Eventually a planning graph $G$ will reach a _fixed-point level_, which is the $k$th level such that for all $i, i>k$, level $i$ of $G$ is identical to level $k$, i.e. $P_i = P_k, \mu P_i = \mu P_k, A_i = A_k, \mu A_i = \mu A_k$.

This is because as $P_i$ grows monotonically, $\mu P_i$ shrinks monotonically, and $A_i, P_i$ depend only on $P_{i-1}, \mu P_{i-1}$.

### Backward graph search

This is just a depth-first graph search, starting from the last proposition layer $P_k$, where the search nodes are subsets of nodes from the different layers.

The general idea:

- let $g$ be the set of goal propositions that need to be achieved at a given proposition layer $P_j$ (starting with the last layer)
- find a set of actions $\pi_j \subseteq A_j$ such that these actions are not mutex and together achieve $g$
- take the union of the preconditions of $\pi_j$ as the new (sub)goal set to be achieved in the proposition layer $P_{j-1}$

When implementing this search, we want to keep track of, for each proposition layer, which set of subgoals have failed the search (i.e. are unachievable). The motivation is that if we search and run into a failure state, we have to backtrack, and we don't want to end up in the same failure state some other way.

We use a _nogood_ table $\nabla$ to keep track of what nodes we've seen before. Up to layer $k$, the nogood table is an array of $k$ sets of sets of goal propositions. That is, for each layer we have a set of sets. The inner sets represent a single combination of propositions that cannot be achieved. The outer set, then, contains all combinations of propositions that cannot be achieved for that layer.

So before searching for the set $g$ in a proposition layer $P_j$, we first check whether or not $g \in \nabla(j)$; that is, check to see $g$ has already been determined unachievable for $P_j$.

Otherwise, if we do search for $g$ in $P_j$ and find that it is unachievable, we add $g$ to $\nabla(j)$.

For this backward search, we define a function `extract`:

- function $\text{extract}(G,g,i)$
  - if $i=0$ then return $()$
  - if $g \in \nabla(i)$ then return failure
  - $\Pi = \text{gpSearch}(G,g, \{\}, i)$
  - if $\Pi \neq \text{failure}$ then return $\Pi$
  - $\nabla(i) = \nabla(i) + g$
  - return failure

The function `gpSearch` is defined as:

- function $\text{gpSearch}(G, g, \pi, i)$
  - if $g = \{\}$ then
    - $\Pi = \text{extract}(G, \bigcup_{a \in \pi} \text{precond}(a), i-1)$
    - if $\Pi = \text{failure}$ then return failure
    - return $\text{concat}(\Pi, (\pi))$
  - $p = g.\text{selectOneSubgoal()}$
  - $\text{providers} = \{a \in A_i | p \in \text{effects}^{+}(a) \text{and} \not \exists a' \in \pi: (a,a') \in \mu A_i\}$
  - if $\text{providers} = \{\}$ then return failure
  - $a = \text{providers.chooseOne()}$ (we may need to backtrack to here)
  - return $\text{gpSearch}(G, g-\text{effects}^{+}(a), \pi + a, i)$


We can combine everything into the complete Graphplan algorithm:

- function $\text{graphplan}(A, s_i, g)$
  - $i=0, \nabla=[], P_0 = s_i, G = (P_0, \{\})$
  - while $(g \subsetneq P_i \text{or} g^2 \cap \mu P_i \neq \emptyset)$ and $\not \text{fixedPoint}(G)$ do
    - $i = i + 1$; $\text{expand(G)}$
  - if $g \subsetneq P_i$ or $g^2 \cap \mu P_i \neq \emptyset$ then return failure
  - $\eta = \text{fixedPoint}(G) ? |\nabla(k)|: 0$ (ternary operator)
  - $\Pi = \text{extract}(G,g,i)$
  - while $\Pi = \text{failure}$ do
    - $i = i + 1$; $\text{expand}(G)$
    - $\Pi = \text{extract}(G,g,i)$
    - if $\Pi = \text{failure}$ and $\text{fixedPoint}(G)$ then
      - if $\eta = |\nabla(k)|$ then return failure
      - $\eta = |\nabla(k)|$
  - return $\Pi$

The Graphplan algorithm is sound, complete, and always terminates.

The plan that is returned (if the planning problem has a solution, otherwise, no plan is returned) will have a minimal number of layers, but not necessarily a minimal number of actions.

It is orders of magnitude faster than the previously-discussed techniques due to the planning graph structure (the backwards search still takes exponential time though).

## The FF Planner

The FF Planner performs a forward state-space search (the basic strategy can be A\* or enforced hill climbing (EHC, a kind of best-first search where we commit to the first state that looks better than all previous states we have looked at)).

It uses a _relaxed problem heuristic_ $h^{FF}$. The relaxed problem is constructed by ignoring delete list of all the operators.

Then we solve this relaxed problem; this can be done in polynomial time:
- chain forward to build a relaxed planning graph
- chain backward to extract a relaxed plan from the graph

Then we use the length (i.e. number of actions) of the relaxed plan as a heuristic value (e.g. for A\*).

For example, with the simplified DWR from before:

- `move(r,l,l')`
  - precond: `at(r,l), adjacent(l,l')`
  - effects: `at(r,l), not at(r,l)`
- `load(c,r,l)`
  - precond: `at(r,l), in(c,l), unloaded(r)`
  - effects: `loaded(r,c), not in(c,l), not unloaded(r)`
- `unload(c,r,l)`
  - precond: `at(r,l), loaded(r,c)`
  - effects: `unloaded(r), in(c,l), not loaded(r,c)`

To get the relaxed problem, we drop all delete lists:

- `move(r,l,l')`
  - precond: `at(r,l), adjacent(l,l')`
  - effects: `at(r,l), not at(r,l)`
- `load(c,r,l)`
  - precond: `at(r,l), in(c,l), unloaded(r)`
  - effects: `loaded(r,c)`
- `unload(c,r,l)`
  - precond: `at(r,l), loaded(r,c)`
  - effects: `unloaded(r), in(c,l)`

Pseudocode for computing the relaxed planning graph (RPG):

- function $\text{computeRPG}(A, s_i, g)$
  - $F_0 = s_i, t = 0$
  - while $g \subsetneq F_t$ do
    - $t = t+1$
    - $A_t = \{a \in A | \text{precond}(a) \subseteq F_t \}$
    - $F_t = F_{t-1}$
    - for all $a \in A_t$ do
      - $F_t = F_t \cup \text{effects}^{+}(a)$
    - if $F_t = F_{t-1}$ then return failure
  - return $[F_0, A_1, F_1, \dots, A_t, F_t]$

Pseudocode for extracting a plan from the RPG (in particular, the size of the plan, since this is a heuristic calculation):

- function $\text{extractRPSize}([F_0, A_1, F_1, \dots, A_k, F_k], g)$
  - if $g \subsetneq F_k$ then return failure
  - $M = \max\{\text{firstlevel}(g_i, [F_0, \dots, F_k]) | g_i \in g\}$
  - for $t = 0$ to $M$ do
    - $G_t = \{g_i \in g | \text{firstlevel}(g_i, [F_0, \dots, F_k]) = t\}$
  - for $t = M$ to 1 do
    - for all $g_t \in G_t$ do
      - select $a : \text{firstlevel}(a, [A_1, \dots, A_t]) = t$ and $g_t \in \text{effects}^{+}(a)$
      - for all $p \in \text{precond}(a)$ do
        - $G_{\text{firstlevel}(p, [F_0, \dots, F_k])} = G_{\text{firstlevel}(p, [F_0, \dots, F_k])} \cup \{p\}$
  - return number of selected actions

The `firstlevel` function tells us which layer (by index) a goal $g_i$ first appears in the planning graph.

This heuristic is _not_ admissible (it is not guaranteed to return a minimal plan), but in practice it is quite accurate, so it (or ideas inspired by it) are frequently used (currently state-of-the-art).

## Planning under uncertainty

Thus far all the approaches have assumed the outcome of actions are deterministic.

However, the outcome of actions are often uncertain, so the resulting state is uncertain.

One approach is _belief state search_. A _belief state_ is a set of world states, one of which is the true state, but we don't know which one. The resulting solution plan is a sequence of actions.

Another approach is _contingency planning_. The possible outcomes of an action are called _contingencies_. The resulting solution plan is a tree that branches according to contingencies. At branching points, observation actions are included to see which branch is actually happening.

Both of these approaches are naturally way more complex due to multiple possible outcomes for actions.

If we can quantify the degree of uncertainty (i.e. we know the probabilities of different outcomes given an action) we have, we can use _probabilistic planning_. Instead of simple state transition systems, we use a _partially observable Markov decision process_ as our model:

- a set of world states $S$
- a set of actions $A$, actions are applicable in certain states: $s \in S: A(s) \subseteq A$
- cost function, gives the cost of an action in a given state: $c(a,s)>0$ for $s \in S$ and $a \in A$
- transition probabilities: $P_a(s'|s)$ for $s,s' \in S$ and $a \in A$ (probability of state $s'$ when executing action $a$ in state $s$)
- initial belief state (probability distribution over all states in $S$)
- final belief state (corresponds to the goal)

- solution (called a "policy"): a function from states to actions, i.e. given a state, this is the action we should execute
- we want the optimal policy, i.e. the policy with the minimal expected cost

## Planning with time

So far we have assumed actions are instantaneous but in reality, they take time. That is, we should assume that our actions are _durative_ (they take time). We can assume that actions take a known amount of time, with a start time point and an end time point.

With A\* we can include time as part of an action's cost.

With partial plans (e.g. HTN) we can use a _temporal constraint manager_, which could be:

- _time point networks_: associates all time points in a given plan, where we assert relations between different time points (e.g. that $t_1 < t_2$)
- _interval algebra_: instead of relating time points, we relate the intervals that correspond to the action execution (e.g. we assert that interval $i_1$ must occur before $i_2$ or that $i_3$ must occur during $i_4$)

One way:

We can specify an _earliest start time_ $ES(s)$ and a _latest start time_ $LS(s)$ for each task/state in the network.

We define $ES(s_0) = 0$. For any other state:

$$
ES(s) = \max_{A \to a} ES(A) + \text{duration}(A)
$$

Where $A \to s$ just denotes each predecessor state of $s$.

We define $LS(s_g) = ES(s_g)$. For any other state:

$$
LS(s) = \min_{s \to B} LS(B) - \text{duration}(s)
$$

Where $s \to B$ just denotes each successor state of $s$.

## Learning

The planners mentioned so far do not _learn_ anything, if they encounter a problem similar to one they have previously encountered, they start from scratch all over again.

The general idea with planners that can learn is:

- let the planner solve a series of similar planning problems
- analyze the performance of the planner
- feedback this analysis into the planning process

## Multi-agent planning

So far we have assumed planners working in isolation, with control over all the other agents in the plan.

Multi-agent planning does away with that assumption, and results in a much more complex problem:

- agents with different beliefs
- agents with different capabilities
- agents with joint goals
- agents with individual/conflicting goals
- joint actions (multiple coordinating agents required to accomplish an action)

Other things that must be considered during execution of multi-agent plans:

- coordination (ordering constraints, sharing resources, joint actions)
- communication (e.g. communicating results)
- execution failure recovery (local plan repair, propagating changes to the plan across agents)

## Scheduling: Dealing with resources

Actions need resources (time can also be thought of as a resource).

Planning which deals with resources is known as _scheduling_.

A _resource_ is an entity needed to perform an action, which are described by resource variables.

A distinction between state and resource variables:

- state variables: modified by actions in absolute ways
- resource variables: modified by actions in relative ways

Some resource types include:

- reusable vs consumable
- discrete vs continuous
- unary (only one available)
- shareable
- resources with states

---

Planning approaches can be arranged in a table:

|                      | Deterministic                   | Stochastic |
|----------------------|---------------------------------|------------|
| Fully observable     | A\*, depth-first, breadth-first | MDP        |
| Partially observable |                                 | POMDP      |


## Markov Decision Process (MDP)

Includes:

- states $s_1, \dots, s_N$
- actions $a_1, \dots, a_k$
- a state transition matrix $T(s,a,s') = P(s'|a,s)$ (the probability of state $s'$ after executing action $a$ in state $s$)
- a reward function $R(s)$ (to define the objective, sometimes called a _utility function_), which associates a cost or reward with each state

The result is a _policy_ $\pi(s) \to A$, which assigns actions to any state.

That is, it doesn't result in a fixed plan, rather, it makes decisions on a state-by-state basis, considering the current state.

Conventional planning, in contrast, when dealing with stochastic environments, has the following problems:

- the branching factor is very large
- the tree can be too deep (e.g. potentially infinite loops)
- many states may be visited more than once

The objective MDP is to maximize the expected sum of all future rewards, i.e.

$$
\max(E[\sum_{t=0}^{\infty} R_t])
$$

Sometimes a _discount factor_ $\gamma \in [0,1]$ is included, e.g. $\gamma=0.9$, which decays future reward:

$$
\max(E[\sum_{t=0}^{\infty} \gamma^t R_t])
$$

Using this, we can define a _value function_ $V(s)$ for each state:

$$
V^{\pi}(s) = E[\sum_{t=0} \gamma^t R_t | s_0 = s]
$$

That is, it is the expected sum of future discounted reward provided we start in state $s$ with policy $\pi$.

This can be computed empirically via simulations. In particular, we can use the _value iteration_ algorithm.

With value iteration, we recursively calculate the value function, starting from the goal states, to get the optimal value function, from which we can derive the optimal policy.

More formally - we want to recursively estimate the value $V(s)$ of a state $s$. We do this by estimating the value of possible successor states $s'$, discounting by $\gamma$, and incorporating the reward/cost of the state $R(s')$, across possible actions from $s$. We take the maximum of these estimates.

$$
V(s) = \max_a [\gamma \sum_{s'} P(s'|s,a) V(s')] + R(s)
$$

This method is called _back-up_.

In terminal states, we just set $V(s) = R(s)$.

We estimate these values over all our states - these estimates eventually converge.

This function essentially defines the optimal policy - that is:

$$
\pi(s) = \argmax_a \sum_{s'} P(s' | s,a) V(s')
$$

(since it's maximization we can drop $\gamma$ and $R(s)$)

## Partially-Observable MDP (POMDP)

Partially-observable environments may require information-gathering actions in addition to goal-oriented actions. Such information-gathering actions may require detours from goals but may be worth it in the long run.

## Reinforcement Learning

Reinforcement learning agents can learn in what states rewards or goals are located without needing to know from the start. Such agents can learn to find them on their own.

Reinforcement learning involves a sequence of state-action transitions, with rewards associated with each state, and learns an optimal policy $\pi(s)$.

To put it in terms of MDP, you may not know the reward function $R$ or even the transition model $P$. Reinforcement learning can learn these or substitutes for these by interacting with the world.

If we do know $P$, a _utility-based agent_ can learn $R$ and thus $V$, which we can then use for MDP.

If $P$ and $R$ are both unknown, a _Q-learning agent_ can learn $Q(s,a)$ without needing either. Where $V(s)$ is the value over states, $Q(s,a)$ is the value over state-action pairs and can also be used with MDP.

A _reflex agent_ can also directly learn the policy $\pi(s)$ without needing to know $P$ or $R$.

Reinforcement learning agents can be _passive_, which means the agent has a fixed policy and learns $R$ and $P$ (if necessary) while executing that policy.

Alternatively, an _active_ reinforcement learning agent changes its policy as it goes and learns.

Passive learning has the drawbacks that it can take awhile to converge on good estimates for the unknown quantities, and it may limit how much of the space is actually explored, and as such, there may be little or no information about some states and better paths may remain unknown.

### Temporal Difference Learning (TDL)

In TDL, the agent moves from one state $s$ to the next $s'$, looks at the reward difference between the states, then backs up (propagates) the values (as in value iteration) from one state to the next.

We run this many times, reaching a terminal state, then restarting, and so on, to get better estimates of the rewards (utilities) for each state.

We keep track of rewards for visited states as $U(s)$ and also the number of times we have visited each state as $N(s)$.

The main part of the algorithm is:

- if $s'$ is new then $U[s'] = r'$
- if $s$ is not null then
  - increment $N_s[s]$
  - $U[s] = U[s] + \alpha(N_s[s])(r + \gamma U[s] - U[s])$

Where $\alpha$ is the learning rate and $\gamma$ is the discount factor.

### Greedy TDL

One method of active reinforcement learning is the greedy approach.

Given new estimates for the rewards, we recompute a new optimal policy and then use that policy to guide exploration of the space. This gives us new estimates for the rewards, and so on, until convergence.

Thus it is greedy in the sense that it always tries to go for policies that seem immediately better, although in the end that doesn't necessarily guarantee the overall optimal policy (this is the exploration vs exploitation problem).

One alternate approach is to randomly try a non-optimal action, thus exploring more of the space. This works, but can be slow to converge.

### Exploration agent

A better approach is an _exploration agent_, which favors exploring more when it is uncertain. More specifically, we can use the same TDL algorithm, but while $N_s < \epsilon$, where $\epsilon$ is some exploration threshold, we set $U[s] = R$, where $R$ is the largest reward we expect to get. When $N_s > \epsilon$, we start using the learned reward as with regular TDL.

### Q-Learning

In Q-Learning, we don't need $P$ or the reward/utility function. We directly learn the rewards/utilities of state-action pairs, $Q(s,a)$.

With this we can just choose our optimal policy as:

$$
\pi(s) = \argmax_a \sum_{s'} Q(s,a)
$$

The $Q$ update formula is simply:

$$
Q(s,a) = Q(s,a) + \alpha(R(s) + \gamma Q(s', a') - Q(s,a))
$$

Where $\alpha$ is the learning rate and $\gamma$ is the discount factor.

Again, we can back up (as with value iteration) to propagate these values through the network.

## Hidden Markov Models (HMMs)

HMMs are used to analyze or to predict time series involving noise or uncertainty.

There is a sequence of states $s_1 \to s_2 \to s_3 \to \dots \to S_N$. This sequence is a Markov chain (each state depends only on the previous state).

Each state _emits_ a measurement/observation, e.g. $s_1$ emits $z_1$ ($s_1 \to z_1$), $s_2$ emits $z_2$ ($s_2 \to z_2$), and so on. We don't deserve the states directly; we only observe these measurements (hence, the underlying Markov model is "hidden").

Together, these define a Bayes network that is at the core of HMMs.

Markov chains settle on a _stationary distribution_ as $t \to \infty$. That is:

$$
P(A_{\infty}) = \lim{t \to \infty} P(A_t)
$$

is the stationary distribution.

The key insight for a stationary distribution is that $P(A_t) = P(A_{t-1})$, and that this is independent of the initial distribution.

TODO merge HMM notes from elsewhere

Given an observation $z_i$, we may want to know $P(s_i|z_i)$, i.e. what's the probability of the hidden state $s_i$ given the observation $z_i$?

We can use Bayes rule here:

$$
P(s_i|z_i) = \frac{P(z_i|s_i)P(s_i)}{P(z_i)}
$$

Or more simply, we can drop the denominator:

$$
P(s_i|z_i) \propto P(z_i|s_i)P(s_i)
$$

We may also want to know the probability that we are in a hidden state $s_i$ given the distribution of $s_{i-1}$. We can calculate this:

$$
P(s_i) = \sum_{s_{i-1}} P(s_{i-1}) P(s_i|s_{i-1})
$$

That is, the we check against all possible states $s_{i-1}$.

So the parameters of a HMM are:

- the next state distribution $P(s_i|s_{i-1})$
- the measurement distribution $P(z_i|s_i)$
- the initial state distribution $P(s_0)$

### Particle filters

With particle filters, possible states are represented as vectors (particles); the density of these vectors in state space represents the posterior probability of being in a certain state (that is, higher density means the true state is more likely in that region), and the set of all these vectors represents the belief state.

So to start, these particles may be very diffuse, spread out across the space somewhat uniformly. As more data (measurements/observations) is collected, the particles are resampled and placed according to these observations, and they start to concentrate in more likely regions.

The particle filter algorithm:

```
# s is a set of particles with importance weights
# u is a control vector
# z is a measurement vector
def particle_filter(s, u, z):
  # a new particle set
  s_new = []

  eta = 0

  n = len(s)
  for i in range(n):
    # sample a particle (with replacement)
    # based on the importance weights
    p = sample(s)

    # sample a possible successor state (i.e. a new particle)
    # according to the state transition probability
    # and the sampled particle
    # p' ~ p(p'|u, p)
    p_new = sample_next_state(u, p)

    # use the measurement probability as the importance weight
    # p(z|p')
    w_new = measurement_prob(z, p_new)

    # save to new particle set
    s_new.append((p_new, w_new))

  # normalize the importance weights
  # so they act as a probability distribution
  eta = sum(w for p, w in s_new)
  for i in range(n):
    s_new[i][1] /= eta
```

Particle filters do not scale to high-dimensional spaces because the number of particles you need to fill a high-dimensional space grows exponentially with the dimensionality. Though there are some particle filter methods that can handle this better.

But they work well for many applications. They are easy to implement, computationally efficient, and can deal well with complex posterior distributions.

## Adversarial planning (Games)

In planning so far we have not considered an agent that may actively be working against us, trying to prevent us from achieving our goals.

Games provide a nice model for such scenarios because many of them have relatively simple rules and are designed to be adversarial.

Games may:

- have $n$ players, $n \geq 1$
- be deterministic or stochastic
- vary in utility for each player (e.g. a game can zero-sum)
- can have perfect information or not
- involve taking turns or acting simultaneously

For two-player, deterministic, zero-sum game, we can define a value function for any state in the game tree (collectively this approach is called _minimax_):

```
def value(state):
  if state.is_terminal: return utility(state)
  if state.is_max_turn: return max_value(state)
  if state.is_min_turn: return min_value(state)

def max_value(state):
  m = -infinity
  for (action, successor) in state.successors:
    v = value(successor)
    m = max(m, v)
  return m

def min_value(state):
  m = infinity
  for (action, successor) in state.successors:
    v = value(successor)
    m = min(m, v)
  return m
```

TODO combine this with other minimax explanation

We can adapt this for games with stochastic elements by introducing chance nodes as well and updating the value function to reflect this:

```
def value(state):
  if state.is_terminal: return utility(state)
  if state.is_max_turn: return max_value(state)
  if state.is_min_turn: return min_value(state)
  if state.is_chance:   return expected_value(state)
```

### Game Theory

Game theory can be thought of as an approach to finding a policy when it depends on an opponent's policy.

A commonly-studied game is the _prisoner's dilemma_.

There are two players. They are arrested by the police. Either player can snitch on the other, or they can cooperate and both remain silent. The payoffs, here framed as prison sentences, generally take the form:

|          | A snitch   | A silent   |
|----------|------------|------------|
| B snitch | A=-5, B=-5 | A=-10, B=0 |
| B silent | A=0, B=-10 | A=-1, B=-1 |

A _dominant strategy_ is one in which the player does better than they would any other strategy, no matter how the other player plays.

Dominant strategies don't always exist. In prisoner's dilemma, the dominant strategy for each player is to snitch.

For example: the possible payouts for player A snitching is -5 and 0. The possible payouts for A remaining silent is -10 and -1. If A snitches, they will do better no matter how player B players than if they were to remain silent - if they remained silent, player A could have a very bad payoff if B snitches.

A _Pareto optimal_ outcome is the outcome that all players would prefer. More concretely, it is one in which no player can unilaterally switch their strategy to improve their payoff without making someone else worse off.

In prisoner's dilemma there are three Pareto optimal outcomes - this is (A silent, B silent), (A silent, B snitch), (B silent, A snitch).

For example, with (A silent, B snitch), player B has a payoff of 0 which is great. A would want to switch to a better payoff, i.e. A would also want to snitch, but that would hurt B's payoff.

An _equilibrium_ is an outcome in which no player can get a better outcome by switching, assuming that every other player keeps the same strategy. Every game has at least one equilibrium.

In prisoner's dilemma, there is one equilibrium: (A testify, B testify). With that outcome, no player can unilaterally improve their own payoffs.

Here, if both players play rationally, they end up with the equilibrium outcome, but it is clearly not the ideal outcome - it's better for both to remain silent.

Another game is _two finger morra_. There is an "even" player and an "odd" player, each player can show one or two fingers. If the total number of fingers is even, the even player wins, otherwise, the odd player wins. The winning player gains points equal to the number of fingers played, and the other player loses that many points (it is a zero-sum game).

|         | Odd: 1    | Odd: 2    |
|---------|-----------|-----------|
| Even: 1 | E=2, O=-2 | E=-3, O=3 |
| Even: 2 | E=-3, O=3 | E=4, O=-4 |

In this game, there is no dominant strategy for either player. Rather there is a _mixed strategy_, which defines a probability distribution of possible moves (in the other case, when there is just a single strategy to play, we call that a _pure strategy_).

---

## References

- Planning Algorithms. Steven M. LaValle, 2006. <http://planning.cs.uiuc.edu/booka4.pdf>
- Artificial Intelligence Planning. Dr. Gerhard Wickler, Prof. Austin Tate. The University of Edinburgh (Coursera). 2015.
- [Intro to Artificial Intelligence](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271). CS271. Peter Norvig, Sebastian Thrun. Udacity.

---

## Reactive planning

Reactive planning is an approach to planning in which no sequence of actions is planned in advance; rather, new actions are chosen on-the-fly. Thus they are well suited for real-time environments.

## Case-Based Goal Formulation

With _case-based goal formulation_, a library of cases relevant to the problem is maintained (e.g. with RTS games, this could be a library of replays for that game). Then the agent uses this library to select a goal to pursue, given the current world state That is, the agent finds the state case $q$ (from a case library $L$) most similar to the current world state $s$:

$$
q = \argmin_{c \in L} \text{distance}(s, c)
$$

Where the distance metric may be domain independent or domain specific.

Then, the goal state $g$ is formulated by looking ahead $n$ actions from $q$ to a future state in that case $q'$, finding that difference, and adding that to the current world state $s$:

$$
g = s + (q' - q)
$$

The number of actions $n$ is called the _planning window size_. A small planning window is better for domains where plans are invalidated frequently.

## References

- Integrating Learning in a Multi-Scale Agent. Ben G. Weber. 2012.
